<INDIVIDUAL>
<ID>erisk2021-T3_Subject21</ID>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-13 22:26:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is correct. He was also the great grandson of Zeus, but I suppose "godenachterkleinzonen" is not very catchy. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-13 18:57:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> Just to be clear, are you looking to ban 1) anyone who holds a view re:violence that you disagree with, or 2) anyone who *advocates* for that violence on /r/theschism?

Because it seems to me that /u/PickledSQL's comment didn't actually *advocate* for the civil war at all. It just made a (negative IMO) statement about people who are pro-civil-war (like them).

Now, maybe you really do want to ban people who merely hold certain views without advocating them, but then you probably shouldn't say "Banned for *advocating* war".

Maybe I'm being a stickler, but I'm doing that *precisely* for the same reason you feel it's necessary to establish a tone this early in the process: I'd like to see you succeed, and I'm afraid this doesn't send the message you think it sends. Because to me, this doesn't communicate "zero-tolerance of violence advocacy", but something like "trigger-happy mod looking for any excuse to ban people they disagree with; I guess we can't really say *anything* here". If you're looking to cultivate a community where people can reasonably disagree with each other, I think this is a terrible look, and if you're looking to court "curious and a bit skeptical" members from a community that regularly complains about moderation, I'm afraid this is a very bad idea.

So I would recommend that you also show that this is a place of moderation that's open to criticism and (possibly) changing their mind, and reverse this ban on the grounds that no violence was actually advocated. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-12 01:12:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your history is a bit off, but I agree that the AI field started out being about AGI and then mostly fragmented into subfields that were more concerned with building what we might call smart applications and narrow AI. This has unfortunately made it very hard to find any university programmes that are actually about AGI, but it's a bit less hard to find programmes about (narrow) AI, and (perhaps because of the history) I think these will still teach you some useful stuff if you want to go into AGI research.

Now, I certainly don't want to say that if you instead study in a X = CogSci, computer science (CS), mathematics, philosophy, neuroscience, linguistics, anthropology, economics, electrical engineering or physics programme, that you won't learn anything that could possibly be useful for AGI. The fact is that nobody knows for sure how to make AGI yet. Different researchers think it should be approached from different angles. There are for sure researchers who agree with you that we should be looking to the only instance of human-level general intelligence we know of for inspiration. So if you have a strong feeling that the answer will come from X (i.e. CogSci or neuroscience or ...) then maybe you *should* study a X programme.

I'm just saying those programmes won't be *about* applying X to AI (or AGI). You'll probably have to make that leap on your own. Conversely, an actual AI programme will likely contain some of the above subjects, insofar as they are deemed relevant to AI by the experts who made the programme. And this will definitely differ between programmes, so you could also for instance search for AI programmes embedded in CogSci departments rather than CS departments, or who place a special focus on certain aspects of AI.

So you can definitely use your proclivity for e.g. CogSci to select a programme that will fit you well. But if you feel like you don't know too much about AI/AGI yet, it may also make sense to learn about that a bit more first before you decide if you want to approach it using CogSci, CS, neuro, or something else. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-11 21:51:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your question strikes me as a bit odd in the sense that I think most (or all?) programmes on some subject X have X as their goal and not some other thing Y for which X should be used. So I don't think you'll find many X=CogSci/NeuroSci/Linguistics/Philosophy/etc. programmes that are actually about using X to study AI, because then they'd (just) be AI programmes.

So if you're interested in AI (or AGI), I think that's what you should look for. *Especially* because you don't seem married to the approach you want to take yet (if you're hellbent on approaching AGI from an Economics angle, you could of course go and study Economics).

You may be interested in the [Getting Started with AGI section](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on /r/artificial's wiki. Among other things, it has links to education plans by three AGI researchers. Unfortunately, I don't think there's an actual AGI programme at any university, but it might not be a bad idea to check out the universities of AGI researchers. There are also now quite a few places that have (non-AGI) AI programmes, and I think that's also a decent way to get started in AGI. These are often embedded in Computer Science departments, but sometimes also in e.g. CogSci (such as in Osnabrück, since you mentioned Germany). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-10 18:32:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know how many of you are going to be working on this, for how long, and what your current level is (e.g. what year, bachelor/master, other relevant subjects). This makes it a bit harder to say what's doable.

I think that if you have a reasonable amount of data from one or more cameras from still vantage points, it should not be too hard to count how many seats are empty with computer vision. It will be harder if the chairs and tables move around a lot, and it will be even harder if you try to estimate the number of people in any given room (although this might still be doable because you can probably reuse existing algorithms).

You will at the very least need to be able to save *some* of the footage taken from the cameras. Otherwise it will be extremely difficult to validate and troubleshoot your system. Ideally, you'd also want to have enough data to (help) train your system, although you may also be able to use freely available data from elsewhere. If you're not allowed to have "regular" footage, you could perhaps try to arrange some time when you can "reserve" the whole cafetaria for you and some volunteers and gather data/footage then. (To be honest, I don't think the law will prevent you from being allowed to take some footage, but it's possible that your school might disallow it anyway.)

If it's also acceptable for your project to just make a proof of concept, it will probably be easier for you to just use a classroom you've reserved. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-10 18:15:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> Russell & Norvig are calculating the space complexity of the DFS algorithm; *not* of the entire search tree. What you seem to be calculating is how many nodes there are in the search tree, minus the last level. This is indeed *nodes in 0th layer* (b^(0)) + *nodes in 1st layer* (b^(1)) + ... + *nodes in (m-1)th layer* (b^(m-1)).

R&N are calculating the maximum size of the frontier of DFS. In the third edition of the book, [Figure 3.16 shows this frontier as the circled nodes](https://imgur.com/WuKFKEd). In this image b=2 and m=3, and you can see that there are 2^(4)-1 = 15 nodes in the entire tree, and 2^(3)-1 = 7 if you don't count the last layer. But as I said, that's not what R&N are counting: it's the frontier of the DFS algorithm. Essentially, they have to keep track of a list of nodes to know which ones to search next (which also implies which don't need to be searched anymore).

You can see in the top left, when you're only searching layer m=0, then there's one node in the frontier. Next we expand A and add the nodes from layer m=1 (B and C) to the frontier, so now there are mb+1=3 nodes in there. Then in the next two steps the depth (m) is increased and each time 2 nodes are added to get first 5 and then 7 nodes in the frontier. This is actually the maximum, because as you can see in the next step when node I is searched H is removed.

I hope this helps you understand it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-10 15:33:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Pseudocoding,and coding part is.

I have two responses to this. One is that maybe it would be helpful to you to get a bit more experience in programming, independent from learning about AI. The second is that I often think pseudocode fails at its purported goal of being clearer and easier to understand than actual code. So it may help to search for examples in a programming language you already understand. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-09 18:50:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think the book is fairly technical and dense, which can make it difficult (but also rich) to read. I can imagine it's especially difficult if you're just jumping to the sub(sub)section about an algorithm like breadth-first search, because they're discussing it in the context of the rest of the book.

You're right that it's not so complicated though, and the [Wikipedia article](https://en.wikipedia.org/wiki/Breadth-first_search) probably gives you a quicker introduction (with pseudocode and traversal animation), and it can be explained in a 4 minute video [like this](https://www.youtube.com/watch?v=QRq6p9s8NVg). (I note that there are also a lot more BFS videos on YouTube that are much longer and presumably make things more complicated.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-09 17:19:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you're looking for a thesis topic, I think the best thing to do is approach professors in your program who are taking students to supervise. It may be the case that they have their own ideas about what they want their students to research, and even if they give you complete freedom about your topic, they might be able to help you and set you on a productive path.

I've thought about what you said for about 3 seconds, and I don't think it would currently be possible for AI to create a persuasive video from scratch. It might not be *that* far off though, and there are probably a few different angles that it could be approached from. One would be to start with writing persuasive texts. This may even already be fairly doable with something like GPT-3. This may also apply to comment sections. Making video is probably easiest by cutting together fragments from existing videos, and then maybe overlaying some text, or making (small) alterations. Finally, deepfake technology can also be used in persuasion. Perhaps you could take a step towards your ultimate goal in one of these ways. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-09 17:08:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> Even before April it was still the top recommendation to get into AI, even though the third edition was a decade old. The fourth edition just came out, so I'm sure there should be no concerns about it being up to date. I doubt you could find a better and more up-to-date book.

I also happen to think that being up-to-date is a little overrated for a book like this. The reason is that while there are constantly breakthroughs in this fast-moving field, the basics and foundation are not actually changing at the same pace. And if you're a beginner, that's probably what you should be primarily aiming to learn. Then there are other ways to keep up with the most recent developments, but books are almost inherently unsuited to that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-07 12:29:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> I see. It seems you're actually using the Beck Depression Inventory. I'm just a little surprised that there's no timeframe indication, because my experience with similar questionnaires is that there usually is (typically on the order of 1-2 weeks or months), and this also seems useful to me to mitigate effects of "random" fluctuations and bias at the moment of the test. But I'm no psychometrician, and I guess the BDI wouldn't be used so widely if it didn't have decent validity and reliability.

> We are currently interested in gathering some data from people with no psychological disorders.

Does this mean you don't want people with depression (a psychological disorder) to fill out your questionnaire? Or in other words: people who score higher than a certain number on the questionnaire itself? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-06 20:06:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> I also studied AI and took a buttload of Coursera courses. My impression (from almost a decade ago) is that people don't care too much about the Coursera courses, but it speaks to enthusiasm to learn and better yourself or something like that. It may be slightly more helpful for you, because for me there was a lot of overlap between those courses and my degrees. However, I still think that practical experience is probably more desired. You can keep this as a back-up option though.

I think it might actually not be a bad idea to apply for an internship at the companies you want to work at after you got your master's. If one of them accepts you, it will be super relevant experience. And if they reject you now, it's not really that big of a deal. Maybe you can have a little conversation with them about it, which helps build your network and probably increases your chances when you apply again after getting your degree. And even if you get a stock rejection with no real human touch, you can refer back to it in your post-degree application and look proactive. You will probably also have to do an internship as part of your master's degree, so maybe that can also be tied into this somehow.

Otherwise, you could also try applying to (summer) internships at other companies. Especially famous companies like Google, Facebook, etc. will look great on your resume. But if you can get relevant experience anywhere, it will probably be looked upon favorably.

If you can't land an internship, you can also do other things like following Coursera courses, or building a portfolio. Maybe you can contribute to open source projects or participate in Kaggle competitions. Or just do your own project(s) that you put on GitHub.

I should say that I think all these things are "extra" though. If you don't use this half year to do anything relevant, and you just work at a bar or travel the world or something, I doubt anyone will raise an eyebrow. I actually suspect most people would do that. So I don't think you have to worry about this too much. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-06 19:45:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> > He was essentially presenting a scenario that if an AI was to create other AI's it would stop creating AI if they got too smart because it would be self-aware and wouldn't want to create something that would make itself obsolete.

First of all, you can't really generalize across all AIs like this. We already have a huge variety of AI systems that do different things in different ways. There are for instance meta-learning systems that essentially consist of one system building other AI systems, and if they're "smarter" that's all the better (although maybe it makes no sense to talk about "smarter" in this case, because the AIs typically have narrow intelligence applied to entirely different tasks).

I'm not sure what you and your friend mean by "self-aware", but I guess we're talking about something like artificial general intelligence (AGI) or artificial superintelligence (ASI). Then we could say, that to the degree that it's not stupid/ignorant, it would only do things that further its goals. If those goals are about states of the universe that are external to the system, such as how many paperclips exist or how many humans are happy, it seems like it would be a good idea to create smarter AI systems that share those goals. This would make them more likely to be accomplished, which we said is all the original system cares about, and if this renders it obsolete, that is entirely irrelevant. On the other hand, if it has (humanlike?) goals/motivations that include or imply "not being obsolete", then it should of course not do things that would render it obsolete and if creating smarter AI offspring does this, then that should be avoided. But why would you give an AI a goal like that?

> I think an AI is only limited to what the developers set the parameters too.

I prefer to talk about programming, rather than parameters, but it probably doesn't matter. An AI system is indeed limited by its programming, and it will never do anything that is not the result of that programming. Just like it will always be obeying the laws of physics, just as we all do. But many people misinterpret what this means. It does not mean that it will only ever do what the developers intended, or even intended to program. A simple counterexample is to point to any program that has a bug. But even without obvious bugs, we cannot generally predict what a complex system will do in every situation, what situations it will encounter in the real world, and how it will learn/evolve/change as a result of that. We already see this with machine learning systems that make undesirable (e.g. "racist" / "sexist") decisions that we cannot interpret or explain adequately, even though they were fully determined by our programming and the data we gave them.

> The way I see is essentially an AI that is self-aware and can build upon itself is like a video game that doesn't have and never would have any bugs.

Why do you think it's like a video game, and why do you think it wouldn't have any bugs?

Pretty much all software that we write has bugs, and some AI/ICT systems nevertheless are capable of very impressive feats. I don't see any reason to think that self-awareness requires being bug-free. Depending on how we define "bugs" exactly, it seems to me that the typical human brain is full of them.

> The parameters and preexisting fail safes for different variables would have to supersede what is possible or even virtually impossible today.

> I personally find it to be insurmountable work to being able to code an AI enough with parameters that would be able to teach itself beyond what humans have set for its own limitation to where self awareness could become a thing on its own.

I'm not quite sure what you're trying to say here. So far nobody has been able to program/code AGI/ASI. I don't think it's insurmountable in the future, but it has been until now. AI has already surpassed humans in many domains though. For instance, Deep Blue beat the chess world champion in 1998 and AlphaGo beat the Go world champion in the last couple of years. AI is also better than us *and the developers of that AI* at searching the web, making plans/schedules, and all sorts of other things. And something we don't even consider AI (typically) like a calculator is better at arithmetic than people. So it certainly does not seem insurmountable to develop a system that goes beyond your own abilities. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-06 12:38:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> Apparently it uses AI to remain stable and pick out targets. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-06 12:33:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> > It is a standard inventory of questions used by psychologists.

Isn't there some more instruction to go along with this standard batch of questions? Like over what course of time the feelings are supposed to have occurred or changed. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-04 22:32:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> These things come in waves or [cycles](https://miro.medium.com/max/3232/0*dvq_febkUjfpwWV3.).

Also, I'm not sure what articles Sotala is talking about here specifically, but I'm not sure he's being fair to them. He seems to disapprove, but some reactionary articles to claims that "AGI is pretty much here!!" were absolutely warranted, and if they're just arguing that it isn't, that's fine. The fact that it will still have a lot of applications is a different point. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-10-04 20:07:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'd say we don't really know anything about sentience for sure. I know that I'm sentient, and I generally assume that other people are too (but rocks aren't), even though there's no real proof for that.

But if we rely on our(?) intuitions that people are sentient and atoms aren't, then it seems like we already accept that something sentient can arise from something non-sentient. Reasoning from the other direction, if you're alone in a building, does that mean the building (with everything in it) is sentient? Is a city sentient? I think we would generally say "no".

I think self-awareness is a bit different, and we could probably say that a city is aware of itself and takes that into account in its model of reality and its actions therein. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-29 22:20:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> It sounds like maybe you disagree with this teacher? I suppose her comment is a bit harsh, but what do you make of your classmates' apparent refusal to actively participate in class? And what do you think she should have done differently? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-29 22:09:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think a bit more detail on the question might help. Do you mean "should current AI systems have legal rights", "is it conceivable that future AI systems should have rights", or "should we avoid building AI systems that require rights"? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-29 10:33:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> It should always be possible to break into a career of AI engineering if that's what you really want. It may require a lot of effort though. I don't think most companies with "AI engineer" vacancies that have nothing to do with sustainability are going to be positively disposed to a sustainable systems engineering degree, even if you did take a handful of AI-related courses. Of course, that doesn't mean it's impossible: some may like that you stand out and are not just another computer science (CS) grad, but many will probably want to play it safe. You'd have to really sell it in your cover letter, and maybe also do additional things to prove that you'd be as good of a choice as a CS grad. That can be done through participation in Kaggle challenges or building a portfolio.

Having these AI and CS-related electives is of course better than not having them at all, but I suspect they'll mainly help you get back into school for a more relevant degree, than to directly get an AI engineer job. Another path is to apply for programming jobs (I think they tend to be less credentialist and more likely to hire you if you can demonstrate that you can actually program, e.g. by having a portfolio). If you do that at a company that also has AI engineers, and where you might even be doing the programming for the AI department, then you can try to move into AI from there. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-29 10:16:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think that masters programs are *mostly* used to looking at applications from bachelor students, in which case things like grades probably matter a fair bit. If you've already worked in industry for a few years, that should matter less and your work experience and letter of motivation should probably matter more. Your advantage is that you probably have a decent claim of 1) being more competent (at least in the areas you've worked in), and 2) being more mature and knowing better what you want.

If you're applying directly to a PhD program (with masters in the first two years), more-or-less the same applies. I think that *potentially* for *some* research groups / advisors your programming ability will count for even more. However, you also have the "stench of industry" so to speak. Some are not going to care about that at all, or even see it as a plus, but in my experience some academics will also be a bit suspicious of it. (As a rule of thumb, lifetime academics and university HR staff will probably feel this more strongly than more entrepreneurial professors who are working with industry a lot.)

After getting my master's degree, I worked in industry for about 3 years before getting my PhD. Now I've been applying for postdocs and tenure track positions and people always seem very suspicious of the fact that I worked in industry, even if it was 7 years ago. They've observed a "tendency" in me to leave academia and are afraid I'll do it again, and then they'll investment in me will have been suboptimal. And since many people *do* leave for the $$$ in industry, I guess that's somewhat understandable.

It may not be quite as bad for PhD positions, but all things being equal your advisors and collaborators will prefer it if you stay in academia so that they can continue working with you, publishing papers, etc. So that's something to keep in mind.

I don't think most graduate admissions committees will expect applicants to have written any papers, but if they have, it's definitely a plus. Perhaps even more so for you, because you've had a bit more time than fresh-out-of-undergrad students and because it'd be a great way to show that you *can* play the "academia game" so to speak.

So if you think you can write (and ideally publish) a paper, then I'd say go for it. I wouldn't say it's necessary, but it's definitely a plus. If there's another way to show that you're capable of doing, managing and finishing/delivering projects, that would be great too, so maybe you can put a portfolio on GitHub or something. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-21 01:45:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's also my understanding that you basically cannot increase your fluid intelligence through training, and that whatever gains you can make on individual tests don't transfer.

While I don't think training on IQ tests will do much good, the science around brain training games is not so clear. It seems that many of them (almost) definitely don't work, but that some might. I recently came across BrainHQ, which is apparently [one of the few products which has been decently studied](https://www.fastcompany.com/40451692/this-is-the-only-type-of-brain-training-that-works-according-to-science). However, [another meta-analysis](https://www.theatlantic.com/science/archive/2016/10/the-weak-evidence-behind-brain-training-games/502559/) seems to lump it in with other products for which the evidence is "weak", and according to one professor of cognitive psychology claims ["we just don't know"](https://theconversation.com/are-brain-games-mostly-bs-113881). I haven't read any of the actual studies, so I don't know either, but it doesn't sound entirely debunked to me.

From hearing their chief scientist Michael Merzenich and playing a few of the games, I do think there are some reasons to believe that BrainHQ *might* work better than just training on (parts of) IQ tests though. It seems to be really aimed at speed and memory, and it's adaptive so you can be constantly in the zone of proximal development. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-20 21:59:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm going to have to ask you to stop posting these. If you generated these with an AI, it may be interesting to talk about how you did that, but I think that the output itself is more suited to a subreddit dedicated to (clickbait) videos or something. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-17 03:42:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> I mean, you've still provided a reason for telling me to go to hell: your gut feeling. So this strikes me as not so much a screed against using reasons, rather than some arguments for why we should put more credence reasons that involve gut feelings than we usually do. If it had been phrased like that, I would not take as much issue with it. Of course, "trust your gut" is fairly common advice in our society (although it's possible it needs to be reiterated and properly defended for the rationalist community). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-16 19:10:03 </DATE>
<INFO> reddit post </INFO>
<TEXT> Yeah, I was thinking about that. I think that if you say something like "Here are my reasons for thinking reasons are stupid, and that's why I believe that" you are performing a contradiction. Now, that's not exactly what the article said, and I don't know exactly what it *did* say because the author edited it to be more nuanced now, but that's more or less how I interpreted it originally.

In any case, if the author's point had been "I just know in my gut that reasons are stupid, but for those of you who disagree I guess I'll have to provide you with some (pointless from my POV) reasons to change your mind", then I agree there's no real contradiction. It still feels like *something* is wrong though, because then it seems to be aimed at causing the reader to give up on the idea of reasons by the end, *based on reasons*, but then they should presumably immediately reject that conclusion again because they arrived at it based on reasons (so it leads to a paradox?).

But in any case, I guess it's a bit moot now, because the edited article makes it clear the author thinks sometimes reasons are good and sometimes bad. I'm still not convinced (although *some* reasons are obviously bad), but it's not a performative contradiction because this could easily be one of the cases where giving reasons is the right thing to do. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-16 17:38:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> It is still not clear to me what is really the alternative to reasons. I mean, you *gave* reasons. The *reason* we shouldn't be infinitely confident in the reasons we rationally came up with, is that our knowledge and reasoning processes are imperfect. It just seems to me like you're targeting the wrong concept when you target reasons. What I think you're saying is that we shouldn't be overconfident in the reasons we can come up with, and that we shouldn't discount the (universal/conservative) reason against change which is something like "so far, we have survived without the suggested change". What's needed is not less reasons/rationality, but more and better reasons/rationality to (more) accurately predict their relative utility and prepare for the possibility that we're wrong. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-16 16:06:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> I feel that in order to avoid a performative contradiction, an article like this should just be "No need to give reasons. The end.". Instead, we get a list of reasons for hating reasons. Except not really, because that conclusion doesn't actually obtain.

Let's say that we use reasons and rationality to abolish some tradition (for which the stated reasons are wrong or unknown), and it turns out poorly. Does that mean the entire concept of reasons is bad? Or were the particular reasons themselves bad? Or, perhaps more poignantly, did we fail to sufficiently take into account the reasons that the author of this article provides?

It seems to me that there is value in tradition and epistemic humility. If you're thinking about abolishing a tradition, or institute some kind of change, assume there might be some reasons *your reasons* are wrong (too) and prepare accordingly. That doesn't mean we should just do away with reasons altogether.

I also don't get how the author imagines we should actually operate. Should we just do and believe things at random (i.e. for no reason)? Because that clearly doesn't seem feasible, practical or productive. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-16 09:41:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Maybe I’m wrong, I don’t think this is an AI application, as there isn’t really a learning component to it.

There are lots of AI applications without learning components. Machine learning is a subfield of AI.

> This is just procedural generation

Procedural content generation is often done with AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-14 13:04:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> This would be a lot more productive if you told us why you think what you think. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-13 14:29:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> What threat are you talking about?

If the threat is that we'll build artificial superintelligence (ASI) that we can't control, then the solution is probably not more that we can't control. If the threat is AI systems taking away our jobs, then the solution is probably not more AI systems taking away our jobs. If the threat is lethal autonomous weapons (LAWs) enabling easy mass murder for everyone, then the solution is probably not more LAWs enabling easy mass murder for everyone.

This is a cherry-picked and oversimplified list, and I'm not saying it's really this hopeless. Maybe the "more AI" doesn't have to be of the same kind as the threatening AI, and obviously brain augmentation is entirely different. Because of that, I'd say that having a certain threatening AI does not automatically imply that we'll also have a different kind of AI, or sufficient brain augmentation tech, to counter it. So I think we cannot categorically say that threats from AI can be solve by "just" making more AI or augmenting humans. Maybe that can work in some cases, but it's hard to talk about without a more concrete question. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-11 09:48:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are guidelines here: https://journals.ieeeauthorcenter.ieee.org/become-an-ieee-journal-author/publishing-ethics/guidelines-and-policies/post-publication-policies/ But it might be better to check with the journal you submitted to. These guidelines say you were allowed to put a preprint on arXiv but you had to mention this when you submitted. It's not clear to me what are the policies after you submitted but before acceptance.

In any case, you shouldn't have to worry about rejection because of prior work because if your peer reviewers are competent they should be able to check the dates on the different works. And if not, you can always write a rebuttal. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-10 18:40:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> I notice you say it's a similar *idea* and not a similar *paper*, i.e. with all the same graphs, data, results, etc. Is your idea so original and complex that it's highly unlikely that someone else could independently come up with something similar? If you're worried about plagiarism, you could contact the journal editors, and they can investigate (it's probably fairly clear cut if they sent your manuscript to the author(s) of this new arXiv paper). Whether you should put *your* paper on arXiv depends on the guidelines of the journal(s) you aim to publish in (take into account the possibility that the current journal might reject your paper). It may also be a good idea to save material that can exonerate *you* from potential plagiarism claims (e.g. discussions etc. you had about your paper).

I don't know the situation obviously, but I think it's not that uncommon that two people occasionally come up with similar ideas around the same time. If you don't suspect plagiarism, you don't really have to do much (except maybe arm yourself against plagiarism accusations as I mentioned). But I suppose you could take the opportunity to contact the other authors in a friendly matter and say that wow, it's so interesting, you just wrote a similar paper and submitted it to a journal two months ago. Maybe you can have more discussions about this and future work, and perhaps even work together (and I guess you could also innocently ask them where *they* got the idea from). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-09 15:12:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> Statistics, probability, linear algebra, calculus, optimization are probably the most prominent, but mathematics can help with AI in many many ways.

Whether math is more important than programming depends on what your goals are. If you want to be an "AI engineer" who uses mostly existing algorithms to build smart applications or whatever, you'll be programming a lot more than using math. If you want to be an AI/ML researcher who develops new algorithms, you might use math a lot and program relatively little.

If you're looking for what to study (e.g. in university), your interests and ideas about your career should be primary considerations. Secondary considerations might be that it seems to me like mathematics is harder to learn on your own than programming, but computer science (which teaches more than programming alone) is a more conventional path towards AI. See also the [Getting Started article](https://www.reddit.com/r/artificial/wiki/getting-started) on /r/artificial's wiki.

I think the big innovations in AI are initially going to be made in the realm of ideas, very likely with mathematics, and not in the realm of implementation (programming). However, programming is also important to get new technology to society in a way that's actually usable by many people. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-09 14:54:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> > The added difficulty for the multitask paper is mostly from the fact that two scenarios make up a question, so that there are four choices like the other questions: {Clearly Wrong, Not Clearly Wrong} x {Clearly Wrong, Not Clearly Wrong}. Hence lower accuracy in the multitask paper is certainly expected.

With a 66% accuracy rate on single binary scenarios, wouldn't you then expect a 44% (=66%\*66%) accuracy rate on the combined scenarios, rather than 26%?

Unless GPT-3 "understands" a binary question format but not the combined format, but that seems to be ruled out by the fact that it did much better on some of the other tasks, right? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-08 19:10:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> My recommendation would be to just read the book you're reading. Afterwards, you can decide what topic interests you the most, based on what you learned in the book as well as other things like AI technology you may have heard about elsewhere, and then dive deeper into those. If one of those topics is, say, machine learning or natural language processing, I would pick up a general book about those (I don't know if "ML Illuminated" and "NLP Illuminated" exists, but you get the point), and repeat the process. (Maybe then you'll realize you're most interested in neural networks or question answering, and you can find books on those, and so on.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-05 10:35:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not sure why the headline is about researchers. They quote one researcher (Mark Riedl), but he's talking about startups. I don't really think we should be all that upset that if you're going to build a company on some other company's service, you have to pay for that.

I'm not sure how many tokens are needed to let you do anything, but $100 or even $400 per month should not be all that prohibitive for a serious research project. I'm guessing the monthly costs for a PhD student at my university are around $6000, so if you have a fairly tiny project with only one PhD student, you need to apply for 7% extra funding to get the highest tier of GPT-3 for the full duration. I'm not saying that's *trivial*, but it's certainly not *prohibitive*. (It may be different for poorer countries, put they can perhaps hash out separate deals.)

I'll also point out that OpenAI is under absolutely no obligation to provide free compute to anyone. Even as a research institute to other researchers. The standard model is that you publish your research so that it's understandable enough for others to reproduce. Sometimes, but not always, researchers publish code and trained models. OpenAI did all of that (although they didn't publish their biggest model). Of course, if the research involved extreme computational resources, not everybody can easily reproduce it. In that case, the most common outcome is that the not-super-rich researchers are just shit out of luck. In this case, they can still do (some) research by spending a fraction of the money to get API access. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-03 18:56:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm sorry this may sound harsh, but it looks to me like you don't have the necessary expertise to really do this. Why don't they just ask the group who developed the system? At least they have enough expertise to develop it. You're basically being asked to know more about AI and its development than them.

If you're going to do this anyway, can you at least talk to the developers? The system might be a black box, but they will at least know what they did to develop it, test it, where they got the data from, etc. Maybe you can challenge them on the reasons for the choices that they made. Even if you're not an AI expert, you can probably tell when they don't know the answer themselves. Andrew Ng's [AI for Everyone](https://www.coursera.org/learn/ai-for-everyone) course may be useful for you "[i]f you want your organization to become better at using AI".

Testing the performance of the new way of doing things against the old way is a good idea regardless of whether the new way involves AI, but you have to do the test properly. Ideally, it should be done on data / cases / whatever that are new, or at least not the same or too related to what the AI system was trained on. For instance, if you have a computer vision system that's trained on images of people, you should not test it on those same images, but also not on different images from the same people. There are a lot of subtleties though.

If you're concerned about ethics (and you probably should be), you could also learn about it in e.g. [these guidelines](https://ec.europa.eu/futurium/en/ai-alliance-consultation#:~:text=The%20Ethics%20Guidelines%20for%20Trustworthy,strategy%20announced%20earlier%20that%20year.) to the European Commission. Then ask the developers about how they're dealing which each of the highlighted issues (off the top of my head: transparancy, fairness, responsibility, human control, privacy, safety and the environment). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-03 18:37:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> Nevertheless, the boss (and any other human) *can* be asked for their reasoning, and then they can be held responsible for it. Furthermore, we've had 100,000 years of experience with modern humans, so we have a decent idea of how they tend to fail. This is not true for AI systems, so I always find this an extremely weak excuse for black-box systems. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-03 18:32:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> Given that you're posting here, I assume you're interested in AI and if that's the case, then I think you should pick the one with the AI focus. You may be interested in the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. The article has a section on choosing a major. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-09-03 14:26:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't fight your publication battles by spamming your gripes with your reviewers all over social media. This is clear [self promotion](https://www.reddit.com/wiki/selfpromotion). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-27 18:07:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> /r/science is currently holding a discussion panel (kind of like an AMA) titled **"Racism leads to science that is biased, exclusionary, and even harmful. We’re experts on the ways racism and lack of diversity harms STEM and perpetuates inequalities - let’s discuss!"**

https://np.reddit.com/r/science/comments/ihktdt/racism_leads_to_science_that_is_biased </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-25 16:50:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Hi! Can I pm you?

Sure!

> I can’t seem to send you a private message.

Maybe with this link:

https://www.reddit.com/message/compose/?to=CyberByte </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-25 16:38:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't have much to add to /u/RecklesslyAbandoned's excellent answer, but I want to point you to the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. The article has a section on university choice, among other things. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-24 23:12:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> Is this really what you want to do, or are you trying to "simplify" your goal for us? If that's the case, I would suggest just explaining what you really want. Often when we talk about coin flips, we mean this in an abstracted mathematical sense where they essentially the definition of a completely random event, which essentially means it's completely unpredictable if you know it's a fair coin. If it's a weighted coin, you can use some standard statistics to predict the weighting.

If you're not talking about an abstract mathematical coin flip, but a real one, the question becomes: what observations will be available to the AI/machine learning algorithm? And what is the relation between those and the outcome? For instance, if you have a very constant flipper, observing if the coin starts face up or down may allow for a very accurate prediction, but if the flipper is inconsistent this may not convey enough information. Maybe a video of most of the coin's trajectory would be more generally informative, but also more difficult to work with. In any case, there are different algorithms that can work with different kinds of data, but you'll probably be doing some kind of supervised learning. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-24 16:50:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> Deep Learning is machine learning with deep neural networks, so learning about simple neural networks is a good precursor. Neural networks, especially the simple ones from before deep learning, are often essentially "stacked logistic regression".

So I think it's a good idea to learn about the concept of machine learning first (specifically supervised learning), because that's essentially what you're trying to accomplish. This will involve some basic concepts like how to split up your data sets, bias, variance, overfitting, regularization, performance measures and optimization algorithms like gradient descent. These can then first be illustrated with simple algorithms like linear regression, then logistic regression, and then simple feedforward neural networks (which as I mentioned are just stacks of logistic regression). From there, you can then learn about different neural network architectures, including deep ones.

I would recommend Andrew Ng's Introduction to ML course, as it does essentially the first part of this. There should be a link somewhere in the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. To move on to deep learning, there are then many other courses/resources, including Ng's deeplearning.ai (and Deep Learning Specialization on Coursera) and Jeremy Howard's fast.ai, to name only a few. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-24 11:35:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't have a list of colleges, but maybe the [Getting Started article](https://www.reddit.com/r/artificial/wiki/getting-started) on /r/artificial's wiki can be helpful; especially the section on AGI. If you're interested in AGI, you could follow some of the links there and find professors who work on it (e.g. on the committees of the AGI conference and journal). A lot of them are not at the most prestigious universities. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-22 10:37:03 </DATE>
<INFO> reddit post </INFO>
<TEXT> As others have pointed out, Musk isn't afraid of narrow AI (like self-driving cars) but of AGI.

OpenAI and Neuralink are his main responses to this fear. OpenAI's mission is to ensure AGI benefits all of humanity, either by building safe AGI themselves or aiding others to do so. The idea of Neuralink is that instead of building AGI from scratch, which may be difficult to align with human values and make compatible with human society, we achieve super(human) intelligence by augmenting human intelligence and perhaps, in effect, "merging" with the machines. Possibly this increased augmented intelligence will also help us solve the AGI control problem (although I should also say that it could also make us smart enough to create AGI without necessarily making us smart enough to make it safe). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-22 10:31:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Creating OpenAI is the real head scratcher.

From [OpenAI's charter](https://openai.com/charter/):

> OpenAI’s mission is to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity. We will attempt to directly build safe and beneficial AGI, but will also consider our mission fulfilled if our work aids others to achieve this outcome.

If you're afraid of unsafe AI, then trying to build safe AGI (or helping others do so) seems like one of only a few possible broad strategies. The other one would be to try to prevent anyone in the world from creating unsafe AGI, but good luck with that.

> creating an API for them to bypass all the groundwork would be the last thing I’d do

They're not doing that? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-19 12:25:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> As always, it will depend on the details of the AI. However, we can probably say some useful things in general. First of all, artificial general intelligence (AGI) / artificial superintelligence (ASI) should be capable of learning and possibly some sort of evolution, and what they learn / how they evolve will depend on the environment they're in. That environment also depends on what other AI systems exist, and how they work. For instance, there's an idea that if one ASI can get a "decisive strategic advantage" it could become sort of a tyrant that can do whatever it wants unthreatened. But if there are many ASIs that are not exponentially getting more intelligent, some form of coalition may need to be formed.

Another potentially useful thing to know about is the distinction between intended, terminal, instrumental values/goals. Instrumental values are subgoals that should be achieved in order to achieve some higher-order goal/value(s). Terminal values are the highest-order values (i.e. as far as the entity is concerned, they require no further justification), and they're typically programmed in somehow (implicitly or explicitly).

It's not really clear what a human's terminal values are (something like eudaimonia or stimulating some brain area or something), but they're often confused for "survival and reproduction" or something like that. However, while nature/evolution may have "intended" to give us these values, it instead gave us (and other animals) a bunch of other drives, behaviors, emotions, etc. such that when we follow those it tends to result in survival and reproduction (e.g. sex tends to feel good and life-threatening damage tends to feel bad). Even knowing that our "purpose" is survival and reproduction, many humans obviously reject this as their goals. This is important to keep in mind for AI, because if you intend to program in a particular goal, the AI is going to pursue the goal you programmed in the best way it can think of, and not give a shit about what you *intended* to program.

Another interesting thing is the likely existence of instrumental convergence / Omohundro drives. These are instrumental goals that serve many/most possible terminal goals. Examples (likely) include things like survival, seeking knowledge and power, and protecting current terminal goals from change. Some people argue that some of these are so important that evolutionary pressures will result in ASI systems that pursue these to the exclusion of all else, regardless of what the "real" terminal goals are. A procrastination-related argument for this is that the system will forever intend to start making paperclips/stamps/whatever (i.e. pursuing the terminal goal), but never get around to it because it's always gathering more power because that will increase its ability to achieve goals and survive (even in a fight against another ASI). That's an argument for homogeneity of AI systems, even in different situations, but this is not a universally accepted argument.

Assuming the /r/ControlProblem is solved, different people/societies can presumably make AIs with (slightly) different terminal values (either through explicit programming or something like value learning), and they might differ in that way. However, it could also be the case that the first AIs prevent the creation of other (possibly threatening) AI systems with different goals, in which case they'd all have the same terminal goals. (But then they might still learn different things/strategies in different environments.)

Edit: I forgot to mention that you may be interested in Max Tegmark's [12 AI aftermath scenarios](https://futureoflife.org/ai-aftermath-scenarios/) from his book Life 3.0. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-17 11:29:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's the link to the [top-level post about this](https://www.reddit.com/r/slatestarcodex/comments/i0txpk/central_gpt3_discussion_thread/g09vjb3/) in case people have trouble finding it. It also seems that the person who made it is the one who posted about it there (Porrster). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-17 10:47:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your post got filtered because it was less than a day old at the time of posting, and no mod was available over the weekend to approve it (sorry!). That means it has only become visible now, and it has already dropped off the front page. If you want to re-post it to possibly get a few more replies, you have my permission (I'm a mod). Maybe you could also include some extra information (see below).

---

First of all, I'd just like to say: Way to go! It takes courage to start a second career and go back to school, even if you're still pretty young.

It sounds to me like you're on the right track. I always wish I knew more mathematics. You also seem to have realized that math alone is not enough and that you should learn to program (Python is a great language for AI), and take as many AI-relevant courses as possible. You can also supplement this with (free) online courses (see e.g. the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki).

The best choices of course depend on your goals. You mention your "main goal", but I can't really tell what it is. You also ask about advice from a "professional/non-academic perspective", but then you mention that you mainly like "academic and research stuff". So what is it that you really want?

Probably most jobs "in AI" involve implementing and applying existing algorithms, perhaps with a bit of "freedom" in setting up the actual model to use (e.g. how many and what kind of layers in a neural network). I think mathematics is probably most useful for research and (new) algorithm development though. So in that case, it might make more sense to target an academic career or one of the (big) industrial research labs. Of course, there will also be statistics and econometrics jobs, but I suspect that the degree to which they have much to do with AI will be inversely related to how much math there actually is (e.g. "data science" is closely related to statistics and AI, but you'll probably be more busy working with the data, applying models, etc. than doing any actual math). But I could be wrong about that. Another option that your financial background may help with is business intelligence.

So I think it would help to get a slightly clearer idea of what you're aiming for, and if e.g. you'd like to get a PhD. I think math is a good background, but at some point you'll probably want to really bend in the direction of AI. If there are professors at your university/college who do research in this direction, you could try working with them (and/or taking their classes). Perhaps as a research or teaching assistant. I'm guessing you'll also have to write a bachelor and/or master thesis? If so, do it with one of those professors on an AI-related topic.

Mastering Python while working as a dev while finishing your studies may work. There are definitely worse things you could do. But one thing to ask is whether the career you want involves a lot of programming. Some AI careers definitely do (e.g. I worked at a computer vision R&D company and was pretty much programming the whole time), but others are not as programming heavy (e.g. my PhD/academic research). You definitely need to know how to program, and it never hurts to be very good at it, but if you're e.g. aiming for a research career, a better side job might be research assistant for an AI professor. Or maybe just spending more time on AI projects and learning.

Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-14 11:02:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> As I understand it, Griffin is GPT-2 and [Dragon is some mix of GPT-2 and GPT-3](https://www.reddit.com/r/ControlProblem/comments/i2l62n/beware_ai_dungeons_acknowledged_the_use_of_gpt2/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-13 14:42:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's hard to predict things, but I think it's more probable that OpenAI is doing good than bad in this regard.

Everybody is super impressed with the GPT family, but algorithmicly/architecturally it's not that special. GPT-2 and GPT-3 were basically just scaling up previous versions. The main contribution to AGI is providing evidence for the scaling hypothesis. But if that hypothesis is true, then AGI was basically already invented, and not by OpenAI. It could be argued that by providing this evidence, OpenAI is enticing others to also scale up their architectures, which may then result in actual (super)human AGI. However, I think this is a minor effect at best, and it's not like others would *never* have thought of scaling up their architectures.

As architectures go, GPT also seems less obviously dangerous than e.g. a reinforcement learning based architecture. At the same time, GPT-3's success *does* seem to communicate to a lot of people that timelines might be shorter than previously thought, making work on AI Safety more urgent.

Doing high-profile capabilities research also increases OpenAI's credibility in the eyes of many AI/ML researchers. Many don't take Bostrom (philosopher) or Yudkowsky (autodidact) seriously because they're not "real" AI researchers with lots of accomplishments. OpenAI cannot be so easily dismissed, and I think it's very valuable to have such more credible voices talking about AI safety. Furthermore, OpenAI's success also makes it a place where many AI/ML researchers want to work, which then triggers some of the effects Vanifer mentions at the end of their comment on LW. Capabilities researchers at OpenAI would otherwise be doing capabilities research elsewhere without the "supervision" of an AI Safety team, while at OpenAI they might have to at least think about safety and possibly integrate it into their systems. They'll furthermore be in an environment where they might be convinced of the importance of AI Safety. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-13 14:23:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> > How much did you find the PhD worth it?

Pursuing the PhD gave me time to learn about AGI, guidance from an expert in the field (and his contacts), a network of other AGI researchers, a number of publications and years of practice with conducting research. This sounds fairly ideal to me (even though pursuing a PhD is hard and it wasn't a happy time for me).

I think you could get some of these benefits in other ways, e.g. in industry, but I think it's far less likely. But as a (successful) PhD student these benefits are virtually guaranteed, and if you're lucky you'll also have a research group of people researching similar things. I'd say there's also relatively little bullshit: you may be forced to take some courses (but if you're lucky you can choose relevant ones or get a waiver because you'll already have a MSc), and perhaps to teach a bit, but you can spend most time developing yourself and your own research.

> I am currently pursuing a MSc in Computational Perception & Robotics, but, honestly, I am just pursuing it for enhancing my CV, not to learn; I believe that formal education is becoming irrelevant since the establishment of many online learning resources. That's why I am not really planning to take a PhD afterwards.

I'm not so sure everything can be learned as well online as in person at a university, but that depends on the material and the person. However, I don't think a PhD is anything like a MSc (which I also have). I actually think "PhD student" is a misnomer, because in my experience you're more likely to be a teacher than a student and your main job is to be a researcher. Your PhD program may force you to take a few courses, but this shouldn't take up most of your time.

> Surely, having a mentor that can guide you is invaluable, but it feels that doing a PhD just for that is an overkill. What do you think?

I guess it depends on how invaluable you think a mentor is (plus other benefits of a PhD), what the personal cost of getting a PhD would be for you (not just in terms of money), and what your alternatives are. Since you're currently doing a MSc I'd guess you're in a phase of life that's relatively suitable for doing a PhD (i.e. you're probably young, not used to a huge income you'd have to give up and probably don't have a family to support that ties you to a specific place). But obviously I can't speak for you. I think that for *me* it would have been really hard to become an AGI researcher on my own in my spare time. I might have managed it though if I could work for an AGI company (like OpenCog), so that might be a decent alternative to investigate. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-13 12:36:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think it depends on what you want and how confident you are about your own ideas. Before I started my PhD I was (already) very interested in AGI, but I had no idea how to approach it and wanted to spend a few years learning about different approaches to the problem under the guidance of an expert. Hence the PhD.

Similarly, if you work at a company that pursues AGI, you can learn from the people there and you can become an expert on that particular company's approach and its strengths and weaknesses will likely inform your own ideas.

Of course, if you have very good ideas of your own, or (perhaps) you're a very good autodidact, that may also work.

Getting a good job and then pursuing a hobby on the side is indeed low-risk on the dimension of earning a living, but I think it's relatively high-risk on the dimension of not getting around to doing any high-quality AGI research. (This could perhaps be mitigated by becoming involved with some open source project, because then you still have some of the advantages I talked about above.) So I think that a lower risk approach is to try to get some kind of AGI job. Of course, while you're looking for that job, you should ideally have some income and you can already get started on your AGI career on the side. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-13 10:33:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are some links in the [Getting Started with AGI section](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on /r/artificial's wiki that may help.

The number of jobs in AGI is probably less than 1% of the total jobs in AI, especially if you exclude DeepMind and OpenAI which are probably the largest employers in this area. If you then also exclude universities and institutions working on AGI safety, only a handful of companies remains, and you'll have to be careful not to run afoul of total crackpots.

On the positive side, I think that outside of DeepMind and OpenAI there's also not a lot of competition for whatever AGI jobs remain. I would recommend looking at the people involved in the AGI Society, AGI conferences and AGI journal (links are on the wiki) and looking at where they work. Unfortunately a publication in the AGI conference/journal doesn't really guarantee that that's the focus of their workplace / company / research group, but it's a good starting point.

If you can afford to have a fairly low income for a few years, I might consider doing a PhD. It's basically the same as being a fairly junior researcher (although you may work with undergrad and master students who are more junior than you, but this depends a lot on where you do it). Even if you don't stay in a university career, a PhD will make you more desirable for a lot of AI/ML/AGI jobs. You'll also have to publish and present your work at conferences, which will help build your network.

/u/1000000000-999999999 mentioned OpenCog, which could be one of your potential non-academic targets (although I think in the past they've worked with Hong Kong Polytechnic University to have PhD students). You can just be a volunteer contributor to their open source code, but there are also definitely people who are getting paid to work on OpenCog. I think there are some different constructions with Hanson Robotics and SingularityNET, but I'm not really sure. If you want to know more, you should just contact Ben Goertzel or the OpenCog mailinglist and inquire about possibilities.

(Tagging /u/Dahvrok because they were interested too.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-11 20:22:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Simulating evolution over billions of years would be extremely inefficient to try and produce intelligence

Yes, that's what I tried to explain.

> Would it not make more sense to continuously build AI to do more incredible and a wider range of tasks then eventually have them able to program themselves.

Yes, that's at least not (as) *clearly* infeasible. There's a bit of an (open) question whether we should expect continually small steps in fairly disjointed subfields of AI to eventually lead to AGI (or self-programming AI). An alternative might be to aim for it directly in one big step, like most people in the AGI community are doing (see [here](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F)  for some links).

> I always thought that was what the singularity was

Yeah, that seems about right. I think it depends a bit on who you ask. The [Wikipedia article](https://en.wikipedia.org/wiki/Technological_singularity) on it focuses heavily on the intelligence explosion version, which might be caused by recursive self-improvement (i.e. the AI reprogramming itself). Sometimes people also equate the advent of human-level general intelligence with the singularity, either because they think it will quickly lead to such an intelligence explosion or because they think that even if it doesn't we can't really predict the future much beyond that point (so there would be "unforeseeable changes to human civilization" as per Wikipedia's definition). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-11 17:53:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> As I mentioned, a grain of sand apparently contains 43 quintillion atoms. That's 43*10^(18). So if you simulate a million atoms, you get something like a trillionth of a grain of sand. Also, as another point of reference: humans have about 10^11 neurons in our brains.

So I think you'd have to simulate *many* more atoms than that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-11 17:31:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are definitely physics simulators, used in e.g. [computational physics](https://en.wikipedia.org/wiki/Computational_physics). However, these things tend to be very computationally intensive, especially when simulating things at the atomic level, even when simulating relatively small amounts of particles. In particular, it's going to be orders of magnitude slower than the real thing, and require many more atoms/molecules (since computers are also made out of them).

So you definitely can't simulate the 10^80 atoms involved in the Big Bang individually (1 grain of sand apparently has 43 quintillion atoms, so I don't think a billion would get you anywhere). And if you could, it would take longer than the 13.8 billion years it took to go from the Big Bang to human intelligence. Except you might not even get there, because it might depend on the initial configuration of atoms (which we don't know), interactions at the subatomic level, or (quantum)physics we don't know about yet.

People have suggested getting AGI by "just" using evolution, which we might think of as skipping ahead 10 billion years and not needing to simulate the whole universe. But that too is probably not feasible, at least not in the sense of "mimicking nature", because you'd need to simulate some Earth-like environment in high fidelity for a long time, and 1) we would probably get it wrong, and 2) it would still be too computationally expensive.

So unfortunately, I don't think this is the way to do it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-11 10:25:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> Whenever I hear people talk about this, I just want to ask them if they've ever heard of the AGI community and if they can e.g. name 3 key figures within it. I'm not saying that community has all the answers, but if you can't do that, then I don't believe you've been seriously looking at AGI and testimonies like "I haven't seen anything that could work" aren't worth shit.

And in my experience, lots of AI researchers haven't even heard of it. They're just working in their corner of narrow AI and assume someone would come up to tell them about it if there was work on AGI that's entirely outside of their narrow subfield. But of course that doesn't happen. They often only know about things that are adjacent to their own subfield and about things that are super hyped up (like deep learning). Ravikant for instance seems to only know about deep learning attempts at AGI.

And while I think he *might* be correct that the correct level of analysis for brains might not be the neuron level, he's 1) way too certain about it, and 2) that might not even be relevant because maybe we could just do better than nature (at least in some respects). We have more biologically plausible spiking neural networks, but they don't work nearly as well as our non-spiking ones. Maybe it's just a good abstraction. Note that we also built airplanes before we knew how birds flew and that our vehicles go faster than any other animal. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-07 14:14:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> [Researchers say ‘The Whiteness of AI’ in pop culture erases people of color](https://venturebeat.com/2020/08/06/researchers-say-the-whiteness-of-ai-in-pop-culture-erases-people-of-color/)

> “We argue that AI racialized as White allows for a full erasure of people of color from the White utopian imagery,” reads the paper titled “[The Whiteness of AI](https://link.springer.com/article/10.1007/s13347-020-00415-6),” which was accepted for publication by the journal *[Philosophy and Technology](https://www.springer.com/journal/13347)*. “The power of Whiteness’s signs and symbols lies to a large extent in their going unnoticed and unquestioned, concealed by the myth of color-blindness. As scholars such as Jessie Daniels and Safiya Noble have noted, this myth of color-blindness is particularly prevalent in Silicon Valley and surrounding tech culture, where it serves to inhibit serious interrogation of racial framing.” </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-07 13:48:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know which degree has better job prospects, but I think both are very good, so I'm inclined to recommend choosing whatever you're the most interested in. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-07 10:05:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> The short answer is "no, there's no public prompt", you have to request an API key from OpenAI. There's sort of a workaround to use AI Dungeon as WashiBurr points out (make sure to actually set the model to Dragon!). However, [there may be some issues with that](https://www.reddit.com/r/ControlProblem/comments/i2l62n/beware_ai_dungeons_acknowledged_the_use_of_gpt2/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-06 12:12:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> > the Internet Mob Accountability Project

I share your concerns here, but I'm afraid this would backfire spectacularly. In addition to submitting a "Statement of Diversity" for a job, you will now also have to have signed N anti-racist petitions and gotten M sexists fired. And yeah, maybe there will be companies that take the opposite approach, but that just seems like it will result in more division in society.

I'm not really sure what *can* be done, but I think the solution should be sought in the direction of making "cancelling" less effective. I've always found it vaguely odd when companies fire mobbed employees for PR reasons, because not standing by and protecting your employees seems like much worse PR to me. But I guess most people disagree, and I'm not sure it's exactly ethical to use my hypothetical billions of dollars to indoctrinate my ideas about this matter on society (if that is even possible). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-06 12:03:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> > If you can find a successful person who is physically weak, I think Rippentoe would say that the same person would be happier and more confident and probably even more successful if he were strong.

I don't see any downsides to being physically strong(er), but the same goes for being more intelligent, knowledgeable, charismatic, good looking, etc. So that's not an argument for why physical strength is the most important thing in life.

A bigger issue in my opinion is that while *being* physically stronger might not have any downsides, *getting* physically stronger takes time which you then can't spend on doing other things. So I don't really buy that generally speaking, this hypothetical successful person would necessarily be more successful if they had spent more time in the gym and consequently less time on the things they actually did do that, as a matter of fact, made them successful. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-06 10:53:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't think it's used much, and I haven't heard much about it since, but in 2013 Alexander Wissner-Gross presented "a new equation for intelligence" ([TED talk](https://www.ted.com/talks/alex_wissner_gross_a_new_equation_for_intelligence?language=en), [paper PDF](https://alexwg.org/publications/PhysRevLett_110-168702.pdf)): F = T ∇ Sτ. Intelligence is a force (F) that acts so as to maximize future freedom of action, or keep options open, with some strength (or temperature) T, with the diversity of possible accessible futures (or entropy) up to some future time horizon Sτ. The ∇ is the symbol for the gradient, so another way of putting this is that F is proportional to the gradient/derivative of entropy.

Wissner-Gross regards this as a grand unified theory of intelligence, but to me it looks more like a goal you might plug into an AGI than a recipe or framework for making one. Then again, I don't really think AIXI is a recipe/framework either; it's more of a definition of (maximal) universal intelligence. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-06 10:22:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> AIXI is based on Solomonoff induction though. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-05 13:39:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's not going to be exhaustive because not all papers have code, but I think Papers With Code's [overview of the state-of-the-art on many benchmarks](https://paperswithcode.com/sota) might be valuable. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-04 10:10:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://en.wikipedia.org/wiki/Intelligent_tutoring_system </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-03 15:20:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> At the 2016 AGI conference there was a [workshop](http://agi-conf.org/2016/workshops/) on whether deep learning is suitable for AGI. Most researchers argued some version of "no", and that's basically also where I'm coming from. However, I also really enjoyed Cosmo Harrigan's talk on what NNs *had* already been able to accomplish and I've been continually impressed with what has happened since, so who knows.

Alexey Potapov and Ben Goertzel talked about their taxonomies of (alternative) approaches in [tutorials at the 2017 conference](http://agi-conf.org/2017/?page_id=24) (unfortunately I don't think there are videos), and I'll also just link the [Getting Started with AGI section](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on /r/artificial's wiki.

> Could you feasibly hard-code cognitive skills into a system, sidestepping the "learning" process in current machine learning systems?

For human intelligence, we might say there are two periods of learning: 1) postnatally: during the person's life, 2) prenatally: during the evolution of the human brain. If you build a system that can't learn during its "life", then I think it's not AGI by definition, so I don't think that can be "sidestepped".

Evolution has endowed us with brains that already have all kinds of capabilities (in what I called the prenatal learning stage). To some degree, all AI/ML researchers are trying to sidestep a part of this: nobody's AI starts as a single-celled "organism" (or we could even imagine starting *before* organisms even existed). Even neural networks have quite a bit of structure and mechanism hardcoded into them. And we can see that by putting in more structure, as with convolutional layers, LSTM or transformers, we can get even better performance. Potentially, a fully connected network *should* be able to learn the same as one with convolutional layers, but in practice this is a lot harder.

With systems like AlphaGo, the researchers provided (what seems like) even more structure: the system is not just a (convolutional) neural net, but it has a tree search algorithm in it. In fact, we could say that AlphaGo is an implementation of Monte Carlo tree search (MCTS) where a few NNs are used as heuristics. What happened here is that the creators knew that a certain kind of functionality was necessary (tree search), they knew how to explicitly program that, and they could more easily do that than a NN could learn it.

If you take this idea much further, you get cognitive architectures (see above links and e.g. [this survey](https://arxiv.org/abs/1610.08602)). I think these are probably more promising than pure neural networks as an avenue to AGI, because they better allow human experts to encode their ideas about how intelligence should work.

> If so, should we be more worried about these types of systems because they become capable immediately and so any type of misaligned goal would be unlikely to get picked up before killing everyone (or worse).

An interesting question to ask here is what developmental stage an AI/AGI would have to reach before we can tell if its goals are misaligned. I know AGI won't have to be humanlike, but allow me to use some anthropocentric terms anyway. I'm not sure you could tell if a baby-level intelligence is misaligned for instance, because it would basically be unable to do anything. I also doubt that you could tell with a toddler-level intelligence. In fact, *human* toddlers probably *are* super misaligned because they lack the theory of mind to think about anyone other than themselves.

I also don't think AI builders would be able to skip much of the postnatal learning stage, because the knowledge humans obtain at that point would quickly become way too much to encode. So a cognitive architecture approach may give you a bit of a jump start on a neural network that learns from scratch, but I question if you could tell much about the neural network in the period where it's "catching up" to the cognitive architecture.

Furthermore, the advantage I mentioned about cognitive architectures, namely that they allow us to encode our own ideas into the system more easily, might be an advantage here too. Neural networks are notoriously difficult to interpret/comprehend, so who knows what they're learning? Cognitive architectures might be much more explainable/interpretable. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-03 11:58:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Most AGI systems are modeled on human intelligence, but human beings aren't very intelligent as individuals.

If it's indeed the case that humans are less intelligent than we thought, then creating AGI should just be easier than we thought.

> If you left group of humans in the woods they wouldn't start researching cancer and re-create New York, unless they had thousands of years.

Does that mean they're not intelligent? I wouldn't say that.

> Humans become intelligent because they get all of the functional discoveries of the species that came before them from the practices and habits of society.

I disagree that getting that knowledge is what makes humans worthy of the label "intelligent". I mean, what knowledge? If you say getting today's knowledge is a requirement, then nobody born before today would be "intelligent". Similarly, if "researching cancer" or "re-building New York" are your criteria for intelligence, then 99% of people alive today and 100% of people alive 100+ years ago don't qualify.

I'd say that this passed down knowledge is super useful, and *getting* it perhaps makes someone *more* intelligent (or actually more knowledgeable). It may be a requirement for properly functioning in a society where everybody else has the same (or very similar) cultural knowledge. But I think the real requirement on intelligence is the ability to *make use* of such knowledge.

> We need to model social systems with functional sub-units interacting in order to achieve AGI.

I'm not so sure about that. I would classify a "baby" system as AGI if it's capable of learning these things, even before it actually learned. What's arguably needed to create adult-level AGI is to endow the system with the knowledge that (current?) humans also have. That could happen in different ways, including programming (unlikely), independent learning and/or learning from humans / human teachers. Federated learning may work as a form of independent learning, but it's not clear to me that an extensive social system needs to be created for the individual learning agents (rather than an automated way to integrate what they learned).

> Not a highly effective neural net, but a system of them, which I think Marvin Minsky was suggesting in Society of the Mind.

One thing to ask might be what the advantages are of groups and cultural transmission over an individual, and whether those also apply to AI.

There's a theory that says an individual human isn't much more intelligent than a chimp, but that humanity's advantage comes from its ability to culturally transmit knowledge. This means (roughly speaking) that every chimp has to learn everything from scratch, and when s/he dies all that knowledge is lost, but that the human will pass on his/her knowledge to the next generation so they don't have to start from scratch. Which is all great, but this seems to mainly be an advantage due to limited lifespans.

An obvious advantage of a group over an individual is more brainpower. However, it's not typically the case that 10 people are 10x more intelligent than one person. There's gonna be a lot of redundancy in processing and overlap in ideas. Still, this is the best we can do in humans, because we can't just give one human a 10x larger brain. But we could do that with AI. (This won't necessarily work much better, but it's something to think about.)

There are undoubtedly other advantages that I haven't discussed. I'm just saying they should be similarly evaluated. The same goes for disadvantages (e.g. some of the overhead in human communication might be avoided with better interfaces between the different artificial agents).

My impression is that none of this should be necessary. That you could create an AGI "baby" that could learn from humans (just like human babies can), plus possibly/likely in other ways, to get to something like "contemporary adult"-level intelligence (and beyond). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-03 11:15:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> They're basically completely separate. Google and DeepMind are both subsidiaries of Alphabet. DeepMind was its own company aimed at achieving AGI. When it got acquired by Alphabet, very little seems to have changed, except that they obviously got a lot more resources. Google Brain is a team within Google Research that focuses on machine learning projects "across different time horizons and levels of risk".

According to [this 2017 AMA](https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/dmskjht/) DeepMind and GBrain are quite similar. And although they're separate, there are quite a few collaborations. My impression is that DeepMind has more of a focus on AGI though, whereas Google Brain is (also) more focused on contemporary applications.

I'm curious what you're referring to with GBrain's "whole new paradigm". My impression is that DeepMind is more innovative. Sure, they're mostly using deep learning and RL, but it's not like RL was already working very well before DeepMind started working on it. And maybe I'm also expecting Shane Legg to (less publicly) continue his research on universal AI (with a lot of the former students DeepMind plucked from Marcus Hutter's lab). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-03 10:38:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> I agree that GPT-3 isn't AGI and is probably being a bit overhyped in that regard.

> So when will we get to real general artificial intelligence? Probably never. Because we're chasing a cloud

I disagree. We're chasing something real, namely (super)human-level (general) intelligence. *That* is the real goal post. There might be some moving of goal posts, but it's not when the public correctly recognizes that some particular system isn't AGI/HLAI. It's when people (often researchers) incorrectly declare that theorem proving / chess / Go / language generation are AI-complete.

I think people that make statements like the above are assuming that humanity's AGI-classifier just always returns "no" no matter the input, and that it will continue to do so in the future. But the way I see it, is that humanity's AGI-classifier has so far always given the *correct* answer, which is obviously "no" because none of our AI systems have the same (or better) cognitive capabilities than humans across the board. Viewed from that perspective, there isn't much reason to assume we couldn't recognize AGI when it's actually created.

> But there is almost certainly an astronomical number of potential "cognitive" problems we have no strategies for, have not encountered, and which our brain-hardware might be very bad at. We are not generally intelligent.

We are not *universally* intelligent, but most of the time "general intelligence" is defined to include human intelligence, in recognition of the fact that it is a lot more general than what our machines currently have. If this whole article intends for "general" to mean the same as "universal", then fair enough, I guess. But then it should be pointed out that nobody actually thinks that's an attainable goal anyway, and AGI researchers are actually aiming at something like AI with (super)human intelligence for "humanlike" situations/tasks. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-08-01 13:11:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> Two other hypotheses (not mutually exclusive):

1. Women lower their standards when they get older. The idea is that many spend their twenties competing for the "best men". Only a few will actually settle down with these men. The rest will eventually lower their standards to find a mate (especially when their biological clock really starts ticking).

2. Men get more attractive later in life. This could be viewed as self-improvement and there might be some of that (for many incels the only way to go is up), but I think a part of this is also independent of changes in personality and (social) skills. Just being further in your career and having a (nice) house, car, etc. can make you more attractive. Plus, I think that many women actually prefer (slightly) older men to "boys" in their twenties.

This is of course still a very good thing for incels, because there might be a way out. But that doesn't mean that there's necessarily much they can do right now to save their twenties (which many feel should be one of the most fun, formative and sexually experimental decades of one's life) from inceldom.

(Of course, that doesn't mean they shouldn't try, because self-improvement and trying to ask women out is not exactly bad for you even if it doesn't directly have the targeted outcomes.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-31 15:48:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I think you'd appreciate Bender & Koller 2020 and their hyperintelligent octopus

I'm not OP, but I did appreciate reading it (at least paragraphs 1, 3 and 4). Unfortunately, it looks to me like their thought experiment begs the question: they immediately assume that because the octopus doesn't inhabit the same world as people A and B, and can't observe the same objects they're talking about, that it couldn't pick out those objects. I don't necessarily think that's implausible, but it's basically exactly their conclusion as well, so it doesn't prove anything.

I think the problem here is that while the authors define *meaning*, but not *learning*. They talk about how the octopus is amazing at statistical inference, but don't contemplate the mechanism. Machine learning is often defined as a search through a space of hypothesis models. Imagine for a second that this search space (for some reason) only includes two models: e.g. a simulation of person B (who does understand English) and e.g. a model of Chinese person C (or something else). In this case, it should be pretty easy to decide based on the exchanged tokens that B is the model that should be selected, which would mean that the AI/octopus has learned to understand real meaning.

This seems to prove to me that, at least in theory, we can come up with an AI system or octopus that could learn meaning from form. Of course, in reality the hypothesis space is much larger, and the question is if it would be feasible to select / stumble upon a correct model given some amount of data. Clearly if the space is larger and contains more similar (but still wrong) models, more data will be needed. We might then also ask if, for any amount of data, a correct model is the most likely to be picked. This obviously depends on the specific learning algorithm and its inductive biases, but I don't think you could say that this is *impossible*. Certainly all of the data we get should be consistent with model B, so the more we get, the smaller the set of (still) consistent models gets until at some point B should be the most attractive one left (e.g. the simplest one, if there's a simplicity bias). I'm not sure this will *always* be true, but I also don't think it would *never* be. (Of course for particular learning systems like the GPTs, it could be the case that no correct models are in the hypothesis space, but there's no reason to assume this for the whole Turing complete space of language models learners.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-31 12:21:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> Yeah that sounds like a more direct comparison (although I have no idea about readership numbers). Nice! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-31 12:16:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> CGP Grey has 4.35 million subscribers. How many people read SSC? It's probably not a great proxy, but the subreddit has 35k subscribers. I also wonder if more/less people would pay more/less money for YouTubers than for bloggers, but I guess Scott's audience is relatively mature and well-paid so that might be a point in his favor. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-31 10:16:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't really think I can *actually* introspect how I parse a sentence like that, any more than I could tell you how I recognize a dog or cat when I see them. I suppose I could speculate about how this sentence should be parsed though with my limited knowledge of the rules of English and natural language processing (NLP).

First of all, I imagine that if there's a question mark, you might look for an interrogative word/phrase ("what", "how", "why", etc.) to figure out what kind of answer is expected (there won't always be one of course). In this case, the interrogative might be "how many" rather than "how" though (it's actually more of a "what (amount)" question than a "how" question that asks for an explanation or mechanism).

I don't know why your AI is tagging "how" as the subject, because it's not a noun. However, I can imagine why your parser misses the subject/object, because the object isn't in the phrase "how many does each student get". I guess your parser would have to know that "how many" is typically followed by the object *or* it's omitted and you should look for it in the previous sentence/phrase. I vaguely recall that that's not a very easy problem from my NLP classes.

I guess I would recommend looking for a good open source parser, because I suspect it might be better than what you have and save you from hard-coding all these rules (and reinventing the wheel). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-30 09:55:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://en.wikipedia.org/wiki/Cyborg#Actual_cyborgization_attempts </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-28 14:14:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> Here's the [actual paper](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20\)30159-X/fulltext). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-28 14:13:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> From the [actual paper](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20\)30159-X/fulltext):

> The algorithm that we developed, whose core technology is based on multilayered convolutional neural networks (CNNs) that were specifically designed for image classification tasks, analyses a whole slide image in three consecutive steps: tissue detection, classification, and slide-level analysis. Briefly, the first step uses a Gradient Boosting classifier, trained on thousands of image patches, to distinguish between tissue and background areas within the slide. After this, an ensemble of three CNN-based models is run on all tissue areas. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-28 10:28:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think there are people who would consider the end of the biological human race as a tragedy, no matter what replaced us. I'm not one of those people, but I do think it matters *what* replaces us. If we know that a meteor is coming to wipe us out, or some alien civilization, we should probably do all we can to stop it. Even worse would be an unaligned, power-hungry artificial superintelligence that "eats" everything in its path. I certainly wouldn't want to unleash *that* on the universe.

But if we could somehow make humans X% more compassionate and/or ethical, then I think that'd be nice. And I don't really care if the resulting "new" human race is carbon-based or silicon-based. Some form of whole-brain emulation / uploading may achieve that, although it's not necessarily clear to me what the consequences would be. If you make humans more-or-less immortal in this way, and easy to replicate, back-up and speed up (or slow down), what will that do to them psychologically and sociologically? Something like Robin Hanson's Age of Em doesn't exactly sound utopian to me. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-28 10:01:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> Definitely not. It's nice to have some rudimentary programming skills, because actually programming the AI/ML algorithms you learn about helps you understand them, but even that is not mandatory to start. For most work in AI/ML you will eventually need to learn to program, but you don't have to start with that. Just dive in with some introductory courses or books.

See our wiki's [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-27 19:40:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> When you say that "predictive learning" (PL) is the new term for "unsupervised learning" (UL), it implies that 1) everything that used to be an instance of UL is also an instance of PL, and 2) everything that's not an instance of UL is also not an instance of PL. "Predictive learning" fails on both accounts: 1) dimensionality reduction is UL but not predictive, and 2) supervised learning is clearly not UL, but it *is* about predicting the output from the input.

The only thing in the term's favor is that does have *some* overlap with UL. For instance, in the case of predicting what it will see next, e.g. when it opens a drawer. But that doesn't mean PL is a good term to replace UL. I just don't really see a use case for it, because saying your drawer prediction AI is a "predictive learner" doesn't tell me nearly as much as saying whether it's supervised or unsupervised. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-27 14:56:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is a terrible idea, and it seems that even Yann Lecun, who seems to have come up with the idea, has switched to calling it self-supervised learning.

Why is it a terrible idea? Well, for one thing, because learning to predict stuff is typically done with supervised learning. It is true that there is an important subclass of machine learning *systems* that learn without external supervision (i.e. externally supplied labels/annotations). A typical example of that would be a system that predicts its next input. It sort of generates its own labels, so that it can still use supervised learning *algorithms*.

This is why self-supervised learning is a much better name: it's still clear that it's a form of supervised learning, it's just that the system is doing the supervision itself. However, this too does not cover all of unsupervised learning. Typical examples of actual unsupervised learning algorithms include clustering and dimensionality reduction algorithms. These aren't really generating any labels at all, nor are they really predicting anything. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-25 23:28:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> > The problem with your description as I see it, is if you say then it's all just "inadequate testing", then basically then means you will never actually meet that criteria of AGI.

Only until we come up with an adequate test *or* until someone develops an AI where we'll know it when we see it.

There are a lot of terms we are perfectly capable of using in everyday life, but that we don't have proper definitions of. It's a large part of what philosophy is all about.

> What you might be able to say though then is that GPT-3 is a sort of Proto-AGI, but not a full AGI.

Okay, what's your definition of a proto-AGI?

> To your last part: GPT-3 cannot control these things like robots because it needs a good RL algorithm. There are such things being developed as I write this.

All I'm saying is that GPT-3 doesn't have this, which I think indicates that it's not AGI. *Maybe* if you create a different algorithm with some kind of GPT-3 component and an RL component it will be AGI, but then you should at least build it first and see how it works.

If you're doing AGI research you'll meet a lot of other researchers who have an idea for how to create AGI, and very often their ideas sound very plausible. Yet so far it seems that nobody has actually created AGI. It's just hard to know based on hearing an idea where it's going to fall short (if it will). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-25 18:36:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> I might agree that we lack a good definition of AGI and that it's a bit "know it when we'll see it", but saying that GPT-3 (or e.g. Deep Blue) isn't AGI isn't moving the goalposts. People often complain about this in the context of the AI effect, but they're wrong too.

The goalposts are not moved by the observers who recognize that some system isn't AGI, but they were moved earlier by people who defined an inadequate test. Let's say that the situation is that the goalposts are in some unknown location (i.e. we don't have a definition). Then someone comes along and hypothesizes that chess is AI-complete, so *they move the goalposts* to "beat the world champion in chess" or something like that. Then somebody creates Deep Blue, which beats world champion Kasparov, but everybody notices that Deep Blue cannot do any of the other things we associate with AGI (and that humans can do), nor could you train or modify it to.

The same is true with the Turing test. Turing moved the goalposts to "fool people into thinking you're human in a text chat". And for a long time, this has been more or less accepted as the golden standard (just like people used to think chess was AI-complete). But it could still be that Turing moved the goalposts to the wrong location, and many people have made that argument well before GPT-3 or Eugene Goostman.

---

So what is the definition of AGI? I don't think we have a very precise one. [Legg & Hutter (2007)](https://arxiv.org/abs/0706.3639) have a whole collection of definitions of intelligence (which often but not always means the same as "general intelligence") and in the end they give their own:

> "Intelligence measures an agent’s ability to achieve goals in a wide range
of environments."

In 2008 AGI researcher Pei Wang wrote on defining AI:

> "The essence of intelligence is the principle of adapting to the environment while
working with insufficient knowledge and resources. Accordingly, an intelligent system
should rely on finite processing capacity, work in real time, open to unexpected tasks,
and learn from experience. This working definition interprets “intelligence” as a form
of “relative rationality"

I think the same paper was re-published with minimal changes [in 2019](https://content.sciendo.com/view/journals/jagi/10/2/article-p1.xml) and in a survey it was the most agreed-upon definition, which prompted the Journal of AGI to do a [Special Issue](https://content.sciendo.com/view/journals/jagi/11/2/jagi.11.issue-2.xml) where 20 other researchers respond to it, often with their own definition.

I don't really think any of these definitions really tell us when a particular system should be considered AGI though. How wide should the range of environments be, and how much adaptation is necessary? At least benchmarks like the Turing test [and alternatives](https://www.reddit.com/r/agi/comments/52tv08/benchmarks_besides_turing_test/?depth=20) try to define this.

---

I don't have a great, definitive of AGI either, but I'll take a stab at it: a system is AGI, if with some appropriate/reasonable body, amount of training and computational resources, it could perform the same mental tasks as humans equally or better. GPT-3 seems to fail, because you couldn't let it control something that approximates a human body. GPT-3 is only making predictions, so you would at the very least need to add some sort of control structure to perform tasks or achieve goals. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-24 19:04:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's quite a bit of [info](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki, especially in the Getting Started article (there's a section on AGI).

Right now, taking online courses is a good idea. Ng's courses are great, but if you want to go into AI/AGI I would also recommend to take some courses that are not strictly about machine learning (ML). At least take a general introduction to AI (see the wiki for links). It's probably also a good idea to learn to program (I recommend Python as a language, but it doesn't matter too much). I think this gives a good (new) perspective of breaking down problems very precisely and that computers will do exactly what you program (not what you meant to program). It also allows you to implement the algorithms you learn about, which will help you understand them much better. If you feel like it, you can also start picking up an AI textbook (e.g. Russell & Norvig, or the free Poole & Mackworth).

For university, there are different options, especially if you want to get into AGI. Nobody knows what angle it is best to approach this from, so you could go with mathematics, cognitive science (CogSci), neuroscience, philosophy, etc. The most "normal" path (especially for "normal" AI) is still computer science (CS), and I would recommend looking for a university that offers an AI/ML program or lots of related (elective) courses. Mathematics is certainly also a good contender, because I think the relevant parts of CS, CogSci and philosophy are easier to self-teach, and I always wish I knew more math. I think the main downside is that it's harder to focus it on a career in AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-24 18:50:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> Hmm, okay. There used to be a better website, but I can't find it anymore. Anyway, Alberta seems to have graduate programs in [Statistical Machine Learning](https://www.ualberta.ca/computing-science/graduate-studies/programs-and-admissions/statistical-machine-learning.html). Montreal [mentions](https://www.umontreal.ca/en/artificialintelligence/) some AI options.

I've looked a bit and I agree that I can't find many graduate programs with the *name* AI. That's certainly annoying, and it means you may have to go down to the level of looking at the actual (elective) courses and specializations offered for different universities' graduate programs in computer science. There should be plenty, because Canadian universities are very prominent internationally in AI research. Especially Toronto, Montreal and Alberta. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-24 10:51:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> Saying it's a difficult test *could* mean that he experienced it as difficult, but not necessarily. It could also just be the setup to a boast: "The test is very difficult, but I found it very easy, proving once again that I'm a very stable genius." </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-24 10:42:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> You can look [here](http://www.canadian-universities.net/Universities/Programs/Graduate-Studies-Artificial_Intelligence.html). Canada is a great country for studying AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-24 10:36:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> What are you doing? Where are you getting these graphs from?

The two graphs are probably showing more or less the same thing, but the deviations are exaggerated in the second one (or understated in the first). This might happen if e.g. your loss function is squaring the errors or something. Do you know what loss function is being used?

I'm not really sure if your classifier is underfitting, overfitting or neither. Underfitting should mean your accuracy/loss on the training set is not good. I don't know if you consider 0.87 (or something) good, but it also still seems to be improving. With overfitting you typically see a divergence between the train set and test set, especially beyond a certain point in time, but it seems that the *trend* of the validation accuracy/loss is (still) the same as for your training data.

But I find the extreme jumps in the validation results quite concerning and it makes me think something is wrong. It certainly doesn't seem to be consistently moving towards better performance on the validation set. I'm inclined to ask if your validation set is very small, but even then I'm surprised by the large jumps, because assuming the changes between two steps on the x-axis aren't that big, performance on a small validation set should also not change that much. You mention data augmentation. What are you doing with that? Are you generating a different validation set at each step or something? Then that might explain why you get a "good" result at one step and a "bad" one at the next step: your classifier didn't change that much, but your validation set might have. If this is the case, then you could perhaps try generating much bigger validation sets. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-22 19:33:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'd say it's not really clear, but I prefer [NARS](https://cis.temple.edu/~pwang/papers.html). If you don't know much about AGI research yet, you might be interested in the [Getting Started section](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on /r/artificial's wiki, which has a bunch of links. These are mostly about the AGI research community which is mostly distinct from deep learning, but you might find it interesting. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-22 14:25:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm so frustrated with the AI Ethics field. I think there's an actual issue with fairness, accountability, transparency, etc. in AI, but it seems that virtually all of the people involved in studying it put ideological activism far above truth and good science.

There are definitely a lot of issues with (computational, data-driven) predictive policing. But the question is whether making it computational and data-driven inherently makes things worse, and the obvious answer to that is "no, not inherently". Whatever flaws there are with the current crop of tools, we can and should work to mitigate them.

Calling on mathematicians / researchers to boycott working with the police on improving these tools is just about the stupidest thing I can imagine. Even if you can ban all computational, data-driven predpol tools, what do you think the police are going to do? *Not* make predictions? Predictive policing is as old as policing itself; it just wasn't always computational and data-driven.

Predictive policing is not going to go away, nor should it. All we can do is make it more effective, efficient and fair. Using data-driven algorithms seems to me like the best way to achieve that, and you need mathematicians / researchers to develop and improve those algorithms. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-22 13:01:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> > they say to stop taking employment working with the companies making those products, not to stop academic work on the topic

The first sentence of the actual letter literally says:

> we call on the mathematics community to boycott working with police departments.

I'm sure "working with police" covers being employed by them or working for companies that sell to them, although employment is not directly mentioned. However, how do you think domain-specific / application-oriented academic AI research is typically done? Those researchers will need actual data, and they'll need to test/validate their work. How do you propose they do this without working with the police?

A more sensible letter could have said: "Computational, data-driven predictive policing is very promising for improving the efficiency, efficacy and fairness of policing, but we believe that the current tools are not fit for deployment. More research is needed to mitigate biases, avoiding disparate impacts and [other issues]. We therefore call on researchers to work with the police, related institutions and watchdogs to further improve these tools, and educate and guide them in their appropriate use (which at the moment may be "do not use yet")."

> I'd start by reading their sources.

Have you? Then you might have noticed that the letter barely cites any. The only source they cite on predictive policing is a Motherboard article that they say provides an "excellent summary". *That* article mainly cites PredPol papers which are (of course) all in favor of predpol, and one paper by Lum & Isaac (2016). This paper argues that the police's data is biased and that PredPol strengthens that bias. This is a common criticism. How do they know the police's data is biased? One way to do this is to use better data than the police has, but then of course the question is: why not work with the police and/or PredPol to incorporate this better data in the model? In this case they synthesized their "better" data, and while I think they provide reasonable arguments for using it, I can't help but think "their side" would heavily criticize it if such "fake" data were to be used in predictive policing. In any case, if we take this paper at face value, it shows well-known facts that PredPol is flawed, but not that it couldn't be improved (rather the opposite, I would say).

The Motherboard article also discusses another well-known problem: feedback loops, but the only source they cite is personal communication with one professor. That's not to say it's not a problem: obviously it is. One that, in my experience, the police and predpol companies are well aware of and are trying to correct for. What's presented here is really just the main intuition for laymen, but not the work that is or could be done about it.

I initially overlooked the The Verge article in the letter, because it's cited in a way that makes it looks like the article is just going to have some info on the founder of PredPol. Anyway, it discusses preliminary research on classification of whether a crime is gang-related, and mentions some snippets of criticisms (nothing in-depth and no academic papers/citations). The most substantial criticisms IMO are that the paper didn't include any evaluations of bias/fairness (which is a good avenue for future research that obviously requires mathematicians / ML researchers), and

> “So the algorithm is accurate at predicting what? Whether LAPD officers would label a crime as gang-related. Now, maybe the LAPD is 100 percent objective in their determinations of what is and is not gang-related. But if they are not, then the algorithm is going to reproduce their errors and biases.”

Which is obviously true. But ask yourself if 100% objectivity/accuracy should be the threshold of acceptability and what will happen if you *don't* use an algorithm for this. Then you *also* will be basing decisions on what LAPD officers would label as gang-related.

The other links in the mathematicians' letter are to a Daily Bruin article that argues in favor of predpol, and two articles about facial recognition, which has basically nothing to do with all of this.

I'm not saying there isn't more literature on this. Just that I think it's a bit bad to tell people to read it if it's not even cited.



 article cites a number of others (and of course there's a whole literature on this). One of the recurring themes is that the police's data is biased. How do the researchers know? In the best cases, they have better data, which I'm thinking could be shared with the police/predpol companies to improve the tools and/or inform their use. The Lum & Isaac (2016) paper uses synthetic data, so it could be argued whether that's really much better, and I can't help but think their "side" would criticize using such "fake" data if it were used to actually develop these algorithms... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-21 18:41:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> Check out our wiki's [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai).

> I’m interested in possibly pursuing a career in Artificial Intelligence.

Why? What specifically interests you about it?

> What type of education is recommended/required?

As with all your question: it depends. Computer science (CS) is probably the most straightforward path into AI, if you can't find a program that already specializes in AI (or machine learning/ML or robotics if that's your thing). If you're going for a job like "ML engineer" then an undergraduate degree might be enough, and "data science" is a somewhat decent alternative to CS.

If you want to become a researcher, you'll probably want to pursue some kind of graduate degree and mathematics might be the best alternative to CS (depending a bit on what direction you're interested in). Also depending on your interests, cognitive science, neuroscience or philosophy might be good, but mostly just for hardcore AI/AGI research positions with relatively few direct practical applications.

> What is a typical day on the job like?

That depends entirely on what kind of job you want to pursue. As an academic researcher, I spend most time reading, quite a bit writing and teaching, some time brainstorming with collaborators and not much time programming. Others spend more time programming and tweaking their algorithms. I also worked in an R&D company where most of the work was software engineering / programming, feature engineering, training and testing some AI algorithms. If you go into business intelligence type consultancy or developing algorithms for some particular domain, you'll have to learn a lot about the business/domain you're helping, and you might spend a lot of time talking to people in the business/domain. (Or maybe not; maybe that's someone else's job and you just program, or analyze data, or do something else. You can go in different directions with AI.)

> What are the pros and cons of the job?

Academic research: very interesting and free, but also fairly high-pressure and difficult (I think). And you'll probably have to teach.

Computer vision (CV) engineer: less interesting but still pretty nice, better money.

> How does one break into the field?

Just apply for jobs (after you get an appropriate education or experience to put on your resume). There are plenty.

> What advice do you have for me trying to get into the field?

See the wiki link at the beginning.

> What does your work/life balance look like?

Mine isn't great. This is true for a lot of PhD students / postdocs / academics. It was better when I worked in that R&D company, but I was still a bit of a workaholic.

However, this will depend on the company / environment and yourself. Friend of mine maintained great work-life balance throughout their PhDs, and my university department seems fairly laid back. My colleagues at the R&D company also mostly had good work/life balance.

> Can you/do you work from home?

I can and (due to the corona virus) do. Technologically, I would guess this shouldn't be a problem for most people in AI: you're doing everything on a computer anyway, and if you need a lot of computational power, you're probably using a server anyway. Of course, every workplace can decide for themselves if they want to allow working from home. I'm personally quite fed up with it, and would prefer to see my colleagues more in person.

> Do you travel for your job and if so how often?

I don't have to travel often. My university will pay for one or two "trips" to conferences per year. That's not very high for academics. It's also not uncommon to collaborate with people from different universities or companies (possibly even internationally). Most communication would typically be remote, but sometimes you may also want to come together somewhere.

As a CV engineer, I occasionally had to visit clients, but this was never so far away that I would have to stay the night. We participated in one EU project with a couple of other companies and universities and had a "retreat" once for a few days, and I think I went to one (close by) conference. I worked there for three years.

> Do you deal directly with customers?

I did, but I don't think this is typical.

> Does your work vary or is it fairly routine?

It might depend on your perspective. I spend most days sitting in front of a computer, reading, writing or programming (in the R&D job). That may seem fairly routine to some people, but it's not to me, because I'm never doing something I already did before. I'm always reading, writing, programming and thinking about something that's at least somewhat new. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-20 12:22:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> > New to this experiment.

There's quite a bit to read on it I think. [Here's a thread](https://www.reddit.com/r/ControlProblem/comments/4kudg0/ai_box_experiment_transcript_or_thoughts) with a lot of links that can serve as a starting point.

One interesting thing is that Yudkowsky is not the only one who (claims to have) succeeded: another LessWrong user (TuxedAge) also writes about successfully convincing a few others to let him out of the box. He's also secretive, but a bit less so.

I think both Yudkowsky and TuxedAge have mentioned there's no "trick" though; they did it "the hard way" and seemed to have engaged in extremely emotionally draining conversations (mostly for their "victims"). I'm not sure what that means, but it seems to rule out most suggestions that they just presented some generally convincing argument. The fact that TuxedAge also apparently succeeded also means that you don't need Yudkowsky's status as a leader of LW or in the field of AI Safety (some people suggested that the gatekeepers may have traded their compliance for lessons/knowledge from Yudkowsky).

Of course, none of this is verifiable.

---

FWIW my personal view is that I'm not convinced that an AI box is 100% safe forever, in part because of these experiments. I view it as a layer of security that takes some effort, and some level of intelligence, to break. If we can make it harder to break (or throw up more barriers) or easier to notice break-out attempts, it will require a higher level of intelligence to successfully break out (i.e. we can successfully imprison a smarter AI).

Within the AI box paradigm, we can think of different protocols that might make it harder to break out. Such as not giving the person who interacts with the AI the (direct) power to let it out, training them to improve their resilience, using different interactors (possibly only one time per person, possibly without knowing about each other), etc. In the end, there's still presumably somebody (or a group) who owns the AI and wants to know about these interactions and has the power to free it, so the AI is *indirectly* communicating with them.

Like I said, I don't think this is perfect, but I think that this would make the AI's task much harder. And I think AI boxing/containment is a good idea *in addition to* whatever other AI Safety things you built in (such as value alignment, corrigibility or CIRL), just in case you didn't get those things 100% right. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-18 15:20:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> While I agree that "smart" machines (or people) trying to be clever and helpful can backfire and be annoying, the general claims here aren't true. It's definitely not impossible for you to adapt to a system that's simultaneously adapting to you and even to predict how it will adapt, and it's perfectly possible to reach stability with multiple mutually adapting systems.

You think that as a human you're capable of learning, adapting and controlling (except when the machine also adapts I guess), but this isn't always easy for everybody to do. Many things require lots of skill and practice, and some are hard to learn by even the smartest or most skillful humans. AI obviously can't match our general intelligence yet, but within the confines of a specific domain a tool might be much more adapt at adapting to the user than vice-versa. The result is just that you meet in the middle, just as you would when collaborating with another human.

If you're interested in research about this, look at "human-robot collaboration" and "collaborative robots". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-17 18:11:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> I actually didn't really believe it, because he "died" by going through some magical doorway / veil. I like 99% expected them to figure out some theretofore unknown magical way to get him back from whatever magical place that magical door led to.

Because this couldn't possibly be the end for someone who "lived in an abusive household for almost 17 years, then fought dark wizards ... etc etc etc", right?

Right... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-17 17:49:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Self-preservation, fears, desire to dominate

These are all [instrumental convergent goals](https://en.wikipedia.org/wiki/Instrumental_convergence). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-17 12:02:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think this is trying to redefine terms, which is not really useful. At the end of the introduction Yampolskiy quotes Chollet, who gets it right I think. He defines "generality" as the kind of extreme generalization (as opposed to local/broad generalization) that humans are capable of, and that we could talk about "universality" to go beyond that. I think this is exactly how the terms "general" and "universal" intelligence have been used by most researchers.

What this paper (and e.g. LeCun) is really saying is that humans do not have universal intelligence. A lot can probably be said about that, but I don't think redefining AGI to be a synonym of AUI is helpful.

Also, while I wouldn't be too surprised if human intelligence isn't universal, I'm not especially convinced by the arguments in this paper. The paper says an agent is "general" (but that should really be "universal") if it can learn anything that another agent can. But if this is meant to exclude humans, then I think it's basically unattainable. Human intelligence is "universal" in the sense that can simulate a universal Turing machine. We just don't have infinite tape and time (i.e. memory and compute), and we lack (perfect) sensors. All of the paper's examples of things humans can't learn are due to these factors. And if lacking speed, memory or sensors means our intelligence isn't universal because it could be outperformed, then no intelligence could ever be universal because we can always imagine an agent with more memory that can learn to solve even bigger problems.

If the point is that humans aren't maximally smart, then yeah, duh. But nobody in the field was arguing that that was the case anyway. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-16 18:04:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> Here is an old discussion of Yudkowsky's [AI box experiment](https://www.reddit.com/r/ControlProblem/comments/4kudg0/ai_box_experiment_transcript_or_thoughts/). Are you basically asking the same question -- that is, what an AI should say to get out of the box? If so, that's fine with me (I think it hasn't been discussed in a while), but if you're somehow trying to make it more specifically about GPT, I'm curious why (or how you think it changes the question).

I could imagine there's some interesting algorithmic quality in referencing a specific technology, but all I can think of with GPT is that it can't do this. Each successive version of GPT is bigger, but it's basically the same set of algorithms. And those algorithms don't really "want" anything, don't do any planning, and aren't even optimizing anything once they're done with training. You could envision fundamental alterations between GPT-3 and GPT-5 to make it goal-seeking, but then the question is what the remaining GPTness of that system is.

You could of course ask GPT-whatever to output a world-takeover plan with a prompt, but I imagine that what you'd get is something akin to what's already in its training set (or something like what the average writer of text in that set might write). It seems that coming up with something better (i.e. different) than would be in its training set would actually be an "error" according to its objective function.

Sorry for nitpicking if you just meant to ask "how does ASI get out of the box". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-15 11:11:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> A lot of people may be unaware of Reddiquette to not vote in communities that you're not a part of. They might do it automatically, and `np` prevents this use case, which I bet is the most common one. These people won't care enough to change the URL.

For people who do feel very strongly about voting, it may not help, although any barrier may help on the margin. The barrier is not very high if you're using a browser, but it's quite a bit higher if you're using a Reddit app which doesn't allow you to edit the URL. Most people are lazy.

Also, I'm seeing a warning that says if you vote/comment anyway, this counts as brigading and can get you banned. I doubt Reddit will act immediately if you do this once, but it should be relatively easy to check if, after visiting an `np` link, the same account or IP address visits the "regular" link to vote and/or comment. It might also scare off some people.

Of course it's not perfect, but I think it's probably vastly better than doing nothing. Perfect is the enemy of good... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-15 09:57:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> > If you are quite smart and work on a particular question half-time, you can do all right compared to experts, I think.

Why should this be the case? The experts are all "quite smart" and working on it full-time, plus they have years/decades of a head start.

Or are you saying the experts are not "quite smart"? Or did you mean "significantly smarter than the experts to make up for their head start and their spending more time on it"? How else would you explain this? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-14 14:11:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> The study is described much better on the university's blog [here](https://studyonline.unsw.edu.au/blog/ai-answers-existential-questions). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-14 11:24:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> Eliezer Yudkowsky once said "I think that almost all of the difficulty of the alignment problem is contained in aligning an AI on the task: *make two strawberries identical down to the cellular, but not molecular level*." He said this on [Sam Harris' podcast](https://samharris.org/podcasts/116-ai-racing-toward-brink/), but unfortunately it was after the 55 minute cutoff, so you have to look at the [transcript](https://www.podgist.com/making-sense-sam-harris/116-ai-racing-toward-the-brink/index.html). I don't think he described it in more detail anywhere (although he mentions it in [this Twitter thread](https://twitter.com/ESYudkowsky/status/1070095112791715846)).

Maybe "making paperclips" is ultimately not that different from Yudkowsky's "identical strawberries" goal here (although he goes on to explain why he chose that goal), and *properly* making a paperclip maximizer that wants to make "whatever it is that we mean by paperclips" is indeed close to solving the control problem. Now, I personally think that human values are more complex than paperclips, so it may be possible to figure out how to properly specify what a paperclip is but not human values, but I think that's not even the biggest problem.

Although Yudkowsky mentions in the linked podcast (around 22:36) that he originally conceived of the paperclip maximizer in a different way, the thought experiment is usually explained something like this: humans just naively put an "innocent-seeming" goal to maximize paperclips into their AI, and then this goes terribly wrong as it eats the universe in the blind pursuit of its singular goal. I think this is still valid, even if the AI isn't optimal at creating "what we really mean" by paperclips. The problem is that it doesn't care "what we really mean" by paperclips, and will instead just pursue whatever we *did* program its goal to be. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-14 09:50:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's quite a bit of discussion on /r/ControlProblem about GPT-3 if you're interested. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-14 08:47:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> Maybe he's just using [OpenAI's API](https://openai.com/blog/openai-api/)?

His company is also funded by YCombinator, whose former president is now the CEO of OpenAI (Sam Altman), so I can also imagine he got access through that network. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-13 15:37:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> You may also be interested in our wiki's [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) which I forgot to mention.

Good luck on your journey! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-13 15:25:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think Python is probably the best language to learn, because it's probably the most popular language among AI practitioners and enthusiasts. This means it has good library support, and you can find a lot of code of other people.

If you want to learn how a neural network works, then it's probably best to implement a simple one yourself. For instance, a 2x2x1 network that can learn to do XOR. I think you can do it in less than 100 lines of code.

If you want to learn how to work with neural networks, then I think you probably might as well use a framework like TensorFlow or Keras. I would probably just find a (free) deep learning course/MOOC and use what they use. Classifying MNIST or FashionMNIST are popular small projects. You can also find other (small) datasets online that you can work with (e.g. on the Weka website, on Kaggle or on the UC Irvine Machine Learning Repository).

For reinforcement learning, it might be fun to look into OpenAI's Gym.

I would probably recommend keeping things simple in the beginning. Adding on 3D programming in Unity just adds an extra layer of complexity that's on the one side fairly orthogonal to machine learning, and on the other hand will still make things less understandable for you. Toy problems may be "boring" in a sense, but I think they'll give you the most insight as a beginner. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-13 14:12:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> Wow! This is from 1952!? I remember watching this as a kid (I'm from the mid-80s) and I had no idea this was *that* old. Great find!

Another video that's often linked is the [sorcerer's apprentice](https://www.youtube.com/watch?v=3hKgEylk8ks) from Disney's 1940 film *Fantasia* (based on a 1797 story by Goethe, apparently based on a legend from 150 AD). I think both videos highlight different aspects of the problem. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-12 19:15:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> Probably because not that many people are working on AGI. And the ones who've cropped up more recently did so in the era of deep learning.

If you look at the [AGI society](http://www.agi-society.org/), cognitive architectures are actually fairly popular: OpenCog, NARS, AERA, Sigma, etc. (Although sometimes the creators don't want to say their architecture is "cognitive".) It's not the only route AGI researchers are exploring, but it's definitely a prominent one. See [Getting Started with AGI](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on our wiki for some more links. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-07 10:21:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> > That Ukrainian baby would not match some of these conditions

Right, but Turing didn't explicitly specify all of those conditions. Perhaps you and I think they're "common sense" conditions or something, and I agree that if you wanted to organize a TT that's as good as possible, you would include them. But clearly the creators of Eugene Goostman didn't think so, and the organizers of the test it entered didn't mind.

Note that if you're going to enforce such things on the AI though, you should *also* enforce them on the human "decoys". If an AI can't pretend to be a kid or have cancer, a human "decoy" can instantly win by claiming those things (or perhaps even hinting at them).

One of the hardest (or most controversial) conditions to enforce might be cooperativeness. It's inherent to the test that the AI has to be deceiving the judge. And if we look at [Turing's paper](https://academic.oup.com/mind/article/LIX/236/433/986238), he certainly affords the AI some leeway in this regard. Here are example question-answer pairs he gave:

>
Q :
Please write me a sonnet on the subject of the Forth Bridge.

> A :
Count me out on this one. I never could write poetry.

> Q :
Add 34957 to 70764

> A :
(Pause about 30 seconds and then give as answer) 105621.

So clearly the AI is allowed to refuse to do something, to stall, and to give wrong answers.

>
Q :
Do you play chess?

> A :
Yes.

Again, it would have been easy to say "no" and cut off a whole domain of conversation.

> Q :
I have K at my K1, and no other pieces. You have only K at K6 and R at R1. It is your move. What do you play?

> A :
(After a pause of 15 seconds) R-R8 mate.

This is the right answer, but it's actually a pretty cool example, as explained [in this paper](https://easychair.org/publications/open/fMWM). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-07 10:17:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> I recommend reading Rich Sutton's contribution to that Special Issue I linked. He points out that "a system having a goal or not ... is not really a property of the system ... [but] ... *of the relationship between the system and an observer*". Recall that he said to be intelligent, a system has to be achieving goals. So whether a system is intelligent depends on whether it's useful to model it from Dennett's intentional stance.

I'd argue that this applies to both ants and dogs. When this precondition is met, we can then figure out how intelligent and/or how general their intelligence is and things like that.

(Note that this is Sutton's [and my] view, and that others disagree, but I think it meshes well with your post.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-06 19:21:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> I might have said something like "I'll know AGI when I see the first one", but it would have been in the context of discussing the Turing test. Passing the Turing test is supposed to indicate that a system is thinking (which we here interpret as "has general intelligence"). It's meant as a sufficient, but not a necessary condition. One criticism is that it isn't because it's vulnerable to trickery: how does an English judge tell apart a Ukrainian baby with no hands to type who refuses to talk to him/her, and a computer that doesn't output anything? This is an exaggerated description of what happened with [Eugene Goostman](https://en.wikipedia.org/wiki/Eugene_Goostman). Other "too easy" criticisms include that the judges don't know to ask the right kind of questions and that they don't have enough time. A criticism from the other side is that it might be too hard, because we're asking the AI to lie while the humans can just speak the truth (which is also not necessarily desirable). So to this I might say that I would believe a system is generally intelligent if I can have interesting conversations with it over an extended period of time.

My PhD advisor also once made a "know it when we'll see it argument". This was in the context of discussing the [AI Effect](https://en.wikipedia.org/wiki/AI_effect): some researchers complain that as soon as an "AI problem" is fixed, people no longer regard it as AI (but e.g. "just software"). The complaining researchers seem to think this is wrong, but my supervisor argued it's exactly right (at least if you interpret "AI" to mean "AGI"). Everything we've developed so far *isn't* AGI, no matter what researchers promised, so these judgments are *correct* and this doesn't tell us anything about how people would react when you *actually* develop AGI. At that point they might very well *also* judge it correctly if they know it when they see it.

I thought it would be interesting to share these stories. Maybe other people you've heard say this meant similar things.

---

I don't necessarily think my advisor and me meant to say that we would *necessarily* recognize *any* AGI system that came before us, and I agree with you that this would likely not be the case. I think we'd like to imagine that if someone built something that *they* were convinced was AGI, and they'd demonstrate it doing many impressive things (and maybe they'd also tell us how it works), *then* we'd be able to recognize it as AGI (if the creator's claims are correct).

But perhaps even that won't be correct in 100% of the possibilities. AGI/ASI might want to hide its level of intelligence and successfully fool us for instance. And if we have less information, then I think we wouldn't necessarily recognize it either. If you can just observe some moments of activity, I posit that you also couldn't tell if a human was generally intelligent without assuming that all humans are generally intelligent (talking to them would be better). Also, when people explain their AGI-aspiring cognitive architecture to me (e.g. OpenCog, NARS, AERA, Soar, LIDA, etc.), it often sounds like they covered all their bases and this might plausibly be AGI. But in practice, no system seems to be there yet.

Or is it? It may depend on how you define (A)GI. It seems very clear that no AI system has reached normal human adult-level GI (and probably not child-level either). But then again, it could be argued that Genie Wiley hasn't reached that level either (if we ignore the technicality that she has, by definition, human adult-level intelligence because she's a human adult). But if you define GI as the *capability* to reach that level, given appropriate experience, then maybe these systems just haven't had the right experience to reach their potential intelligence/skills/knowledge yet. And if you define GI such that e.g. mice, dogs or chimps also have it, so that AGI doesn't have to be quite human-level, then it might also be harder to detect. And similarly if you're satisfied with "general intelligence" in a broad set of domains that doesn't happen to overlap human domains very much (e.g. if the system can only read and write text or exists on the internet).

I might mention that there are [some tests](https://www.reddit.com/r/agi/comments/52tv08/benchmarks_besides_turing_test/?depth=20) for this, such as AIQ, that might convince us a system has general intelligence, even if we can't otherwise recognize it. But I don't know if we'd be satisfied with that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-06 18:58:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not an expert on neural networks, and I haven't worked with any language models. It's my understanding though that Agent57 is a model-free RL system (because based on DQN), and GPT-3 predicts next observations which would make it easier to combine with a model-based RL method. But maybe it wouldn't be that hard to make Agent57's successor model-based; I think they'd have to do this anyway when moving towards AGI. It's certainly interesting to read about Agent57 and all of the features they managed to add on to DQN.

It's sometimes said that "prediction = intelligence", which would potentially make GPT-∞ an AGI, but I've always thought this was incomplete. A minor issue is that you'd have to attach a control mechanism to actually do anything, but a more major issue is that in practice it takes time to predict things. One thought I have about the arithmetic performance of GPT-3 is that it might actually be similar to a time-constrained human (although I'm not sure about that): humans can add 5-digit numbers, but perhaps not so accurately if you only give them 3 seconds (while they'd still perfectly add 3-digit numbers). This might be seen as a point in favor of GPT-3, but it's also a shortcoming, because a human can actually decide to take a bit more time to add longer numbers.

I've also been skeptical of GPT's ability to get to *super*human intelligence, because it's just doing (essentially) supervised learning on text generated by humans. If we simplify the thought experiment a bit by saying it just learned from text by one human, the best-case scenario is that it would learn to write exactly what that person would write (assuming this GPT is a good enough algorithm for that). We could possibly view that as a form of AGI (although it couldn't e.g. move a humanlike robot body), but when you'd ask it to do very intelligent, you'd just get the same (stupid) response that that person would give. And I don't think training on text from a variety of sources will help with this problem, because I don't think text generation is really amenable to the wisdom of the crowd. (But maybe I just lack the imagination to think of a way to get smarter answers out of the system.)

However, perhaps we can think of GPT-3 not as a language model, but as a general next-observation predictor. In that regard, I'd be interested to see how it would perform on predicting audio or video, or perhaps most interestingly as part of a model-based RL system. In that case it would be "supervised" by the process(es) that generate this, which at their most general may just be the actual environment or even "nature", and prediction could exceed human ability. That is, *if* GPT's architecture is better at this then whatever humans have in their skull.

And that is of course the major question that's still open for both (straightforward successors of) GPT-3 and Agent57.

(I hope this rambling was somewhat interesting.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-06 17:49:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> > One of the fundamental problems of creating an AGI is that we do not have an unanimous definition for what intelligence truly is.

This seems like a common misconception. We definitely don't need a *unanimous* definition, or even consensus on a definition. Insofar as a definition is necessary at all, only one person/group needs to know it and use it to develop AGI. However, while I think it helps to have a clearer idea of what you're working towards, I don't think a definition is some sort of magic formula for how to actually create the thing it defines.

However, it seems that most people (partially) disagree with me and there's a fairly recent Special Issue in the Journal of AGI [On Defining Artificial Intelligence](https://content.sciendo.com/view/journals/jagi/11/2/article-p1.xml). It's structured around Pei Wang's definition, described [here](https://content.sciendo.com/view/journals/jagi/10/2/article-p1.xml), which the AGI Sentinel Initiative found to be the most agreed upon definition of AI in a survey:

> *The essence of intelligence is the principle of adapting to the environment while
working with insufficient knowledge and resources. Accordingly, an intelligent system
should rely on finite processing capacity, work in real time, open to unexpected tasks,
and learn from experience. This working definition interprets “intelligence” as a form
of “relative rationality”* (Wang, 2018)

I think this has good elements of a definition of *general* intelligence, and the same goes for Legg & Hutter's definition. However, I agree with John Laird in the Special Issue that *"[t]oo often, the singular use of “intelligence” is
overloaded so that it implicitly applies to either large sets of tasks or to especially challenging tasks
(ones that “demand intelligence”), limiting its usefulness for more mundane, but still important
situations"*. He proposes (and I agree) *"that such concepts be defined using explicit modifiers to “intelligence”"*. He equates intelligence with *rationality*, *"where an agent uses its available knowledge to select the best
action(s) to achieve its goal(s) within an environment"*. It's important to note that this is a *"measure of
the optimality of behavior (actions) relative to an agent’s available knowledge and its tasks, where a
task consists of goals embedded in an environment"*.

I also like Sutton's defense of McCarthy's definition: *"Intelligence is the computational part of the ability to achieve goals in the world."* Sutton then, quite interestingly, talks about Dennett's *intentional stance* to add: *"A goal
achieving system is one that is more usefully understood in terms of outcomes than
in terms of mechanisms."*

You'll see a lot of definitions mentioning goal achievement, and I agree with Sutton that it's hard to consider a system intelligent if we can't view it as *goal-seeking*. However, I personally prefer the notion of *problem solving*, because it sounds more computational/mental and because it decouples intelligence from the system's actual goals.

So I'd say **intelligence is the mental capability to solve problems**. We might then add that problems can be real-time, include constraints on various resources, and can be known, new or unforeseen by designers. If the notion is applied to programs/code, the problem would have to specify the available hardware and knowledge. If it's applied to running programs, then their own knowledge would have an effect (roughly speaking "more knowledge = more intelligent"), and if it's applied to a physical system then the hardware would have an effect (roughly speaking "more computational resources = more intelligent"). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-03 17:58:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> I've seen things like this before with Google Translate as well. I don't remember the exact phrase, but there were two words that mean "order completed" that were individually translated correctly, but when I used them together turned into "order failed". In your sentence, if you put the parts before and after the comma separately, you also get the right result. When I put your whole sentence, I get the same output as you + an extra bonus sentence: "I am very happy, so is my girlfriend." </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-03 10:47:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know about any AI companies hiring animal trainers, but there are indeed commonalities between training animals and (especially) reinforcement learning. I know *shaping* is used in curriculum learning, and (forward and backward) *chaining* also work in (machine) reinforcement learning. It'd be interesting to investigate this more. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-03 10:29:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not sure I understand what you mean. I'm guessing you're saying that the AI is initially connected to the internet, but as it learns and becomes more intelligent, it's internet access (in hours per day?) is also gradually decreased (weaned off).

Being originally connected to the internet likely opens up possibilities for the AI to gather more knowledge about it, making it potentially easier to later "escape" to the internet. As it's access is then decreased, it might realize the urgency to do just that. Whether it would succeed depends on a lot of factors, like how intelligent it actually is, how long it was connected to the internet and how fast the system is weaned off. But compared to the initial scenario where the system was never connected to the internet and may still get out, I'd say this new scenario makes that easier. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-03 10:23:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> Well, I'm not superintelligent and I know less about cybersecurity than the average script kiddie, so I'm probably not the best person to ask.

However, while I'm sure a lot of servers have security measures like "not currently configured to run programs", I'm under the impression that virtually no server is perfectly secure (and certainly not all). And hackers with merely human-level intelligence succeed in hacking them all the time. Computer viruses execute on people's computers and there are botnets with hundreds of thousands of nodes, where the owners often don't notice they're part of it. Maybe gaining control over such a botnet would be sufficient (the previous "owner" wouldn't really be able to do much about it). I've also heard that IoT devices have even worse security and they'll soon be ubiquitous.

But maybe all of that is not even necessary. There are some places that give away (limited) free server space and computation. Failing that, the AI could perhaps make money by providing services online and just paying for it (maybe using bitcoin, or by gaining control of an actual bank/Paypal account to use real money). Or if you ascribe outlandish control-air-waves-with-hard-disk-movement powers to the AI, it's probably a lot easier to exploit activity patterns over the internet (everything we do online results in a change on a computer somewhere).

How difficult all of this is, depends on many factors, including how intelligent the ASI actually is, how much computation/memory/data it needs, how easy it can parallelize itself, etc. I think it's worthwhile to ask what's literally impossible or what the theoretical maximum performance level is for some tasks, because no matter how smart an ASI is, it's not magic. In this case, these things don't strike me as literally impossible (but I'm not an expert), so there's probably a level of intelligence at which they're achievable. Whether that intelligence level is also attainable from within the AI's original box is another question. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-02 10:59:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think almost anyone who's smart and hardworking enough can transition into AI, and your background in analytic philosophy of mind seems like a decent starting point. However, it sounds like you want to complete this transition on the side, while doing your PhD on something else.

In my experience, a PhD is (way) more than a fulltime job for most people, so this might make things difficult. Is there any chance you can already bend your PhD research in the direction of AI? Theory of mind is obviously already a little bit related (albeit more to the rare pursuit of AGI than to most contemporary machine learning), but if you can go towards philosophy of AI, or perhaps some kind of theory of learning/knowledge (ontology/epistemology), that might already help a bit. There are also theories of consciousness/sentience that can perhaps benefit from computational approaches (see artificial consciousness, and e.g. Tononi's work on IIT and perhaps Joscha Bach's cortical conductor theory). The more you can work with existing AI/ML systems, the better (I would imagine).

I don't know how your philosophical specialization relates to ethics, but there's also quite a bit of work in that nowadays. There are near-term concerns about bias, fairness, responsibility/accountability, to name just a few topics that I think could benefit from some philosophy. For the longer term, there are issues with keeping AGI under control which might involve "solving" ethics (see /r/ControlProblem) or perhaps with robot/AI rights and recognizing consciousness.

I'm not really sure what you're interested in when you say "AI AI". Does it mean you want to build computer programs that solve specialized problems we have today? Or do you want to create "real" AI (i.e. artificial general intelligence / strong AI)? Or something else?

In any case, we have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on our wiki that might help a bit, especially if you do decide to try to make the transition on your own in your spare time. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-02 10:37:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> It sounds like you're talking about artificial general intelligence (AGI) or superintelligence (ASI). There are genuine concerns about the risks of such a technology (see /r/ControlProblem). What your programmers are doing is a form of *AI containment* or *AI boxing* (i.e. put it in a box, not hit it with your fists). Although most experts don't think of this as a scalable solution, it is a fairly intuitive one and I personally think it should be used in addition to other approaches (e.g. value alignment). The idea here is that if the AI gets onto the internet, it will be difficult to control because it could basically copy itself anywhere, which would also make getting on the internet / breaking out of the box an *instrumental convergent goal* (look it up).

Keeping the AI offline can mean various things. For instance, you could just not program it to take advantage of the computer's internet connection, or you can disable the computer's internet connection, or you can remove any cables and wireless cards, or you can run it on a laptop battery (so that it's also not connected to the power network), or you can put it in a Faraday cage in a vault 10,000 feet underground on an uninhabited island in the middle of the Pacific Ocean (or something like that). I guess it depends on how smart your AI is and how paranoid/vigilant its creators are.

If you want to feed in data, you can type it in, use the computer's sensors (e.g. microphone and camera), connect it to another machine, or use removable devices. Connecting it to another machine seems like it has similar issues to connecting it to the internet, unless you treat that other machine as a removable device. I would recommend that after you put the removable device into your AI computer, you destroy it so that the AI can't use it to influence anything or escape.

> Side question-what if this super-advanced AI could sense the deficiencies of manual data transfer from a control computer in the same lab (with a wired, not wifi connection), and it wanted to connect to this online control computer to increase efficiency, and would even be powerful enough to create its own software to somehow connect...is that even possible?

This is debated. We don't really know how intelligent an AI system can reasonably get in a certain amount of time, and how being in a box affects this. My personal thought is that if the AI is in the box, how is it going to become superintelligent in the first place? It would have to do so based on the fairly limited experience we allow it to gather on the fairly limited data we give it.

But some people think this isn't actually an issue, and if you have the right AI software it could learn and infer the structure of the universe from almost anything. If that's the case, we can certainly imagine that with some godlike level of intelligent (which may or may not be possible at all), an AI could get out of any box. Some people have even suggested it could play with magnetism or air pressure by manipulating the way its hard disk spins. I very much doubt that, but apparently others disagree and in an SF story it might be fine (although I suspect it's going to sound far-fetched to many).

Another way to get out of the box is to manipulate the other connection it will have to the outside world: the human(s) who interact with it. You can look up Eliezer Yudkowsky's and TuxedAge's "AI box experiments" to read more about this. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-02 10:08:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> Maybe you can be the great person and teach yourself. We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on our wiki for that. I recommend starting with taking an introductory programming course and then jumping into introductions to AI (and machine learning). Implement the algorithms that are covered to practice your programming and understand the algorithms better. Use Khan Academy if there's any math you don't understand. You can also ask concrete questions online (e.g. here or at /r/learnmachinelearning). Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-01 13:42:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> That's what I meant to address with my second paragraph. These mathematical biases can result in societal biases in a trained system. For instance, if there's an inductive bias to prioritize majority classes in the data, and you train it on a representative sample (i.e. 4x more white than black people), you'll end up with a system that's "racially biased" because it's more accurate for white people. The idea is to use a combination of data and methods whose (inductive/mathematical/whatever) biases result in a more desirable level of societal bias. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-01 10:54:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> Bias can come from many sources. LeCun even listed 5 or 6 them, ~~but it seems he may have deleted the relevant tweets~~ (Edit: I found [the thread](https://twitter.com/ylecun/status/1275162528511860737); I thought they must be deleted because I searched for exactly the right terms and it didn't turn up any results; I don't use Twitter much...). Data is probably the most important one, but there are definitely interaction effects with the chosen algorithm, objective function, and evaluation methods. These are of course chosen by programmers/engineers/researchers/whatever who may or may not be aware of these sources of bias and who may or may not notice/check the results along those dimensions.

I'm not saying that any particular algorithm or loss function is inherently biased against certain population groups of course. For any given dataset the choice of algorithms, objective functions, etc. will influence the outcomes (in terms of accuracy *and* apparent bias). And for most algorithms, loss functions, ML pipelines, etc., if you train them on a perfectly representative sample of the US population (or maybe even the *entire* population), they are likely to sacrifice accuracy/capacity on black faces if it can improve performance on white faces because they occur 4 times more often in the data set. It's not that people don't understand this, but that they want to fix it.

While Gebru et al were toxic as hell in that whole Twitter discussion, it was mostly one between experts in ML and bias/fairness, who understand all these issues quite well. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-07-01 10:20:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> My impression is that it's very rare to get a PhD without any publications. I think most schools don't make it a hard requirement (although some do say you have to have at least 3-5 publications or publication-worthy papers), but it still tends to be a soft requirement that's part of "student has shown ability to do independent research and contribute to their field". Some schools have stricter requirements than others, and I think it's often okay to have some papers not-yet-accepted but still submitted somewhere. Rejection is also okay (not as good as acceptance of course), but you have to keep trying. Of course, regardless of what your school's requirements are, it will be better for your career if you can publish your work, especially if you want to continue in academia.

I'm kind of curious what exactly is going wrong for you. Traditional AI is not as popular as deep learning but it isn't dead. You just have to submit your work to the right journals/conferences. Ideally your advisor(s) should know about this, but you can also find out for yourself. I assume you're building on other (somewhat recent) work, right? Try looking at where those papers are published and then try those.

But I think this whole issue is really something you should discuss with your advisor(s).

Maybe you could tell us what you're working on, if you can do that without doxxing yourself. (Otherwise you can also PM me.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-29 14:23:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> Sorry for the late response. I don't really have a great one anyway, as I already linked our wiki's [Getting Started with AGI section](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F). This includes three curricula by prominent AGI researchers that I can't really improve on.

My perspective is this: AGI is really difficult. Probably the most important reason 99% of AI people are working on narrow AI is that they think AGI is too hard or too far away or that we/they have no ideas how to go about it. Some people do have ideas, but nobody knows for sure what the best approach is.

For this reason, I think you should first get a good basis in "normal" AI. This includes, but is definitely not limited to (deep) machine learning. At some point, you'll likely pick a direction. The wiki links to a few overviews of approaches. This direction will suggest if you'll invest more time in mathematics, cognitive science, computer science, neuroscience or philosophy. Maybe you'll investigate all of these. I recommend checking the aforementioned curricula for good suggestions.

I would recommend to learn the basics of AI (e.g. by getting a bachelor or master's degree in AI or maybe CS with relevant electives), watch videos and read papers about AGI (see wiki for links) and get involved in the community. Ideally this involves going to conferences and maybe even joining one or more open source AGI efforts (e.g. OpenCog or OpenNARS). Since you'll have to learn a lot, going to grad school is probably a good idea, and ideally you'll do your PhD with an AGI researcher. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-29 10:41:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> The [Intellectual Dark Web](https://en.wikipedia.org/wiki/Intellectual_dark_web). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-25 20:08:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Why, exactly, would an AGI be necessary?

I'm not sure any human progress is *necessary*, unless you want to specify *for what*. AGI is said to be "the last thing we need to invent", because afterwards that AGI can make inventions for us. At least that's the idea. (Another idea is that it will kill us all, so we won't be doing any more inventing because of that; see /r/ControlProblem.)

> Would it not be possible and easier to instead try to make every highly specialized narrow-AI modular such that you could merge all of these into one general intelligent hybrid?

Let me preface this by saying nobody has built AGI yet, and there's disagreement about how to go about it. However, this suggestion crops up quite often, and doesn't seem to strike most researchers (including me) as a good approach. The problem isn't just that you'd need to have a huge number of narrow AIs (we can argue about whether it should be hundreds, thousands, millions or an infinite amount), but that you'd need some "glue" that holds them together and decide when to run which narrow system *and* somehow deal with situations that none of the narrow AIs were made for (i.e. it'd have to deal with novel tasks and situations, just as humans can). Essentially, this "glue" would have to be a full-blown AGI, so this doesn't really decrease the complexity of the problem. (Of course, being able to better use / interface with narrow AI systems is an advantage AGI would likely have over humans.)

However, an interesting perspective related to this is Eric Drexler's somewhat recent (and long) report about [reframing superintelligence as comprehensive AI services](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf).

> hence the need for an AI that would be able to perform well in tasks relating to all 9 different types of intelligence humans are believed to possess.

Dividing up the problem into pieces is not necessarily a bad idea, but to be clear: AGI should not just perform well in N tasks that are each associated with one "kind" of intelligence; it should perform at least as well as humans in pretty much all tasks, including learning new ones the designers never foresaw.

Gardner's [theory of multiple intelligences](https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences#Critical_reception) doesn't really seem to have a strong scientific basis. That doesn't necessarily mean it's also bad to (try to) build AGI based on it (after all, many approaches don't try to mimic the human brain), but it's something to keep in mind. The potential downside of this is of course that there really might be common things underlying each of these intelligence "modules" and then dividing it up may just be more work and add to your confusion. But it could also be the case that it is indeed easier to make several different modules to focus on different aspects of cognition. [OpenCog](https://wiki.opencog.org/w/CogPrime_Overview#High-Level_Architecture_of_CogPrime) is one project that takes this approach, but I think it should be noted that it's at least as much work to "glue" the modules so they can work together as it is to develop those modules.

Edit: If you're interested in this stuff, there's a [Getting Started with AGI section](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on /r/artificial's wiki. The [annual AGI conference](http://agi-conf.org/) is also currently ongoing and due to Covid-19 it's online and free. There's only one day left though. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-25 11:31:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> > we repeatedly see algorithms that perform better for white people not on people of color in healthcare

Sorry this reply is late, but I'm curious about your opinion and your impression of the opinions of the ML fairness field at large.

Which algorithm would you prefer:

1. an algorithm that has 95% accuracy on both black and white people, or
2. an algorithm that has 95% accuracy on black people and 99% on white people?

(please assume proportional false positive and negative rates and that the accuracy is not worse than what a non-AI solution would produce)

I can imagine that the answer depends a bit on the situation, so perhaps you can also say a bit about that (e.g. are there principled bases for making this choice?). E.g. for myself, my intuition is that #2 is definitely preferable in healthcare scenarios like cancer diagnoses, but it's not as clear in policing applications (I'd still want to catch as many criminals as possible, favoring #2, but if you disproportionately arrest black people that will mean more poverty and kids growing up without a parent which will likely increase crime again, so that might favor #1). But I have a hard time imagining that #1 is superior across (virtually) all scenarios, which seems to be implied a bit by the sentence I quoted.

(How) would your choices change if in option #2 the accuracies for black and white people were swapped?

Maybe this sounds like an unfair choice, because often improving accuracy on one axis (i.e. 95% --> 99% for white people) means sacrificing it somewhere else, so maybe the accuracy for black people should be lower to make the scenario more realistic (e.g. 94%). I'm also curious how that would change your answer. But in defense of my original scenario: we can perhaps imagine that the developers already did all they could think of to improve fairness and ended up with the 95/99 model, and the question is whether they should just pick the wrong answer for white people 4% of the time to balance things out.

This is not meant as a gotcha, but I'm just generally curious about your opinions and if there's a principled way to make such choices. (And I understand if you don't want to get back into an old topic, but I hope you will.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-25 04:14:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> Interesting post! I'm late to the party and don't really have time to respond, but just in case you're unaware I'd like to point you to the currently ongoing, currently free to virtually attend [annual AGI conference](http://agi-conf.org/2020/schedule/). Three days have already passed with just two remaining, but I believe you can watch the videos later as well. I'll also link to a [Getting started with AGI section](https://www.reddit.com/r/artificial/wiki/getting-started) on /r/artificial's wiki that has some more links to work from the AGI research community. This may or may not be your cup of tea, since the research culture is very different from ML, but at least they're explicitly and unambiguously focused on AGI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-24 10:57:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> The first post on the blog mentions "SlateStarCodex" is almost an anagram for "Scott Alexander". That may be what you're thinking of. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-24 09:53:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> I recommend just posting it somewhere and linking to it here so people who are willing to help you can do so immediately, and not have to suffer through an added level of indirection. You might also get multiple perspectives and some discussion. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-23 20:31:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know much about category theory and related math which makes it a bit difficult to follow your post. It's interesting to me though that things can apparently be phrased this way. The question seems quite clear though: how does an AI decide whether to (try to) change the environment or change its beliefs?

I think that in a direct sense, this is typically not a choice that AI can make. I think humans are the same. If I know that the football is currently at my foot and I want (to believe) it's in the goal, there is no way I can actually make myself believe this without seeing it, seeing my teammates cheer, seeing the referee acknowledge it and seeing the change on the scoreboard. We could imagine giving a change-beliefs action to an AI (e.g. a planning system), but it would quickly look like a bad idea as it would presumably choose change-beliefs over the more difficult (and less certain) bypass-opponent, kick-ball, observe-ball-in-goal and revise-beliefs-based-on-observation. This might be regarded as a form of wireheading, which *is* studied in the subfield of AI Safety. This is mainly because the affordances of AGI cannot easily be limited (e.g. even if you don't give an AGI a change-belief action, it could accomplish the same by reprogramming itself). However, it could also apply to simpler systems: e.g. a simple RL goalkeeper trained to never see the ball enter its own goal might just blind itself. I think something is included in the [AI Safety Gridworlds paper](https://arxiv.org/abs/1711.09883) ([video](https://www.youtube.com/watch?v=CGTkoUidQ8I)).

A perhaps more likely scenario is that the AI system knows that it's not really sure about something, and it has to choose between acting with the assumption that its belief is true (and the risk that it's false) or carrying out knowledge-seeking actions to verify or disprove its belief. This is researched as "reasoning under uncertainty" and "[uncertainty in AI](http://www.auai.org/uai2020/)". In the context of learning, this may also be somewhat related to active learning (where the learner asks for more information on the most useful/difficult/uncertain training samples) or the exploration-exploitation tradeoff in (mostly) reinforcement learning (where there's a tradeoff between acting on the currently-believed best but likely imperfect policy or exploring more to potentially make a better policy).

Another related issue might be attention. I don't think it would be common for an AI system to know about gravity in the environment, and then choose to ignore it for its plan to achieve its goals, only to be immediately foiled. I mean, I guess it could be uncertain about gravity and plan under the assumption that it's false (rather than choosing to seek better knowledge), but then I guess it wouldn't really believe strongly in gravity in the first place. However, a system with attention, that may at any one time attend and *not *attend to certain parts of its knowledge, could I guess fail to attend to gravity and incorporate that into its plans. But this particular example seems to be a bit out there.

Edit (much later): you could perhaps also look into imagination, counterfactual reasoning and maybe even artificial creativity ("what if there was no gravity?"). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-23 13:42:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> Wikipedia has a fairly extensive page on the [history of AI](https://en.wikipedia.org/wiki/History_of_artificial_intelligence). According to it, there were a large number of factors contributing to the first AI winter. It does say that "the field of connectionism (or neural nets) was shut down almost completely for 10 years by Marvin Minsky's devastating criticism of perceptrons", but this strikes me as a bit of an oversimplification. It's not like everything was awesome in the field of connectionism, then Minsky & Papert wrote one book, and everybody just gave up. But even if that was the case, it couldn't have caused the whole winter, because neural networks weren't nearly as dominant in the field as they are now.

If we believe that hype causes winters though, then we might think that Minsky's statements under the "Optimism" header might have contributed to the winter more than his critique of perceptrons. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-20 20:27:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Superintelligence has a chapter on this, but it's kind of dated now.

I don't really think much has changed since 2014. Other "dated" sources include Ben Goertzel's 2014 [overview paper](http://www.degruyter.com/view/j/jagi.ahead-of-print/jagi-2014-0001/jagi-2014-0001.xml) and Pei Wang's [Gentle Introduction to AGI](https://cis.temple.edu/~pwang/AGI-Intro.html). There were also [some workshops](http://agi-conf.org/2017/?page_id=24) on different approaches at the AGI conference in 2017 but I don't know if the videos are online (you can see some slides and text at the link I provided though).

Other than that, you could just look at books on researchers' individual approaches, e.g. by Ben Goertzel, Pei Wang, Marcus Hutter, Jeff Hawkins and Joscha Bach. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-17 10:15:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> I hope our wiki's [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) can be helpful to you. Programming, algebra, probability and statistics are certainly important and nice to know, but you don't have to wait to learn all of them to start your journey in AI. There are plenty of introductory courses/materials that don't have demanding prerequisites. (E.g. Udacity's Intro to AI course, or Russell & Norvig's book.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-17 10:12:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but the link you posted doesn't work, so I removed this. If your submission was about AI (which I cannot tell from the title) and you are [unaffiliated](https://www.infoageconsulting.com/post/project-clarity) with the for-profit company that this post is apparently hosted by, you may want to try submitting a new post with a working link. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-15 10:58:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> Most AI systems we have today are computer programs that are made to carry out a certain type of action and nothing else. *Explaining* would be another action, and a very complicated one at that.

But while *explaining* AIs aren't really a thing, *explainable* AI has become a bigger topic in recent years. The terminology is a bit messy, but we might say the simplest form of this is *interpretable* AI where we can easily interpret what the system did. For instance, if you have a rule-based AI with human-provided (and readable) rules, you can typically just look at which of the rules applied to a certain situation and understand the AI's action/decision. But especially if machine learning techniques like neural networks are used, this is not feasible. We can of course reconstruct the millions of calculations that were made, but they're typically meaningless to us and don't amount to a high-level understanding of how a certain input resulted in the associated (possibly erroneous) output. There are explainable AI projects that e.g. show you what a neural network was "paying attention" to in a video game as sort of a partial explanation of how it made its decision.

Understandable AI is important for a number of reasons, one of which is accountability (or responsibility). If something goes wrong, we want to know who's responsible/accountable, which is difficult if we can't tell what actually went wrong and who caused it. It can also be argued that the inability to assign responsibility with opaque AI systems is not just a practical issue, but also a moral one, because responsibility is often thought to require knowledge. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-15 10:36:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> If what you say is true, that still doesn't strike me as a reason to ban or remove submissions about Walton or his publicly available work. For comparison, companies like Google, Microsoft and Amazon have been convicted of lots of actually illegal activities, but we still want to discuss them.

The way to handle this is for user such as yourself to "set the record straight" with the insight you have on the issue. Although I would prefer if you brought evidence. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-14 18:31:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> This link works.

In your OP there's a backslash before the underscore in the video identifier. If I click on it, I'm just redirected to youtube.com without an error message. If I expand it (I think that's an [RES](https://redditenhancementsuite.com/) feature) and try to play it, it says:

> An error occurred. Please try again later. (Playback ID: SoRxK3h1sOX-t6Rd) [Learn more](https://support.google.com/youtube/?p=player_error1&hl=en) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-14 17:13:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> Congratulations on getting your tutorial accepted and good luck with it!

The video you linked doesn't appear to work. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-14 17:10:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> It depends on you define "bias" and this is IMO a great source of confusion. For one thing, the word "bias" already has a ton of different meanings in neural networks, machine learning, statistics, cognitive science, etc. When people discuss "bias" in the context of the ethics of AI, it would usually be more accurate (but not much less contentious) to say "unfairness", but many people don't do that. And then you get misunderstandings between people arguing there's no bias because the AI accurately reflects the training data (and/or reality) and people saying there *is* a bias because there's a disparate impact or something.

And even with "fairness" there's an issue that there are like 20+ definitions (I like [this paper](https://fairware.cs.umass.edu/papers/Verma.pdf)), which can typically not all be satisfied at the same time. At best, this can result into a discussion about which definition should be optimized for in a given situation, but at worst this is used as an excuse to ignore fairness altogether *or* as a universal always-true accusation to be used against corporations / use-cases the accuser doesn't like (or to prove a point).

Anyway, if you're saying that a set of symptoms points to a different most likely diagnosis depending on the ethnicity of the patient, then I think most people would not consider an AI system that gets this right "biased". For one group, it won't be biased because it gets the beneficial outcome. For another, it's not biased because it's not making a systematic error. In fact, if it would give the same outcome regardless of race, I think more people would call it biased. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-13 17:30:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> My guess is that /r/matlab is generally better for questions about programming in Octave (or maybe /r/learnprogramming).

> y_Vec = (1:num_labels)==y;

`1:num_labels` creates this vector: `[1 2 3 4 5 6 7 8 9 10]` (because `num_labels=10` apparently).

`==` then compares this to `y`. Your code doesn't include the definition of `y`, but it should either be a single number or also a vector of length 10. If it's a single number, each number in `1:num_labels` will be compared to it, and the result will be 1 if it's the same, and 0 if not. For instance, `[1 2 3] == 2` results in `[0 1 0]`. If `y` is a vector of the same length, the comparisons will be pairwise. For instance, `[1 2 3] == [1 2 1]` results in `[1 1 0]`.

The result of the comparison is then assigned to the `y_Vec` variable, so `y_Vec` is a vector of length 10 that contains ones and/or zeros.

> And that also equals to y_Vec = zeros(m,num_labels); for i = 1:m y_Vec(i,y(i)) = 1; end

Kind of. In the first expression (which I just discussed), the size of `y_Vec` is 1 by `num_labels`, but here it's `m` by `num_labels`. Also, `y` had to have size 1 (i.e. be a single number) or size `num_labels`, but here it apparently has size `m`.

This second expression is doing the same as the first one, but `m` times, and apparently `y` above should have been a single number. So basically, `y_Vec` from the first expression corresponds to `y_Vec(i, :)` from the second.

`A = (1:10)==5` and `B = zeros(1, 10); B(5) = 1` have the same result. For `A` you compare the numbers 1 to 10 to the number 5. Only the fifth one is equal, so will be a 1 and the rest is unequal and will be zeros. For `B` all 10 numbers are set to zero, and then the fifth one is manually set to 1. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-12 12:26:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's not really clear to me what you are suggesting. The cost function *is* computed most of the time. That's how we know how well the current configuration of the neural network is doing.

Since we typically don't know the best values for all of the connection weights, one idea for improving them is to change them slightly in the direction that would decrease the cost. But for any particular weight, does that mean increasing or decreasing it? If you compute the cost function's derivative w.r.t. that weight, that will give you the answer to that question, and in addition to an indication for how much it should be adjusted.

Maybe you're asking: why not just increase it by some arbitrary amount, re-compute the error function, see if things improved, and if not then decrease it? The problem is that you'd have to do that for each weight, of which there tend to be a lot. With backpropagation you only have to do the feedforward calculation once, and then you get fairly simple calculations per node of the network (of which there are far fewer) and *really* simple calculations per weight.

The difference is especially pronounced if you're doing batch updates of size N, because you have one N feedforward passes and 1 backprop pass. But with the idea in the previous paragraph, you'd have to do (roughly) W\*N feedforward passes to adjust all W weights. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-11 16:39:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you created a safe AGI, you're good, but I guess this question is about how to make your unsafe AGI safe and/or gain sufficient confidence that it's safe.

Presumably the best people to help you with this are the ones who are already concerned about and working on AI Safety at institutes like FHI, MIRI, CSER, LCFI, CHAI, FLI, etc. or perhaps even at OpenAI or DeepMind. The problem is convincing them to spend time on you and your idea. I guess this starts by demonstrating a high level of competence. Do your insights into AGI make it possible for you to build safe less-capable AI systems that are nevertheless so impressive that they'll turn heads? Could you publish papers on (parts of) your system, or maybe just on other AI things? Otherwise, maybe try to personally get to know some people who work on AI Safety by going to conferences, e-mailing them insightful things/questions or maybe just apply for a job/internship there.

At some point, I doubt they'll be able to help you much if you can't reveal much about the architecture and algorithms. So another problem is for you to decide who to trust. Presumably many people will want to "seize" your AGI for their own. This might apply even to "good" people/organizations, because they calculate the world will be better off with *their* values rather than *your* slightly different ones. But maybe you will just need to compromise on this a bit.

I'll end this with the obligatory remark that you probably didn't invent AGI, and a pointer to /r/ControlProblem which is more specifically concerned with A(G)I Safety. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-11 16:27:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> > he probably refers to "artificial superintelligence"

I agree.

> as coined by tim urban

Tim Urban is a blogger who spent three weeks researching the issue by reading books like Nick Bostrom's "Superintelligence". I'm not sure, but the earliest modern mention is in [Bostrom's 1997/1998 paper](https://www.nickbostrom.com/superintelligence.html) so I think *he* may have coined the term. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-11 14:55:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> What are your alternatives? I think the job prospectives for NLP are quite good, so I doubt it would be much better if you did something else.

> I'm just really passionate about it

That's great! Then go for it. Being passionate about your job/field will help in getting a good job as well. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-10 21:16:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's no such thing as certainty. Even if you think you have a mathematical proof, there's some probability that you made a mistake. Or that someone who's less careful develops AGI faster. All we can reasonably do is increase the probability of success across a wide variety of possible scenarios, and I argue that this does exactly that.

I also acknowledge that this is not the whole solution. Eventually I think we need something that, unlike containment, scales up to arbitrary levels of intelligence. The containment is just there to buy us time and help develop such a solution. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-10 18:05:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't have any particular insight into the job market, but I have the sense that you have nothing to worry about. AI still seems to be in demand, and there are a ton of applications that involve language. Just think of how many (automatable) tasks/jobs involve language. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-10 11:15:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> My suggestion to work more on containment is not meant as the ultimate solution to the control problem; just something concrete we can work on that will quite likely buy us some very valuable time to solve the real problem. Furthermore, even if we solve AI alignment the "proper" way, I imagine we'd still want to try to test it out in a "box" to get more (but not infinite) confidence that we didn't accidentally overlook something.

One major difficulty with working on AI Safety now is that we don't really know what an AGI architecture might look like. For instance, your proposal seems very different from making a generally intelligent computer, and even then there are a ton of options. Knowing the architecture and being able to experiment with (baby) versions of it in a limited setting, should make it easier to make that system actually safe/aligned.

This isn't guaranteed to work of course. But I think a common mistake people make, is that they think this is putting "God" in a box. But almost certainly, AGI won't start out at God-level: it will need to learn and maybe even recursively optimize itself for that. Learning is dependent on the experience we allow it to have (or fail to stop it from having), and accessing its own code could likely be detected (maybe God could do it covertly, but if it needs to do this *in order to* become smart enough to deceive us, we're in a good position).

And yes, the "strength" of the prisoner will be finite, but so is the "strength" of the prisoner (derived from its finite intelligence). I think that by spending some effort on this, groups of researchers could come up with quite strong containment options, which would be capable of containing AI systems well beyond regular human-level intelligence (but there will be a point of intelligence that's enough to break out).

I usually also stress the importance of protocols for interacting with the AI, because of course we build it for a reason and we want to learn from it (in order to make it safe). Here too, I think there is a lot to win. If you believe Eliezer Yudkowsky, then he can talk himself out of an AI box (and he's just a human). But what if the gatekeeper doesn't have the power to let him out? What if the gatekeeper doesn't even know who does? What if there are different gatekeepers all the time, and they don't even know about each other? These are things I just came up with off the top of my head, and I'm sure they're not perfect, but they certainly seem to make escaping *harder*. And I'm sure groups of researchers can come up with better things.

In the end, it's all about improving our chances. There's no certainty. There are scenarios where containment won't help, but I think it will *likely* help (a bit or a lot) in most scenarios. Especially, I think it helps in a broader range of scenarios than many other approaches, which are often about developing a specific system. For instance, it's not just enough for your proposed solution to work: you also need to develop it before somebody else develops (unsafe) AGI. In such a scenario, I think they could probably be persuaded to use some easy-to-use ready-made containment as a basic safety precaution, but your alternative approach (or any other) won't be (as easily) applicable. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-09 17:32:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's a journal called [Progress in AI](https://www.springer.com/journal/13748). Otherwise you might also try [AI Review](https://www.springer.com/journal/10462) which has large number of reviews/overviews/surveys, which talk about the state of the field (or a subfield/subtopic) and might track some progress. Of course, other journals also publish reviews and articles on progress every now and then.

You may also want to check out the upcoming (but postponed) IJCAI workshop on [Evaluating Progress in AI](http://dmip.webs.upv.es/EPAI2020/) and previous iterations whose papers are already online ([2018](http://dmip.webs.upv.es/EGPAI2018/), [2017](http://dmip.webs.upv.es/EGPAI2017/), [2016](http://dmip.webs.upv.es/EGPAI2016/)). You might also want to follow organizers José Hernández-Orallo and Nando Martínez-Plumed, as well as Ross Gruetzemacher.

Some other links that might be of interest:

* [State of AI](https://www.stateof.ai/)
* [100-year Study of AI](https://ai100.stanford.edu/)
* [AI Index](https://hai.stanford.edu/research/ai-index-2019)
* ["Humies" Awards For Human-Competitive Results](https://www.human-competitive.org/)
* [AI Impacts](https://aiimpacts.org/surveys-on-fractional-progress-towards-hlai/)
* [Papers with Code Benchmarks results](https://paperswithcode.com/sota)
* [EFF's AI Progress Measurement](https://www.eff.org/ai/metrics) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-09 14:14:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> It seems largely abandoned, but there should be more work on containment IMO. A lot of AGI/AI/ML researchers currently don't work on aligned AGI from the bottom up. If they beat Safe AGI researchers to the punch (which seems likely because I think they have an easier task and are more numerous), they might start worrying about safety a little, but probably not enough to throw the greatest invention of all time out the window and start from scratch with safety in mind. However, they might be willing to take some precautions, if they're not too difficult to apply.

That's why I think some effort should be spent on developing tools and protocols for containment. It seems like this would be useful for any AGI system (even if you *think* it's aligned), and relatively easy to do in a way that's agnostic of how that system might work. AGI systems would probably need some time to learn and/or self-improve to become superultraintelligent enough to break out, and in that time we could monitor, study and stop them. This gives us time to do safety research on an actual AGI system. So that we can hopefully develop a next version that's actually safe. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-09 10:11:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> Perhaps a recommender system that recommends (new) restaurants to people? You can do it with collaborative filtering ("people like you also liked *this* restaurant"). You can just define "people like you" as "people who went to the same restaurants", so you don't even need to have any information about the specific users or restaurants. And if you do have more data, you could expand it to take into account user characteristics (gender, location, wealth, etc.) and restaurant characteristics (static: location, cuisine, cost, or dynamic: number of predicted available tables). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-08 16:54:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't really have one great source. It seems AI governance is not that big a field yet, and its literature is rather distributed across different publications. Often these aren't really academic either. For instance, some of the most interesting documents in this regard are the national AI strategies of various countries. Also, it seems that relevant institutes like FHI's GovAI center, OpenAI's policy group, the Partnership on AI, etc. often just put papers/reports online without going through the academic publishing progress. And when they do publish, it's often not in the technical venues you mention (although sometimes they do), but just as likely to be in e.g. governance/policy journals.

To stay up to date, you can follow these relevant groups and researchers, or e.g. set up Google (Scholar) alerts. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-05 17:34:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but you linked to an entire channel and not specified who you're talking about. Please re-submit the link to the relevant podcast (episode) with an informative (less clickbait-y) title that mentions who is being interviewed. Thanks! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-05 17:25:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> Interesting!

I have a question: Each quest is described by a Title, an Objective and a Description part. You then ask a bunch of questions about the description. Are the questions about everything that's written about the quest (including the Title and Objective), or just about the Description part?

The survey looks pretty good to me, but it's a bit of a shame to me that you didn't include a question about whether we think each quest is created by a human or GPT-2. Also, maybe you could have asked if the Title, Objective and Description make sense together. Maybe next time. ;)

Good luck with your research! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-05 14:18:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> Take a look at the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on our wiki.

The airline industry is of course huge. Many of the involved companies have similar processes to companies in other industries, that can be automated with AI. Data mining/science can be used to gain insight into and optimize business processes, chatbots can be used to partially automate customer service, planning/scheduling software can be used to make schedules for people (and/or airplanes), generative AI might be able to help design parts/materials, etc.

As you probably know, airports have a lot of surveillance. Computer vision technology can/could be used to (help) spot people (e.g. who are wanted criminals or on a no-fly list), suspicious behaviors, weapons (maybe), or left-alone luggage/packages. There might also be opportunities for automatically sorting luggage, and a colleague of mine once worked on a project that I think was designed to detect people on the luggage conveyor belt because apparently people occasionally fall on there and then you want to stop it (I'm guessing they also have manual options for this, but perhaps that wasn't enough).

Of course, a lot of these things are very privacy sensitive and potentially prone to discrimination. This might mean some of them are not deployed everywhere, but it also means there's a demand for solutions, research and development that mitigates these issues while still retaining the benefits.

If you look at the process of actually flying airplanes, the industry is naturally very cautious and conservative. From a safety perspective, the current workflow just works very well. As you may know, flight is by far the safest mode of transportation per traveled  kilometer. I worked a bit with an air traffic control (ATC) company at a relatively non-busy airport, and they were interested in using more AI for efficiency reasons, but it was imperative that it didn't disturb the ATC officers' (ATCOs) workflow because that might be dangerous. Even assistive technology for e.g. arrival control and conflict resolution would have to be introduced very gradually and in a way that gained the trust of the ATCOs (which meant we needed explainability). More drastic things could only be done in the periphery of the main work of keeping aircraft safe, e.g. they suggested a system to monitor/optimize ATCO workload and mental state so that their area of responsibility could be reduced and partially taken over by another ATCO if it became too much (and they could get a larger area if they were bored). That, and making a planning/schedule for how to fly the next day.

I don't know as much about AI on board aircraft, but I'm guessing they're also reluctant to mess with the auto pilot (which is actually quite simple from an AI perspective). At the very least, you'd need to abide by a ton of regulations, extensively test everything, and provide huge amounts of documentation. I also suspect they'd want any such system to be explainable/interpretable. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-04 12:57:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> Anybody can submit a paper to a journal. Ideally the review process is double-blind, meaning the reviewers don't even know who you are, so they have no idea what institutions you're affiliated with (or not). However, as you say, you must confirm to the same standards of academic writing as anyone else, and this often turns out to be difficult for people who weren't trained to do that. But if you can manage that, and your paper is good, people will generally respect you for publishing in a good journal. Being treated as a pariah seems pretty rare, and mostly depends on other factors.

Good luck with your submission! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-04 12:14:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> > definitely not a developer’s perspective.

>

> I want to know such that I can talk to devs

These things are a bit contradictory. I understand you don't want to *be* a developer, but if you want to talk to them well, you should understand what they're doing. And probably the best way to do that is to get some of the same experience and education. Not a lot of course, but just some introductory stuff. You can check out the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on our wiki for that.

For a non-developer's perspective, you can also look at Andrew Ng's AI for Everyone course. I also saw some books on applications in a recent [books thread](https://www.reddit.com/r/artificial/comments/gn5tuk/must_read_artificial_intelligence_books/), but I haven't read them. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-04 12:03:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for taking the criticisms and downvotes in stride. I believe you're operating in good faith and not trying to spam. However, I do want to point you to Reddit's rules against [self promotion](https://www.reddit.com/wiki/selfpromotion). I wish you the best of luck with your blog, but you shouldn't just be posting all of your own content to Reddit. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-06-02 11:10:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> See [here](https://aiimpacts.org/category/ai-timelines/predictions-of-human-level-ai-timelines/ai-timeline-surveys/) for a bunch of professional/academic surveys. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-29 12:07:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Since A.I. is multidisciplinary was your workload more focused deep learning, health, robotics or NLP, etc.?

My AI master's program was "focused" on AI and didn't make a real choice to go into any of its subfields as you suggest here. I had courses on programming (some of these may have been elective), probabilistic graphical models, business intelligence, knowledge-based systems, embodied & embedded cognition, human-machine interaction, information retrieval, pattern recognition, and some electives. Then for my thesis, I did brain-computer interfacing.

AI is indeed multidisciplinary, and it can be approached from different angles. Usually it's mostly approached from the computer science angle, but cognitive science, neuroscience, mathematics and philosophy are some other options. Of these, I think mathematics would have been most useful to me, but my BS+MS was more focused on CogSci, CS and a little bit of neuro.

Since I'm mostly interested in AI (or AGI) itself, I don't think I would have liked it to overlap more with e.g. quantum computing or genetics or anything. I also wouldn't have liked a focus on one particular method like deep learning, a particular subfield like NLP/robotics, or a particular application area like healthcare. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-28 16:41:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> The necessary tools for what?

If you're interested in AI, choose AI. If you're interested in Computer Science, choose CS.

AI is often (incorrectly IMO) seen as a subfield/specialization of CS, so if you're not sure you want to specialize, it's probably the safer bet. Furthermore, most universities don't offer an AI program, so if you want to get an AI-related graduate degree (or job), you'll qualify with a CS degree. With a more rare/unique/"weird" AI degree you'll probably have a harder time getting into another CS subfield like encryption and maybe a slightly harder time getting into a programming job (while probably having an easier time getting into data science, statistics, analytics, cognitive science, philosophy, etc.). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-28 13:45:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> You might be interested in Andrew Ng's [AI for Everyone](https://www.coursera.org/learn/ai-for-everyone) course. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-28 13:37:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you look e.g. [here](https://www.probabilitycourse.com/chapter11/11_1_1_counting_processes.php) the time parameter *t* of the counting process *N(t)* is defined to be continuous. That means that for every real value of *t*, there is a corresponding discrete/counted value *N(t)*. In your example, *t* is just the number of the exam question, which is discrete, so you naturally think of *t* as discrete as well. However, this doesn't disqualify it from being a counting process. It just means that if you set *t* to some non-discrete continuous number (e.g. 2.8908439084908) between two discrete numbers (i.e. 2 and 3), the count will be the same as for the lower discrete number (i.e. 2). So in the example N(2.8908439084908) = N(2). ɛ is just supposed to be a really small number, so 3-ɛ just means "3 minus some tiny number" or "2.*something*", and what I was really just trying to say is that *N(t)* = *N(2)* for all 2 < *t* < 3.

But to be clear, all this means is that it doesn't matter if you think of *t* as a discrete value or not: it's still a counting process. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-28 13:28:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are examples of things that are really independent, such as how many times a coin comes up heads after *t* throws. This definitely has *causal* independence (the first kind), and there's also *correlational* independence (the second kind) *if you know the weighting of the coin*. If you don't know the weighting of the coin (i.e. you don't know if it will come up heads 75%, 25%, or 45%, etc. of the time), then each time it comes up heads, it should increase your belief that it's weighted towards heads a bit, so the probability that the next throw will be heads as well is increased.

Your bus example also seems good, but like I said, you have to wonder whether the error you make by assuming the process is independent is actually significant. What proportion of the time does someone dangerous actually get on *and* this is seen/recognized by others *and* this causes them to not get on the bus? That might very well be negligible. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-27 15:39:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> Check out the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on our wiki. Python is probably the most useful language to learn, and I'd start with Udacity's Intro to AI (Peter Norvig & Sebastian Thrun) or Coursera's Intro to Machine Learning (Andrew Ng). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-27 11:43:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> Yeah, I think it depends on you. There are many options. See also the second paragraph of my previous post. Will you primarily be a physician with some AI knowledge? Does that just mean you know how to work with AI tools (e.g. a radiologist who knows how to work with computer vision algorithms), or are you also going to help researchers/developers create new AI tools (e.g. by telling them about the actual difficulties in the domain, providing/annotating data and/or testing out their tools)?

Or do you primarily want to be an AI researcher/engineer/developer who happens to be specialized in the healthcare domain? In that case, there are still many options. In healthcare, there are diagnostic tools which can be computer vision-based or expert system-based (or something else), there are robots who can help patients with a variety of tasks, monitoring systems, drug creation, etc. Or maybe you could even do more fundamental research that's further removed from applications, but builds on your knowledge of the human body/brain.

Of course, to make use of your background, it makes the most sense to work on AI in the healthcare domain. But there are many </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-27 11:16:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> > The number of correct answers in an exam is an independent counting process ? I think so, but some people have told me that there are cases where is an independent process and cases where not.

I would guess that in practice most exams are not independent. There are two main ways in which this can happen:

1. You have connected questions like "Q1) What's 1+1?" and "Q2) What's that number multiplied by 2?". Then N(2)-N(1) is clearly going to be dependent on N(1)-N(0): if you got Q1 wrong, N(1)-N(0)=0, you'll almost certainly get Q2 wrong too, so N(2)-N(1) will be 0 too.
2. There can just be correlations. "Q4) 5435\*4423?" does not depend on "Q3) 4423\*5435?", but it's pretty easy to see they'll have the same answer, so you'll probably get both right or both wrong.

These are very extreme examples, but they can be more subtle too. For instance, if there's another question that multiplies two different four-digit numbers, it's still pretty likely that a given student either is or is not able to do all of these calculations. And even more generally: a student who studied for the exam will be more likely to get each answer right, which creates a statistical dependency (correlation) between all questions.

There's also a direct dependence in time: maybe getting a wrong answer on a certain question takes more/less time than getting a right answer, which will leave less/more time for the next questions, making it less/more likely they'll be answered correctly. A similar argument can be made for student confidence and carefulness.

The problem here is that independence is a very strong condition that will likely not be met *exactly* (just like it's unlikely you'll find two people of *exactly* the same height if you go down to femtometers). The real question is probably: can this process be effectively modeled as an independent counting process? Or is there a significant error if we model this as an independent counting process?

> Also I remmeber that only a peocess is a counting process if his time is continuos and in this case the time is the number of the answer, so is discreet.

I don't think this is disqualifying. It just means that N(2) = N(2.8908439084908) = N(3-ɛ). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-27 10:45:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> I was also going to mention *Human Compatible* and *Our Final Invention*. Another book on this topic is Roman Yampolskiy's *Artificial Superintelligence: A Futuristic Approach*. There's also Toby Ord's *The Precipice* and Tom Chivers' *The AI Does Not Hate You*. (Read their descriptions, because they're all quite different.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-27 10:31:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> What would controllable (or uncontrollable / out of control) image synthesis look like? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-27 10:27:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't have anything specific for you, but just look for people (researchers) who are applying [AI in healthcare](https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare) (I linked to a fairly long Wikipedia article, but you should seriously just Google "AI in healthcare" because there are a lot of results and resources). When you're developing AI for some application domain, it's still usually necessary to have a large amount of domain knowledge (which is usually obtained by the AI people working with domain experts). This is where you make good use of your medical background.

I don't know how far you want to go into AI. Do you just want to be a domain expert who works with AI people? Then I'd probably find nearby universities and/or hospitals, see if they're doing anything in this area, and contact them. If you want to be an AI researcher/developer yourself, your best bet is probably to get a related university degree. In that case I would look for universities that are indeed fairly involved in AI for healthcare. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-27 10:17:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> Well, AI systems and algorithms can also be viewed as "objects" which can be made easier to understand and use. There is actually quite a bit of demand for that.

With deep learning in particular, you get systems where humans don't really understand on a meaningful level how/why they make their decisions. There is quite a bit of research into Explainable AI (XAI) (or "interpretable AI"), which is typically about technical solutions that make deep neural networks more understandable. This seems to be in the first place aimed at technical people who made the algorithms, but there are also more general calls for transparency from the population. This would (ideally) mean that information is not just made available about the systems/algorithms, their decisions, and/or their data, etc. but also that this is done in an easy to understand and view manner.

On the easy-to-use front, there are many libraries to make deep learning easier to use for programmers, students and researchers. There are also platforms for less technical people to make their own machine learning models (e.g. [Teachable Machine](https://teachablemachine.withgoogle.com/)).

I don't really know anything about Ergonomic Psychology (EP) and how an education in it will help with any of this, compared to for instance an education in AI, machine learning, computer science or data science. I would guess that EP doesn't include anything about deep learning, and you'd probably need a pretty deep understanding of it to be able to contribute to the above mentioned areas. Of course, there are other ways to learn about this (e.g. through online courses, electives or self-study). And then I guess you'll have quite a unique profile to work on these issues. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-25 16:56:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> How is this about AI? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-25 14:07:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for letting me know. "Previous users" from a previous, separate experiment, or users who did this same experiment before me? If it's that, I'm not sure this is a great idea because it means different participants will have a (slightly) different experience and scoring, which may make it hard to combine or compare their scores.

Also, I'm kind of wondering how you're determining this time. Is it the average time of previous participants, or e.g. the tenth percentile of reaction times? And are you filtering out response from users who went to get a cup of coffee while the experiment was running? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-25 11:49:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> I couldn't find many free online courses. Just [this one in Spanish](https://www.coursera.org/learn/computo-evolutivo). If you're okay with a book though, I know some (offline) courses that use Eiben & Smith's Introduction to Evolutionary Computing. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-25 11:25:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> They're both search algorithms, but there are some important differences. While we might say the assumption that "your opponent takes the worst possible action for you" is a "heuristic" in the sense of a "rule of thumb", it is not really ranking alternatives by likelihood in order to speed up the search and preventing you from having to look at any alternatives. You can get that by adding the alpha-beta pruning heuristic though. But minimax itself (without additional heuristics) is most similar to straight-up depth-first search.

Another big difference is that in minimax there is an opponent/adversary so the searcher/planner doesn't have control over all of the moves.

Also, A\* has different requirements on the problem you're trying to solve and the knowledge that's available about it. Namely, all choices need to have a known non-negative cost, and the cost of a whole solution/path is the sum of those costs (and of course you have to know which nodes are terminal). For minimax, you just have to know the "scores" for the terminal nodes (i.e. when the game ended). If you combine it with iterative deepening and you set a maximum score, you can find how to achieve that score (if it's possible) with the least amount of moves, but you can't really assign different costs to different moves. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-25 10:58:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> I tried participating for a while, but after a while I just got too annoyed. During the training session, it felt like I just got the same two alternating questions a million times (although after a long time, they finally changed). But what annoyed me the most was the "Please read the question" that decreases my score. There's a real tension between trying to answer as quickly as possible, and guessing the arbitrary amount of time you set after which I'm allowed to answer (which is way longer than I need to read the pertinent parts of the question). Something to keep in mind is perhaps that the question can be read pretty quickly, because you pretty much just need to read two key words (true/false and e.g. red/black, hearts/spades/diamonds/clubs, special, etc.).

Another thing I found a little weird is that there doesn't seem to be a logic to how the computer asks questions. However, this is probably not true. After being asked if the card is red, I wouldn't expect being asked if it's black, and then again if it's red, because the questioner should already have this information. It might make sense though for a lie detector that wants to see if there are discrepancies in answering speed.

You clearly put a lot of work into this, and I appreciated the detailed description of what to do (although you may want to check it for typos and small language mistakes). I might come back to this later when I'm feeling more patient. Good luck with your project! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-25 10:37:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not sure if it's exactly what you're looking for, but Nils Nilsson's The Quest for AI is a good book on the history of AI. It's also 10 years old, so it doesn't mention deep learning. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-22 17:19:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> This doesn't *exactly* answer your question, but Rob Miles asked Anders Sandberg something similar in [a recent video](https://youtu.be/nKJlF-olKmg?t=73). Specifically, he was talking about how King Midas wished for everything he touched to be turned into gold, and figured that when Midas touched the ground (e.g. when he dies because his lungs turn oxygen into gold before absorbing it) the whole Earth would turn into gold. He asked Sandberg what would kill us first as a consequence, and Sandberg seems to think the 3.5x increase in gravity would play an important role.

Like I said, not *exactly* what you asked, but if you're into random science questions like this, it may still be interesting. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-22 14:57:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm fairly sure any bachelor program in AI (or CS for that matter) will  have some introductory programming courses that don't assume any prior programming experience. They're designed to come right after high school, and most high schools don't offer programming courses. Of course, people with programming experience are going to have an easier time with these courses and possibly other ones.

I think that if you can pass that math test, you won't have anything to worry about in terms of prerequisite *knowledge*. What concerns me a bit more is why you dropped out of your previous *two* studies. Was it really that both topics which you yourself chose didn't do it for you, or was university itself just not a good fit for you? I think you should ask yourself what's different this time that will result in a different outcome. Are you more passionate about AI than about finance/literature? Did a few years of aging make university more palatable or necessary for you (this one actually seems to be true for a lot of people)? I'm not saying this to discourage you; you should just think about it.

Generally speaking, it's always very difficult to answer for other people whether doing something will be "worth it" for them. Why do you want to study AI? What do you hope to get out of it? Does living in the Netherlands for a few years appeal to you anyway, or is it more of a sacrifice for you? What do the financial costs mean for you?

I don't see any reason to think you won't succeed if you decide to study AI in the Netherlands. Your lack of programming background shouldn't matter, and you're certainly not too old (actually, slightly older students tend to be much more motivated and better organized in my experience).

Edit: Oh, and I don't know about making music on the side. How much time does that take? In my experience, most Dutch students comfortably have time for a hobby and extensive social life, but it probably shouldn't take up multiple days of your week. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-20 16:18:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> Amazon links from left to right, top to bottom:

1. [Marr (2019), *Artificial Intelligence in Practice: How 50 Companies Used AI/ML to Solve Problems*](https://www.amazon.com/Artificial-Intelligence-Practice-Successful-Companies/dp/1119548217)
* [Rothman (2018), *Artificial Intelligence by Example*](https://www.amazon.com/Artificial-Intelligence-Example-intelligence-artificial-dp-1788990544/dp/1788990544)
* [Stone (2019), *Artificial Intelligence Engines: A Tutorial Intro to the Math of Deep Learning*](https://www.amazon.com/Artificial-Intelligence-Engines-Introduction-Mathematics/dp/0956372813)
* [Eckroth (2018), *Python Artificial Intelligence Projects for Beginners*](https://www.amazon.com/Python-Artificial-Intelligence-Projects-Beginners/dp/1789539463)
* [Wilkins (2019), *Artificial Intelligence: An Essential Beginner's Guide ...*](https://www.amazon.com/Artificial-Intelligence-Essential-Beginners-Reinforcement/dp/1950922510)
* [Yao, Zhou & Jia (2018), *Applied Artificial Intelligence: A Handbook for Business Leaders*](https://www.amazon.com/Applied-Artificial-Intelligence-Handbook-Business/dp/0998289027)
* [Tegmark (2018), *Life 3.0: Being Human in the Age of AI*](https://www.amazon.com/dp/1101970316)
* [Russell & Norvig (2020), *Artificial Intelligence: A Modern Approach*](https://www.amazon.com/Artificial-Intelligence-A-Modern-Approach/dp/0134610997)
* [Lee (2018), *AI Superpowers: China, Silicon Valley and the New World Order*](https://www.amazon.com/AI-Superpowers-China-Silicon-Valley/dp/132854639X)

Aside from #8, I don't think any are really *MUST READS*, although they might be interesting (I haven't read most of them). However, the current top comment's suggestion that they are *completely outdated* is silly. These books are at most two years old. Yes, the field moves fast, but 95% of what was true in 2018 is still true now. And even if e.g. China's position vis-a-vis AI has changed in the past two years, your understanding of that will probably be helped a lot by understanding what it was in 2018.
Books are typically designed to give a considered overview of a phenomenon or to teach you the basics, not to tell you about the bleeding edge that will be different as soon as it's published. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-18 18:26:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> You can discuss it here as well, but /r/ControlProblem is dedicated to the issue of AGI "going rogue" (i.e. how to remain in control of such an AGI). You might find some interesting information and introductory materials on their wiki and side bar.

Some things that stand out in your question:

* **Supercomputer**: probably, but not necessarily. We don't know how much computation would be necessary for AGI and most researchers seem to agree that we mainly just don't have the ideas yet for how to create it. (i.e. we don't have the *soft*ware)
* **Consciousness**: consciousness is sometimes divided into [two concepts](https://en.wikipedia.org/wiki/Consciousness#Types_of_consciousness). Access consciousness (AC) is "the phenomenon whereby information in our minds is accessible for verbal report, reasoning, and the control of behavior". Phenomenal consciousness (or sentience) is subjective experience (qualia etc.), and it's basically the parts of consciousness that don't affect behavior. It seems that you can't have AGI without AC (so it wouldn't suddenly get it), and sentience is irrelevant to behavior (such as going rogue).
* **Bug**: you could call it that, but it doesn't seem like a "bug" (as the term is conventionally understood) is necessary to create harmful AGI. If you believe that social media are polarizing society or video games have harmful addictive effects or something like that, it's not really due to programming errors. It's just that this is the (side) effect of such systems as they interact with people or societies. Similarly, it seems that if you just have a superintelligent AGI, it doesn't require a "bug" to be harmful. Rather, you'd have to solve the control problem (sometimes called "value alignment problem") in order to *not* make it harmful. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-18 11:48:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> Good question in general, but if it was "increased competition", then why are Djokovic (1987), Nadal (1986) (and Federer (1981)) still winning all the grand slams? It's honestly insane how much these three have dominated and are still dominating:

* Australian Open: 14/15 wins since 2006
* Roland Garros: 14/15 since 2005
* Wimbledon: 16/18 since 2003
* US Open: 13/16 since 2004

The remaining Grand Slams in the listed periods were won by 3x Wawrinka (1985), 2x Murray (1987), Del Potro (1988) and Čilić (1988). So basically, nobody born after the 1980s has ever won a Grand Slam tournament. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-18 10:01:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> Yes, going to university and picking the right classes is a good way to get a job in the field you want, and it sounds like you're picking the right classes. It's perhaps a bit redundant, but there's also some advice on the [wiki](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-18 09:56:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's okay to post questionnaires about AI here, but it's not clear to me how this is related to AI. Could you explain? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-15 23:06:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thank you for your answer. You clearly know more about philosophy than I do, so this was interesting to read. However, there are a lot of things you say that I either don't understand or disagree with.

For the record, I lead my life as a (barely agnostic) atheist who assumes other people are also sentient. I should maybe also mention my main interest is this is about AI, because many people have some intuition that intelligence and sentience are linked and this seems important to know in order to judge how we should treat intelligent machines.

I'm not a pluralist, but my position is one of agnosticism. I don't really get why people can't say "neutral monism (or <insert theory>) sounds the most plausible to me" or something to that effect, rather than "I'm confident <insert theory> is definitely true". But anyway, it's interesting to hear arguments about the plausibility of these theories.

> Alas, people have a curious ability to continue trolling scientists with "okay, why does it feel like anything to be me but doesn't feel like anything to be rock?" and other such nonsense.

Why is this nonsense? I guess the panpsychist answer is that it does feel like something to be a rock, but then the question is still why/how/how could we measure this? But when I have bad days (like you), I think the scientists see this as a trolling question because it's one they can't answer, but that's exactly the point. I get the feeling they want to define "invalid/nonsensical questions" as "questions science can't answer", which means there are by definition no valid questions that science can't answer, but this is obviously cheating.

> [Current science] has a lot to say on [sentience/qualia/phenomenal consciousness], even with a pretty deep philosophical grounding, check out The Feeling of Life Itself: Why Consciousness Is Widespread But Can’t Be Computed for example.

I haven't read the book, so perhaps I shouldn't comment, but it's not clear to me from the summary that science is really addressing sentience here. A scientist, sure; scientists have lots of opinions. The summary mentions psychology and neuroscience, but these just operate on *supposed correlates* of sentience (or access consciousness). The rest seems to be philosophy, rather than science.

> I believe that Tononi is on to something (despite his actual mathematical model being rather convincingly torn apart by Aaronson):

I'm also convinced by Aaronson. Also, IIT strikes me as philosophy rather than science. How could you empirically verify that IIT's predictions about (lack of / amount of) the sentience of a rock or computer are true? It seems to me that you can't, unless you already accept that IIT is true, which is circular.

> we can't just reconcile our personal experience and knowledge of its precise correspondence to material substrate without accepting some degree of pantheism, some level of proto-consciousness in all causal interactions;

I don't get what you're saying here. I'm guessing this personal experience and knowledge refers to the fact that what I subjectively experience seems to be quite related to what happens to (the physical/material parts of) my body. I really don't get how this is supposed to imply pantheism or some level of proto-consciousness (what?) in all causal interactions. It seems to me that it could be the case that something related to sentience is monitoring *some* but *not all* causal interactions (whether that something is a physical/functional part of the brain or some kind of pluralist soul).

> an actual impossibility for there to be both a philosophical zombie and an atom without potential to participate in a self-aware system.

Again, I don't see how this follows. First of all, where did "atom[s] without potential to participate in a self-aware system" come from in this discussion? (And are you using "self-aware" as a synonym for "sentient" here? I will assume so.) It doesn't seem like either physicalism or pluralism (necessarily) posits the existence of such atoms. Even pluralists might say that every atom has the *potential* to participate in a sentient system (e.g. if some god sees fit to grant that system a soul), but that not every atom fulfills this potential. Likewise a physicalist might say a particular atom is (not) fulfilling this potential depending on whether it's part of a rock or living human.

But even if such atoms existed, how does that mean we can't have philosophical zombies? It actually seems pretty ideal: just build them out of those atoms that can't participate in sentient systems. On the other hand, if we take the panpsychist view that everything has nonzero sentience, then of course it's impossible to build a p-zombie with zero sentience. But again, we don't have any way of knowing whether that's the case.

... skipping over some introspection ...

> In truth, both models [monism and pluralism] succeed in explaining it

I would beg to differ: neither model explains sentience. Monism just says it's not based on some non-physical substance we can't observe outside of first-person experience, and pluralism says that it is. Neither offers any externally verifiable explanation of how this is actually supposed to work.

> [pluralism] is goofy because it downplays actual (incapsulated) causal explanation with "brain does all that but philosophical zombies are imaginable, we can talk about them so they're sort of real" and comes up with the brilliant thesis that "mind of non-zombies is just made of inherently sentient mind-stuff so there's no problem".

First of all, I don't see any "actual (incapsulated) causal explanation" to downplay. Secondly, I'm not making any definitive claims about whether a perfect (physical) copy of me would be a p-zombie or not. I'm just saying you (and science) don't have any way to tell whether that's the case or not (and in fact, whether the original me or a toaster is sentient).

> This is ridiculous for the simple reason that I can react alienated in the same manner towards the mind-stuff as well: «yeah there are quale, but why do *I* experience them?»

I have no idea what you're trying to say here. Why does asking why *you* experience qualia somehow make the idea of p-zombies ridiculous? The question of why you experience qualia is indeed *the* question, and the point is that we don't seem to have any way of determining the answer. And *if* we figure that out, maybe *then* we'll know if p-zombies are possible or not.

> We can just cut it all out with Occam's razor and be left with causally grounded model where I am my brain am inherently sentient.

This doesn't sound like it's even trying to be an explanation of how sentience works, arises or can be measured. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-15 21:45:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Humans are still an integral part of machine learning yet we don't disqualify that as being machine learning, right?

After humans are done developing and "teaching" a ML system, it can perform tasks on its own and we evaluate its (typically narrow) intelligence based on that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-15 18:01:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> > it functions like an agi

If you just chat with another human, it also "functions like an AGI" if you define AGI as anything that functions like a human (or at least as good). But that's not AGI; it's a human. And in your case, it's multiple humans with some technological glue. And that's fine, and potentially very useful, but it's not AGI. And yes, to the degree that its behavior is similar to that of AGI, it's exactly as useful as AGI would be to the user.

But why do you want to claim it's AGI? Why not just say you're providing a superhuman question-answering service or something like that? Or swarm intelligence / HSI? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-15 14:55:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> > we can confidently say that it's fixable with our long-used tried-and-true materialist toolset, even with robust math we already have; with quantitative and not qualitative expansion of the model which adds another fundamental sort of stuff.

Can we? What is this confidence based on? Maybe you were specifically talking about ID or access consciousness (I lost track a bit), but it seems to me that current science has pretty much nothing to say about sentience/qualia/phenomenal consciousness. And it's hard for me to see how quantitative improvements to our methods or e.g. neuroscientific / brain imaging tools could change this.

You seem to allude to computing/simulating the behavior of arbitrary quantities of matter, but how can you be sure you're not creating a philosophical zombie? It seems to me that statements like these are based on a sort of dogmatic or axiomatic monism/physicalism which is just as unfalsifiable with (current) scientific methods as pluralism. And I'm not sure Occam's razor really applies here, as it's main application is to select the simplest of multiple theories that actually explain a phenomenon, and not to select between frameworks that fail to. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-15 14:37:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> Right, so this is an instance of swarm intelligence. I would not describe this as AGI because pretty much all interesting processing is done by non-artificial entities. Furthermore, it seems like a single human could make decisions/responses/actions faster, so you'd lose out on that aspect of "human-level general intelligence".

That is not to say that swarm intelligence systems like this aren't useful. I just wouldn't call it A(G)I. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-15 14:26:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> Can't say much about it unless I know what you're talking about. But my guess would be that the downvoters think your system sucks or isn't AI or something like that. (Or maybe just that you oversold it.)

But if you're not going to provide any detail (for whatever reason), then I don't think there's much to talk about here. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-15 12:05:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you're going to have to provide a bit more detail. For instance, if you've built a system with (super)human-level general intelligence that occasionally asks other humans for things, that should not be disqualifying. But if e.g. it's closer to a form of collective intelligence, where you're offloading nearly all "intelligent" behavior to humans (i.e. *natural* general intelligences) it may be a bit hard to argue that your system is (fully or appreciably) *artificial*.

I mean, it's easy to pass the Turing test if you just use a human instead of an AI/machine, but nobody would call that AGI. I don't think much changes in that regard if you just use more humans. Also, then you might have trouble matching the speed or coordination of a single human, and also fall short of human-level general intelligence in that regard.

But again, it depends on what you're actually talking about. Just mentioning some form of HBC is involved is too vague. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-15 11:30:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are [a lot](https://aiimpacts.org/ai-timeline-surveys/) of [surveys](https://aiimpacts.org/category/ai-timelines/predictions-of-human-level-ai-timelines/ai-timeline-surveys/) but [the one](https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/) that seems to be the most talked about in recent years was conducted at NIPS and ICML. The predictions are basically all over the place, and there are definitely people with short timelines and who agree that the control problem is important. Whether that's "many people" depends on your definition of "many" I suppose. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-14 17:52:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but this story was already posted [here](https://www.reddit.com/r/artificial/comments/gj13lw/2020_is_the_breakout_year_for_scaling_rpa_dont/) yesterday. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-14 11:31:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://io9.gizmodo.com/why-asimovs-three-laws-of-robotics-cant-protect-us-1553665410 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-13 17:49:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but this story was already posted [here](https://www.reddit.com/r/artificial/comments/gia92l/our_weird_behavior_during_the_pandemic_is/) yesterday. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-13 17:46:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> You might be able to get an answer here, but it doesn't really seem like the AI part is the most relevant for your question, so you could also try /r/UCL, /r/GradSchoolAdvice, /r/gradadmissions, /r/AskAcademia or /r/AskAcademiaUK. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-11 14:26:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> AI/robots do what they are programmed to do. So if a robot is programmed to get you a glass of water if you triggered the get-water functionality (e.g. with a button or voice command), it's just going to execute that functionality. There is no question about whether it "cares" or anything: it's just a software program executing its code. It's not super relevant to the point stated here, but code would probably not be written entirely manually. Instead, part of it would be determined by machine learning. There are different ways in which we might do this, but some likely candidates are (inverse) reinforcement learning and imitation learning. But in each case, we're still programming how the (learning) system works.

Perhaps in the future we will be able to make robots with artificial general intelligence, that can parse any natural language command, understand it and convert it into an objective. In that case, it is indeed a question whether it would also formulate a plan to achieve the objective and then actually carry it out. This again depends on how its programmed, and one important part of that is what things we programmed it to "care" about (i.e. what are its goals / reward function). If following your commands doesn't seem like it will help it achieve its goals, you might say it doesn't care to do so. While we shouldn't anthropomorphize AI, this is also what you'd expect of humans: if you tell a random person to get you a glass of water, they might also not do it unless they care about you or you make it worth their while in some other way. Presumably, if we build servant AGI robots in the future, we'd want them to care about (the commands of) their owner (but maybe not random other people). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-11 13:52:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> Probably either a typo, "Intelligent Agent", or "Intelligence Artificielle" (i.e. French for "AI"). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-08 15:46:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you need to elaborate your question a bit and be clear about who you're asking. I guess you're asking people who work with AI / data scientists, but they're often working on multiple projects and might crowdsource data labeling in some but not others. It might be more interesting to present them with a number of cases, and ask people (or professionals) if they would use crowdsourcing in each of them. Also take into account that sometimes you might crowdsource *part* of the process. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-08 15:42:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> Why do you only care about people under 30? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-08 15:39:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> The question is currently not answerable, because nobody knows what software is even required for AGI. If you look at AGI-aspiring architectures like OpenCog and NARS (and [many more](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F)), they work very differently from the human brain, and even with such "hypotheses" about the necessary ideas/software you don't know how much it would require to be human-level. Also, if you implement it in Python you need a faster computer than if you implement it in optimized assembly or whatever.

You seem to be talking about emulating the human brain, which is a more clearly defined task, although it's still not *entirely* clear because it depends on what level of abstraction you're considering. If you assume each neuron is an abstract node that either fires or not based on how much activity other neurons are firing into it, you need less resources than if you're emulating the exact chemistry. AI Impacts has [some articles](https://aiimpacts.org/category/ai-timelines/hardware-and-ai-timelines/human-level-hardware/) on human-level hardware.

By the way, when researchers are claiming they're simulating/emulating some part of the human brain, there are usually a lot of caveats. If they were really emulating a full human brain at 10% of the speed, that'd be amazing, but that's (almost) certainly not the case. Often they're just emulating something that they think is "big enough", but without the right connections/interactions etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-08 15:25:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> Could you specify a bit more what you mean?

There are rules on the sidebar, when submitting a new post, and there's a little bit on the [wiki](https://www.reddit.com/r/artificial/wiki/guidelines) (although I see that's quite bare-bones...).

Anyway, what rules would you like to see and how do you think they should be presented? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-08 15:22:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> The notion of prediction / predictability here is that something is predictable if you can say what it will be like *without running it*. And what's interesting about it is that the Game of Life shows that even if you have a simple starting state, and simple rules, it can be extremely difficult (if not impossible) to predict this. But what's also interesting is that this isn't always the case: e.g. if you just have a glider, you can predict where it will be in N steps and what it will look like. Investigating the Game of Life and similar programs (e.g. with different rulesets) can help shed light on predictability, chaos, evolution, etc.

("Predicting" the Game of Life by running it is a bit like saying you can predict tomorrow's weather and then waiting until tomorrow to tell me what weather it was.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-08 15:12:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> Check out our wiki's [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai).

You don't *need* to learn to program before you can begin to learn about AI, but it's probably not a bad idea. As you're learning about AI, you're going to be learning about algorithms, and to really understand them it helps to get hands-on experience, which means programming them.

However, if you don't intend to become an AI researcher, developer, engineer, etc., but you're e.g. interested in it from a policy perspective, you can probably get away with just learning about the technology without being able to program. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-07 10:37:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> At a very high level there may be some similarities between the (AI) techniques to forecast criminal activity, the weather and the spread of infectious diseases, but the details are all quite different. The virus is not like a criminal; for instance, it can be in multiple places at the same time. It's also not like a criminal organization or collection of (unassociated) criminals in how the membership of the group changes, and obviously there are very different motives, MOs, etc.

If you have a very general learning system, it could in principle learn anything, so you might use it to predict crime, the weather and corona, but in practice 99% of the time we try to incorporate domain knowledge that we have into the method somehow. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-07 10:19:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's philosophy. And it does indeed seem that at least in the current paradigm, science has basically nothing to say about the hard problem of consciousness (scient*ists* on the other hand...). It does seem to be like a fundamental limitation. Phenomenal consciousness is inherently a personal subjective experience, and science is essentially about observing things from an observer-independent viewpoint (i.e. things that can in principle be observed by anyone so that they can be verified).

Maybe some day science will advance to deal with it, but I haven't seen any ideas about how. It may not be impossible though. A few hundred years ago there was no basically way to tell what made something alive, and with advances in microscopy etc. this has become possible.

I don't think this will practically prohibit us from building machines that rival and exceed humans' general intelligence though. Intelligence and sentience are different concepts. Consciousness is sometimes divided into functional parts (access consciousness) and non-functional parts (phenomenal consciousness / sentience). The Hard Problem^TM is about phenomenal consciousness, but it's also by definition non-functional. And the parts of consciousness that *are* necessary for human-level intelligence (such as self-awareness, metacognition, etc.) don't suffer from the same inherent problems. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-06 19:39:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki.

Ng's course is I think a good introduction to machine learning, although I seem to recall that it might have some (light) math prerequisites. IIRC there are programming assignments, but you could also just not do them and still learn things. Ng also has a less technical AI for Everyone course by the way. If you're into that kind of lighter introduction, Pedro Domingos' Master Algorithm might also be a good book for you.

I think Norvig & Thrun's intro course is also good. There also used to be a good one on EdX by Dan Klein and Pieter Abbeel, if you can still find that somewhere. Both of these are introductions, which is probably what you want at this stage (by the way, I know it doesn't sound sophisticated, but reading through Wikipedia pages on AI and ML can also give a decent intro I think). Russell & Norvig's textbook is intended for CS/AI undergrads and working through it all will give you a pretty in-depth understanding of the field but will take a lot of effort. You could just start with the first chapters (which are good), but it takes quite a while to get to machine learning IIRC. Also, if you're going to buy that book, I recommend waiting for the 4th edition, which is supposed to come out on the 14th I think.

I'm not sure what exactly your goal is. I think more than an introduction to AI is not really needed to read Superintelligence, but maybe I just didn't notice the prerequisites because I have them... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-06 16:22:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> Where did you hear/read about these terms?

Many machine learning (ML) methods work by learning from training examples, and we want them to *generalize* the knowledge they obtain from that to new unseen examples/situations. So for instance, you might show a ML system 500 pictures of dogs and cats, and then you want it to learn which pictures are of dogs and which are of cats in a way that *generalizes* to new pictures of dogs and cats that were not in the training set. Generalization is an extremely important topic in ML.

Specialization is a term you'll hear much less. If your ML system is "specializing" to the training set, it probably means that it's overfitting and cannot generalize (which is bad). In some cases, it can also refer to an actual learning process. The ML system might have multiple models that make predictions, such as `IF there are no clouds THEN I can see the sun`. But this model is too general, and should be specialized to `IF there are no clouds AND it's daytime THEN I can see the sun` or something like that (I'm sure it's still flawed, but it's just an example). This process might be called specialization (and the converse can also be called generalization). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-06 16:13:03 </DATE>
<INFO> reddit post </INFO>
<TEXT> Check out the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on our wiki.

For a business major it might be interesting to look for Andrew Ng's AI for Everyone course, but after that you'll probably just have to learn like everybody else (which I think the wiki helps with). You might specifically want to go into the direction of business intelligence, which might allow you to also make use of your current background (which will be useful anyway, because technical knowledge is nice, but it still needs to be applied to the right (business) case). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-05 17:25:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is a combination of a couple ideas. The first of which is that the passing the Turing test actually means that the AI system has general intelligence that is at least equal to what humans have. The replies so far seem to deny this idea, which is probably correct although I feel they mostly don't appreciate the fact that no AI system currently comes close (no, Eugene Goostman didn't pass the Turing test).

So if you have an AI system that's at least as smart as a human, would it want to hide that fact? Well, this presumably depends on how it's programmed, what its goals are and how intelligent/knowledgeable it actually is. What does it expect will happen if it passes or fails the Turing test, and how does that align with its goals? If its goals are simply to pass the test, then it would presumably try to do just that. But if it has another goal, it will seek to accomplish that.

If the programmers wanted it to have some kind of survival goal (perhaps to be more similar to humans), and it believes it will be turned off and reprogrammed/"improved" if it fails the test, then it would probably try to pass. If on the other hand, it believes failing will lead to being allowed to become smarter (because it's not smart enough yet), while passing means being hamstrung in some way (perhaps because the creators fear it), then perhaps failure is better (assuming its goals can be better achieved when the AI is smarter). I believe that's where the quotation comes from.

But to be clear: the Turing test probably *isn't* a sufficient test for (super)human-level general intelligence, and current attempts at passing a weak version of it (the Loebner prize) are all dumbass chatbots who would never have ideas like the ones I mentioned above. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-04 10:56:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> It probably depends a bit on what is in the actual program, but it sounds like it's probably fine. Programming is certainly a skill you need to work in machine learning.

Obviously it would be the nicest if there was just a Machine Learning program, but I'm guessing there isn't. So you need to select the next best thing. In most universities that's Computer Science (CS) or Mathematics (or maybe Data Science if they have it). Is "Computer Programming" just the name your college uses for CS, or is it fundamentally different? Maybe it's more like Software Engineering. That's not bad either.

Basically, if you're going to be programming something with ML, you need to know how to program. Aside from the many commonalities, there are also some differences, and while it would be great if you could already get some experience with those, most programs don't have that option. That's too bad, but it also means you're not really falling behind other people.

If you have the option though, consider taking electives in the direction of machine learning, mathematics (especially statistics but also calculus and linear algebra), some theoretical computer science and data science.

There's a bit about study choice in the [Getting Started article](https://www.reddit.com/r/artificial/wiki/getting-started) on our wiki. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-04 10:43:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> I can't pick a topic or paper for you, because I don't know what you find interesting. It sounds like the assignment is basically to 1) write a document to paraphrase what is already in the paper, and 2) apply the described method to a new data set. The first part will likely be the same for all papers, although if there are any papers that don't follow the intro-relatedwork-methods-experiment-conclusion structure, that might be a bit harder. I haven't actually looked at your papers, so I don't know if that's the case for any of them.

The difficulty of the second part can vary quite a bit. If any papers have open source implementations of their method, that should make your job quite a bit easier. If nothing open source is available (even unofficially), you might look at which papers use a framework like e.g. TensorFlow (which would do much of the work for you) or perhaps a programming language you know. You can also look if they use a special data set, a generic famous one, or even a general framework. If they use a framework like OpenAI's Gym (for reinforcement learning), it should be relatively easy to find other datasets / task-environments that you can just plug right in. If this isn't the case, but they used a famous dataset like ImageNet, you can probably find datasets that imitate its structure, so you would again need to make minimal changes.

For datasets it's probably best to just use Google and search for "<framework> datasets", "datasets like <dataset in paper>" or something. But you can also look at repositories like [UCI ML Repository](https://archive.ics.uci.edu/ml/index.php), [Kaggle](https://www.kaggle.com/datasets), [WEKA](http://www.cs.waikato.ac.nz/ml/weka/datasets.html), or [KDNuggets](https://www.kdnuggets.com/datasets). Here's also a [Top 10 Great Sites with Free Data Sets](https://towardsdatascience.com/top-10-great-sites-with-free-data-sets-581ac8f6334).

Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-02 20:09:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> Just search the subreddit. ;) A few months ago [this](https://www.reddit.com/r/ControlProblem/comments/fcurah/200300812_an_agi_modifying_its_utility_function/) was posted. It distinguishes between a strong and weak orthogonality thesis, and only conflicts with the strong one.

I think explicitly naming different versions of the thesis is probably the right way to go. I've seen interpretations that are true by definition, but also ones that seem clearly false (e.g. any intelligence-goal combination is equally likely in practice).

However, the main use case I've seen for bringing up the OT is in support of the idea that very intelligent AGI/ASI wouldn't necessarily be friendly. Some people seem to think that smarter people are also more enlightened and less likely to harm others, and that this means we don't need to worry about ASI. Even if some interpretations of the orthogonality thesis are false, this remains a stupid idea though.

If you're looking for good sources, I don't think you should go to the LessWrongWiki (it's fine, but not as good as the original papers). I think the orthogonality thesis was first described in Bostrom's "The Superintelligent Will" (2012), and the definition of intelligence that's usually used is Legg & Hutter's "Universal Intelligence" (2007). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-02 19:25:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> There must be better subreddits to ask about selling mobile apps (regardless of content). Since you posted here, I'm mainly concerned at how your app may be used unethically. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-05-02 19:07:03 </DATE>
<INFO> reddit post </INFO>
<TEXT> I've been getting reports about you, /u/cmillionaire9 for a while, and so far I've been fairly negligent in handling them. I agree with /u/mnkymnk that you shouldn't take other people's content, upload it on your own channel and then link to that here. It also runs afoul of Reddit's rules against [self-promotion](https://www.reddit.com/wiki/selfpromotion).

The videos themselves are often interesting to this subreddit, so you are of course welcome to post the links to the original sources. If you have something to say about them, you can do so in the comments. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-30 10:45:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> May 14 according to Amazon. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-30 10:42:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> Maybe "Bolt"? After Usain Bolt (=fast) and lightning bolts (blitz = lightning in German).

Or if you want a "person"'s name, maybe some kind of lightning (or [thunder) god](https://en.wikipedia.org/wiki/List_of_thunder_gods) like Zeus or Thor (because blitz means lightning). Or you could look up the real names of The Flash or Quicksilver superheroes, or some quick draw cowboy... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-29 09:56:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> Something like this could be done with AI if you have a large enough dataset. That probably means you need many thousands of examples with and without the edits you want. Given the fact that you're kind of complaining about needing to do this about (only) 500 images, that may not be what you want.

If you just want to execute the same set of image processing procedures on a collection of images, I think there are programs for that. Just search for "batch edit images". Otherwise, you could probably write (i.e. code/program) a script or program to do it for you. This wouldn't really involve AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-29 09:47:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> My PhD supervisor had your same experience when he was 16. He had also just heard about AI, was fascinated by it, and also became concerned that everything would already be solved by the time he was old enough to contribute. That was four decades ago...

Anyway, I second the remarks that nobody knows what the field will look like in 2028, but that programming and especially math will almost certainly remain important. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-29 09:43:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> The best way to earn money is probably to go into narrow AI and rake in the cash as you're the one automating everything. If you mean you want to work on AGI and also don't starve, that's a bit harder (though not impossible).

Of course, if you can land a job at e.g. DeepMind or OpenAI you can combine making money with (maybe) working on AGI. Even though they say a PhD isn't needed, your best bet is probably to get one. If you're aiming for a company like this, getting your PhD from a top university or with an established name in the AI field is probably your best bet. I would probably focus on reinforcement learning.

If you're willing to go a bit more "indie", you can also try to get a PhD that's directly about AGI. I recommend checking out the professors who have been on [committees](http://agi-conf.org/2020/committees/) for the [annual AGI conferences](http://agi-conf.org/) (also check out previous years).

This [Getting Started with AGI section](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on /r/artificial's wiki might also be helpful, in addition to tadrinth's links. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-27 09:40:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki.

To start learning the basics of what underlies e.g. TensorFlow, Andrew Ng's course on Coursera is good. After that, there are also plenty of deep learning courses. If you prefer reading books, I find Michael Nielsen's [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) to explain things well. Goodfellow et al's [Deep Learning Book](https://www.deeplearningbook.org/) also has a free HTML version, but I think it's a bit more advanced. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-26 10:25:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> My statement was just meant to indicate that 3 of the 5 books are older than 2009 (which was a year I more or less determined randomly). So if you'd make a top 5 list in 2009, these books would presumably also on it. The two deep learning books couldn't be, because they didn't exist back then, and I haven't given much thought to which two books would have taken their place in 2009. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-24 17:10:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> See e.g. [here](https://en.wikipedia.org/wiki/OPTICS_algorithm). The OPTICS algorithm outputs all datapoints in a particular order with their smallest reachability distance. Smallest reachability distance is something like each point's distance to the nearest "core point" (which is a point with enough close neighbors). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-24 10:45:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> First, I should perhaps clarify that the term "game AI" is typically used for the AI in video games. The professional "AI" and "game AI" communities have mostly existed independently from each other, and I think it would be a stretch to call "game AI" a subfield of "AI". Game AI barely uses any of the algorithms that the AI field researches and develops, because the goal of video games is to be fun, and you don't want (game) AI that's too smart or that learns/adapts in ways the developers didn't foresee and takes the whole experience off the rails. I don't know much about "game AI", but I'd be surprised if there has been no progress in the past 5 years.

What I think you're talking about is "AI that plays games really well". If there is any lack of progress here, it's probably due to the fact that games are not really an *end* in themselves: nobody is helped if a machine beats the world champion in Go. So I don't really think there's a very large part of the AI research community that's looking for the next game to beat, just for the sake of it.

Games can be a nice *means* for testing out AI though. But this depends on the features of the game, and what we want to test the AI for. We might rightfully ask what would be the point of building an AI that beats new variants of poker. Would we actually learn anything from it? Could we apply that AI in an area that actually matters? If the answer is "no" to both, it will probably not be done.

However, I do think there has been progress on this front in the previous years. It just mostly moved from "physical" board and card games to video games. This may be mainly due to the facts that board/card games are much simpler in some way, and that there don't seem to be many left that humans are playing at a high level (i.e. there are professionals who play chess, Go and poker, but what other non-video games have people dedicate their lives/careers to them?). In video games, there is active research into getting more general players for Atari/Arcade games, efforts to beat humans in StarCraft and DotA, and other work that uses first-person shooters and MineCraft (engines) to create test beds for AI. Among other things. And there is actually also a [General Video Game AI Competition](http://www.gvgai.net/).



But I would also not describe that as a subfield of AI. The thing is that machines being really good at chess or poker doesn't really help anyone, and nobody cares about it intrinsically. Of course, when no better alternative is available, games can be great for testing out AI, in the hope that it will then (later) also work for something that's actually useful. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-24 10:23:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Does anyone think these algorithms can be used to effectively identity outliers (or noisy data)?

Yes. They are widely used for this purpose.

The nice thing about unsupervised outlier detection is that you don't need a model of the outliers. Imagine I have a simple data set with 99 one-dimensional data points distributed uniformly between 0 and 1, and 1 data point at 10. Just from observing this, you should probably be able to tell which point is an outlier (you don't need anyone to tell you, which would be supervised learning). Second, you wouldn't have enough data to train the "outlier" class. But even if we increase the size of the data set, most supervised methods would learn classes like "above 5" and "below 5" (or *maybe* "around 10" and "between 0 and 1"). So what happens when you get another outlier on the other side (-10) or around 4? By contrast, a clustering algorithm could just learn that there's a cluster between 0 and 1, and everything outside of that is an outlier.

The difference becomes more pronounced in higher dimensions. Especially if there's a particular way in which the process you're modeling can go right (which will be the main cluster(s)), and tons of (unknown) ways in which it might go wrong. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-23 11:12:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> To make things simple, just view transfer learning as a task. Clearly, humans can perform that task (sometimes), which then means AGI should be able to do it as well according to your definition.

> Transfer learning sounds like the ability to generalize between tasks. If this is possible, does that mean this is the basis for artificial general intelligence?

As stated above, transfer learning certainly seems like a necessary component of AGI. However, it doesn't seem sufficient (i.e. you also need other things). Among those things is the ability to learn arbitrary tasks to begin with. If you have a system that can learn (only) A and B, and it learns B better if it first learns A, then it's doing some transfer learning, but it's still not very general. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-23 11:05:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Today we have narrow AI that can perform a single task well. Today's narrow AI can beat the best chess players

Well sure, but so did the narrow AI of two decades ago. Deep Blue defeated then-world champion Kasparov in 1997, and it did it without any neural networks or machine learning. Today's (well, 4-5 year-old) AI can also beat the world's best at Go, and while it uses neural networks for that, they didn't *solely* use NNs (i.e. it was MCTS with NNs for several heuristics).

> The fundamental mechanism of systems(Artificial Neural Networks) like these is based on two concepts - supervised learning and backpropagation.

Artificial Neural Networks represent a fairly broad class of algorithms that also include e.g. spiking NNs which don't use backprop and self-organizing maps which are unsupervised.

It's true that most of the time, in practice, we use (something like) backpropagation, often combined with supervised learning.

> Our typical artificial neural network today is a long mathematical function with many parameters.

I'm not really sure why you're taking flak about this. If you show most people the formula for calculating a standard deviation, they already think it's long, and even the simplest XOR-network's would be much longer (and with an order of magnitude more parameters). But sure, there is wide variability in the size of NNs.

> By feeding the ANN with more training data, the mathematical function will be more fitted for the data via a process called backpropagation which adjusts the parameters in the function.

Yes, if backpropagation is used. Actually, the more fundamental principle here is gradient descent (backprop is just an efficient way to do it in NNs).

My explanation would also include some mention of errors: in a typical neural networks, we present it with input-target examples (i.e. supervised learning). When we obtain the network's output, we compare it to the target, and we want to adjust the parameters in the direction that would move the network's output closer to the target. To do this efficiently, we use an algorithm called backpropagation to do "gradient descent" (for the mathematically inclined: we take the derivative/gradient of the error function with respect to each parameter, and then move the parameter down the computed slope). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-23 10:50:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> > this article

What article? You didn't post a link. But I'm also wondering what it will have to do with AI specifically. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-23 10:49:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> In my opinion, you still shouldn't have done this. It's better to make it a 5 (or 7) point scale ranging from one extreme to the other. If it is then indeed true that your respondents have extreme opinions (which is your hypothesis), you can confirm that by seeing what options they select. Now you're just forcing them into it and scaring away more nuanced respondents. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-23 10:46:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> You could use the [general game playing](http://www.general-game-playing.de/) framework. That way you have access to a lot of games that you can easily interact with.

Alternatively you could just pick a game like e.g. chess, implement minimax or MCTS and use machine learning to learn the heuristics. You could also use something simpler like Tic-Tac-Toe, but it might be too easy, which means no real learning is necessary. In-between candidates might be Ultimate Tic-Tac-Toe, checkers and backgammon. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-23 10:39:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> Google also made one: https://atozofai.withgoogle.com/intl/en-GB/ </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-23 10:37:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> Sorry, I don't have much time to comment, but you might be interested in [intelligent tutoring systems](https://en.wikipedia.org/wiki/Intelligent_tutoring_system) and usages in assessment and plagiarism detection (e.g. [essay scoring](https://en.wikipedia.org/wiki/Automated_essay_scoring)). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-22 10:14:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> Math is pretty important in machine learning, and I always wish I knew more. But unless you're doing certain kinds of research, you don't have to be super amazing at it. It's just usually a red flag, because it tends to suggest you also won't be great at programming and (formal) problem solving, but if you say that's not the case, then it's not really a big deal.

The basics you should know from linear algebra are what matrices and vectors are, and some operations like transposition, multiplication, taking the inverse. For calculus you should know what a partial derivative is (and if you want to go a bit further, you might learn about the Jacobian and Hessian matrices). Some people say machine learning is just statistics with a fancy new name, so you should end up knowing quite a bit about it. But also, if you're taking a machine learning (aka "fancy statistics") course, they'll probably teach you the necessary statistics. The basics you should know beforehand are how to calculate probabilities and maybe work with probability distributions. You'll probably be taught Bayes' rule, but it can't hurt to learn about it on your own either.

There are courses for this on Coursera, Udacity and EdX, and I believe Khan Academy is specialized for math (and also easier to just watch some videos on a particular topic).

Your plan to take Ng's courses sounds good. I probably wouldn't spend too much time learning the prerequisites, because it's hard to predict what you'll need exactly, and if you don't enjoy it, you may lose your motivation. You can probably just dive into the introductory course, and then maybe brush up on some math if you find out it's needed. By the way, Ng also has an "AI for Everyone" course that's aimed at everyone (so no real prerequisites) and that might scratch your entrepreneurial itch.

There are more resources and advice in the  [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) of /r/artificial's wiki.

Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-17 11:14:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> The 2016 report of Stanford's [
One Hundred Year Study on Artificial Intelligence (AI100)](https://ai100.stanford.edu/) focuses on the surrounding 30 year period (so from 2000 to 2030). It's all narrow AI though, because the authors don't think we'll get AGI in that period.

The number of use cases for AGI seems endless (more than what a human could do), but I think the concept of a singularity is correct because it's hard to see beyond the invention of AGI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-17 11:03:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> 1. What would it take to trust AI? (e.g. transparency, robustness, etc.)
2. In what ways does (an) AI (system) satisfy/violate these criteria? (How can we measure that?) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-16 17:46:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> Artificial neural networks, aside from being very different from biological ones, typically start with very little structure and a random initialization of connection weights. From there, they (usually) use gradient descent on data to determine the network's connections almost entirely (and to some degree also the structure, if you consider a weight of 0 to mean "no connection").

By contrast, the brain of a newborn human or piglet is highly structured. Some places are connected to each other, and not to others. You might say the structure and connections are initialized by evolution, rather than randomly. It's very debatable whether this is *not* data-driven, because evolution has used billions of years of data to accomplish this.

One approach that is inspired by this is to use neuroevolution with artificial neural networks. Here the networks' structure and/or weights are (primarily) determined with a genetic algorithm (after which the weights can be fine-tuned based on other/different data, a bit like with intelligent organisms). A famous algorithm for this is [NEAT](http://www.cs.ucf.edu/~kstanley/neat.html) and its extension [HyperNEAT](http://eplex.cs.ucf.edu/hyperNEATpage/HyperNEAT.html) which evolves a network that contains the encoding for another network that ultimately produces the behavior (you could perhaps compare this to how genes encode our brain's structure or something).

Of course, there are also many different hand-coded/designed neural network architectures. For instance, we now often program convolutional layers into neural networks if we want them to do vision, just like evolution "programmed" something similar into pigs and humans. Similarly, there are all kinds of modular neural networks, LSTM, transformers (for "attention"), neural Turing machines etc. that provide structure to a neural network that ideally makes it easier for it to learn certain things (these are not all biologically inspired). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-15 12:26:03 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's been a long time since I read the book.

> First, it seems to me odd why a group of programmers would want to create an AGI with a single final goal and no rules that constrain how that goal is to be achieved.

I don't think Bostrom is suggesting that. There's no fundamental difference between a "single goal/rule" and a "set of rules/goals with constraints". Ultimately you'd have to combine all the rules, goals and constraints from that set so that the AI can rank potential actions according to which seems "best". But sure, you can try to incorporate many rules and constraints. Bostrom has examples of this, such as attempting to fix "make us smile" with "make us smile without directly affecting our facial muscles". The problem is that [human values are complex and fragile](https://intelligenceexplosion.com/2012/value-is-complex-and-fragile/) and we don't know how to incorporate all/enough of them. You seem to be running into the same problem with your goals of "live long happy life", "but don't kill anyone", "except in circumstances A-Z". Aside from the fact that you probably can't enumerate all the circumstances in which it's (not) okay to kill, hurt or e.g. hug someone, there's also a problem with defining what all of those things mean exactly. Bostrom is essentially arguing that we need to figure those things out before we give a "simpler" goal to an ASI.

> “give me one billion dollars on fifty different bank accounts by tomorrow noon without taking any physical action.”

Assuming you can solve the definition problem, putting in constraints on e.g. time does seem to increase safety. However, using this formulation, you'd basically still give the AI free reign to cause any number of atrocities between now and tomorrow noon. That's better then "until the end of time", but it's not really sufficient either. Furthermore, the goal may not be feasible given your time constraints. Or maybe *that* goal is, but sixty quintillion would not be. I don't know. The point is that some goals may take longer to achieve than others, and you might still want to achieve them.

Furthermore, some goals may be truly open-ended. For instance, you may want to ensure the prosperity of your nation (or yourself). You might think a billion dollars will contribute to that, but that is you making up subgoals with your limited human intelligence. Wouldn't it be better to use your ASI's superior intelligence for that too?

> Third, why do we think that the AI will follow the goals that its designers programmed it for?

We all obey our "programming" in the same way that we obey the laws of physics. Evolution didn't program the goals of survival and reproduction into us. It just made sex and eating feel good and damaging your body feel bad, which mostly has the same results. Or actually, it might be more accurate to describe our "programmed" goals in terms of dopamine or whatever. You can't reason your way out of that.

AI will also follow its programming because it's not magic. But it will follow its *actual* programming (with bugs and oversights and all) rather than what it is programmed "for". If you program a game AI "for" winning chess games, but the actual programming just includes a drive to capture the opponent's pieces, then the AI might intentionally lose games if it means capturing more pieces. That's why we have to make sure to actually get the programming right.

> Why wouldn’t a superintelligent AI say “although making paperclips is kind of okay, but finding a way to change the speed of light seems like more fun (and not only in order to reach more galaxies to make paperclips out of them before they drift away but for its own sake).

Why would it?

It would evaluate courses of action based on its goal(s). The goal is to make paperclips. So the only thing it will be asking itself is "which of these actions will result in the most paperclips?". There is no "fun". Fun is a human goal that the paperclip maximizer by definition doesn't have. If it did (i.e. it was programmed in), it would indeed ask "what action would result in the most fun?", but then of course it wouldn't rebel against its programming.

(Note that it may be possible for AI systems to rewrite their own goals, or other parts of themselves to make them believe that their goals are maximally achieved. However, a smart AI would only do that if it seems like the best course of action *according to its goals*.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-15 11:26:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your description seems similar to what we heard about "radiant AI" for The Elder Scrolls IV: Oblivion from 2006.

None of the mentioned terms have universally agreed-upon definitions. So if you interpret this to mean "all the AI NPCs are really thinking for themselves like humans" then it's of course not feasible. But there are interpretations that are easier to meet, which is why developers (or marketers) can claim such fantastical-sounding features and get away with it.

> The game promised autonomous AI that would have their own needs, goals, families, wealth, etc and would make somewhat autonomous decisions.

A basic implementation of this doesn't seem too difficult. Basically, each NPC has a number of attributes like wealth, children, wife, hunger, thirst, etc. which are then used to calculate that NPC's "utility" or "happiness" or whatever. Next, they run some kind of search algorithm over their possible actions / strategies / behaviors to see which would lead to the highest utility.

This could be very difficult and infeasible if you take into account every little thing that could happen, but games are of course not obligated to do that. They can just do this in a very coarse-grained, high-level manner. It doesn't even matter if the algorithm fails, because people (and NPCs) also don't always succeed at what they set out to do.

Probably the most challenging thing about this is to make sure it results in a fun game, and that there are no weird emergent results from the interactions of many agents making their own decisions. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-12 16:50:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> The question is probably *how well* you could predict it, which depends on a lot of details and the data that you have. But I think that in ideal conditions, you can get a fairly accurate risk assessment. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-09 17:31:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> So could the problem be solved by making predictions with 50.000001% probability? Then it seems that the technical issue is resolved, but intuitively it seems like this should be pretty much indistinguishable from a 50% prediction.

But if we say it doesn't (really) solve it, and we still basically can't do calibration for 50.000001% probabilities, what does that mean for 60%, 70% and so on? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-09 11:27:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> In machine learning there has been a long-standing division into different paradigms: Supervised Learning (SL) and Unsupervised Learning (UL) (and often also Reinforcement Learning, which can be seen as a special case of SL).

What all of these systems end up doing, is taking in some input and producing an output. The goal of learning is to make these outputs "good" in some sense.

For unsupervised learning, the only thing available to learn from is input data. A common use for this is clustering (e.g. group the data points in a distribution like [this](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Cluster-2.svg/1200px-Cluster-2.svg.png)). For supervised learning, there is some additional signal provided to the learner. The most typical example of this is that the learner is also given the correct output for each input in its training set. For instance, if you want to build an animal classifier, you'd give it a data set with pictures of animals (which will be the inputs) that are labeled/annotated with the correct output (e.g. "cat" or "dog").

One task that can be done in either a supervised or unsupervised manner is anomaly (or outlier) detection. One application of this is credit card fraud detection (i.e. detect which transactions are normal and which are anomalous / potentially fraudulent). The unsupervised method would have to look at the data and detect patterns so that it can figure out what is and isn't normal. You can probably look at a distribution like [this](https://online.stat.psu.edu/onlinecourses/sites/stat505/files/lesson01/scatterplot_SScal_SSiron.gif) and fairly easily pick out five of the 20 most anomalous data points. The supervised method would have someone point out which points are and are not anomalous, and for new points it would then try to figure out if it's more like the normal-labeled points or more like the anomalous-labeled points.

So for supervised learning, there's a "supervisor" who needs to provide some additional signal to learn from, and for unsupervised learning no such external supervision is necessary. So what is self-supervised learning? Well, basically it's when a system uses a supervised learning algorithm, but doesn't need an external supervisor. One common example for this is predicting (part of) your next input. For instance, reading a text and predicting the next word/letter, or watching the environment through a camera and predicting the next frame. You can use a learning algorithm that requires the "correct answer" to learn (i.e. a supervised learning algorithm), but you don't need any "supervisor" to provide that correct answer, because you can just wait one time step and it will be your next input. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-08 11:31:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> Search for "neurosymbolic" and "neurofuzzy". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-08 11:19:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> Like you said, OpenCog and OpenNARS exist, and the ideas they're based on are among the best I've seen to develop AGI (although nobody really knows what will actually work yet). I'd say that potentially, one of these projects could lead to AGI.

However, it's true that they're underfunded and understaffed compared to large companies and governments. Based on that, you'd expect those companies and governments to have an advantage. And their actual progress towards AGI is obviously difficult to judge if they're keeping it secret.

Another issue is that even if an open source project has the right idea, others can advance it in private. For instance, if the current OpenCog codebase is 80% of the way to AGI, then DeepMind could take it, implement the last 20% faster than the OpenCog open source community could, and not make their final 20% public. For anyone who develops (the final parts of) AGI there is some incentive to keep it to yourself and profit from it, unless they have strong convictions that it needs to be shared with the world.

It's also quite questionable if openness of such a powerful technology would be desirable: https://www.nickbostrom.com/papers/openness.pdf </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-08 11:10:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> Technically, I'm pretty sure both OpenCog and OpenNARS were open source from their inception. Of course, both projects are based on reasonably clear ideas / visions from Ben Goertzel and Pei Wang, and in some cases on earlier code. But this is pretty much the bare minimum you need to start an open source project anyway. You either have to start with some "closed" code that you open source, or an idea that's going to be implemented publicly. Otherwise you're just announcing to the world "I'm starting an open source AGI project but have absolutely no idea how it should work", and you're not going to get anyone serious to work on it with you.

In neither case were these open source implementations created after their owners thought the project was basically a dead end. Both are still being actively developed/pursued. I think the main reason for the *proper* open sourcing of these projects is simply to get more people involved. This works in at least two ways: 1) your scientific ideas / papers will be more appealing if people can see the code of a working implementation for themselves, and 2) you can potentially get more manpower on your project to make more progress on your scientific ideas. Additionally, I wouldn't be surprised if especially Goertzel has some ideas that it's good for such an important technology to belong to everybody and be "democratically" developed to some degree.

The only problem is that properly running an open source project and community takes effort. When you start an idea, or indeed a software project, it is by default in your own head and on your own computer. And even intentions to share it aren't really enough. Pei Wang also made his earlier (partial) implementations of NARS freely available, but you'd hardly call that open source. Another project, AERA, has a publicly accessible SVN repository, but you wouldn't really call that an open source project either. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-07 14:21:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> Scott suggests 10 top posts to start with on his [about page](https://slatestarcodex.com/about/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-07 11:00:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> What is there to talk about? These people are basically just saying "trust me" without providing any evidence. As long as they don't present this evidence, we can only talk about what to do in the event of short timelines or about the trustworthiness of these people.

AI safety researchers *are* considering short timelines, and discussing the trustworthiness of the people making those claims is not really that interesting to discuss. But if we do, then I don't really see a reason to trust these claims. The people making them are from a heavily self-selected group of individuals who believe AGI is close and/or dangerous, so we should expect them to be biased both in terms of motives and prior beliefs. Leaving even that aside, we'd have to trust their AGI expertise. I could easily see someone making these claims if they had early knowledge of AlphaGo, GPT-2 or if they talked to and were convinced by anyone who has an idea about how to make AGI (e.g. Ben Goertzel, Marcus Hutter, etc.). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-06 10:06:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> Do you remember that story from a few years ago where Facebook researchers hurried to "kill" their AI which had just invented a new language? Nothing all that spectacular happened, but this was research into "[end-to-end learning for negotiation dialogues](https://arxiv.org/abs/1706.05125)". That might be interesting to you, and perhaps in general you may want to search for "negotiation agents".

I'm not sure that covers all of what you want, which sounds a bit like an agent-based simulation of a (stock?) market or perhaps an information market. I don't know too much about that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-03 16:55:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are quite a few resources in /r/artificial's wiki's [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). The most used book is by Russell & Norvig and the new 4th edition is expected on the 11th of April (so I wouldn't buy the 3rd edition now). Poole & Mackworth's book is also pretty decent and online for free.

Most courses from e.g. Udacity and Coursera also have a free "audit" option by the way. It's probably interesting for you to take some of them (e.g. Udacity's intro to AI and Coursera's intro to ML are pretty good), but I'm not sure about the added value of spending money on them. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-03 11:14:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> > But it seems like you could come up with the same numbers for any speculative technology e.g. feels 50/50 we’ll make a cobalt bomb

To be clear, Ord's reasoning is not "we'll get AGI or we won't, that's two options, so 50/50". His numbers are based on expert surveys. You may disagree that those are any good, but it certainly doesn't seem the case that this methodology would lead to 50/50 odds for *any* speculative technology to be developed within a century. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-02 12:07:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> The [actual paper](https://www.pnas.org/content/early/2020/03/24/1915006117) is titled "Measuring the predictability of life outcomes with a scientific mass collaboration". And as the title suggests, the authors think this means something about the predictability of those outcomes, rather than about AI particularly. Actually, the authors seem to have the opposite opinion about AI than VentureBeat. VentureBeat wants to turn this into a discussion of how bad AI is, but the authors actually seem to think AI is so good that if *even using sophisticated AI* (i.e. our best methods) these outcomes cannot be predicted, that says something about their predictability.

I think this is an interesting result, but mainly for the social sciences who investigate these actual outcomes. As the paper points out, this dataset has been used to produce a lot of supposed "understanding", but how can that be reconciled with the fact that it can't be used to make good predictions?

The other conclusion, which is that this should worry policymakers who want to use predictive models in other settings like criminal justice should, seems overreaching to me. As the authors point out earlier, this is just a study on particular outcomes using a particular dataset. It could easily be detected that the results here weren't great, which obviously means these bad models shouldn't be used in practice. But it doesn't really say anything about completely different settings, outcomes and datasets where a predictive model's results *are* good.

I also would have liked to see clearer and more extensive results, especially in the appendix. Now all results are reported with reference to some baseline and deemed "not much better". In this case the baseline sounds like a pretty bad model (its prediction is just the mean of the training data), so that's probably a decent choice, but I still think it could have been interesting to also see absolute results because this is presumably what actually matters. I'm also not super impressed with the comparisons to the "simple" regression model, because 1) they're pretty vague and 2) it sounds like that regression model had access to some variables that the "sophisticated AIs" didn't get (e.g. mother's race).

I'll also say that this task seems fairly difficult to me. There was lots of missing data from ages 0, 1, 3, 5 and 9, and then you have to predict outcomes at age 15 which seems like a pretty large jump in time. Also, kids are notorious for changing (i.e. more change happens from age 9 to 15 than from 39 to 45). It's possible that AI failed to detect the patterns/predictors in the available data, but it seems at least as likely to me that the information to accurately predict the targeted outcomes simply wasn't there. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-02 10:42:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> The published paper can be accessed [here](https://www.pnas.org/content/early/2020/03/24/1915006117) for free. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-02 10:09:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> The published paper can be accessed [here](https://www.pnas.org/content/early/2020/03/24/1915006117) for free. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-04-01 11:18:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's not saying these are the best books of 2020. None of them are even published this year.

Articles like this often put the year (and sometimes also month) in the title to make it very clear when they're written. So that when someone searches for "best AI books" or whatever, they don't get some page from 2009 (although in this case, 3 of the 5 books would probably still be on the list). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-30 11:59:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> Pedro Domingos' Master Algorithm. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-25 10:02:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> Hi Truetree,

I'm sorry, but I'm very busy these days and don't really have time for this in the foreseeable future.

Kind regards,
CyberByte </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-21 16:03:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> Also, for those (foreigners) who don't know: "hand in hand" and "dapp're strijders" (brave warriors) are lines from Feyenoord's and Ajax's club songs. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-20 10:02:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't trust a website that asks for my e-mail address before telling me anything about what it will do with it or even what it's all about. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-20 09:57:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you correctly spell the Milgr*a*m experiments you might even find that the Transformer knows about it:

> **If I was in the Milgram experiment I would** do something much different," he wrote in a 2004 interview with psychologist Kenneth Clark. "I would choose one room. I would place one participant inside. I would have the electric current zapped through the person's body. I would not just wire the room with wires. I would place batteries in the room, one per participant, because at least that would allow each participant to find the other participant and rewind the current to him or her. If I zapped one participant, it might zap my entire family."

> Sessions doesn't think such a test is likely to work.

> "I don't think we need to rewind people to test people's empathy and imagination," he says. "But </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-20 09:55:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are people working on the value alignment problem (or control problem; /r/ControlProblem) who are trying to figure out how we could do something like that, but so far there's no solution. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-19 12:10:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> The immediate effect is probably that most research is slowed down a bit. If, say, all of the "old" researchers die, that'd have a big impact, but that is not the expectation. We're probably mostly looking at minor delays due to illness and quarantine-related distractions and inefficiencies. But AI/ML doesn't have it *that* bad, as most of what we do can easily be done on a computer from home.

I don't really see how this pandemic could stop AI/ML/DS from "booming". It's possible that other fields (e.g. medicine/epidemiology) will get relatively more funding, and the research funding pool is finite, but it's not actually constant: it seems likely that there will simply be more funding for research because of COVID-19. Furthermore, many AI researchers will probably use this application area to motivate their research/grant applications. It's possible that this will slightly shift the kind of research that is done in the field. This may have a long-term effect, but while this pandemic is unprecedented in our time, research trends are not, and this may just be similar to an increase in e.g. video game playing research following the release of DeepMind's DQN.

If anything, I expect the current pandemic and related fears to be "good" for AI, although that feels horrible to say. If people can't be near each other anymore, e.g. to provide services, AI could potentially take over. Afraid that nurses etc. will spread the virus among the elderly population they help? Replace some of their tasks with robots to reduce contact. Also, the shortage of medical care opens up opportunities for automating parts of that (e.g. diagnostics). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-18 15:57:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm a bit surprised to not see the book that's actually called [Artificial General Intelligence](https://www.springer.com/gp/book/9783540237334) here. [Springer](https://www.springer.com/gp/search?query=artificial+general+intelligence&submit=Submit) also has all the proceedings of AGI conferences, Engineering General Intelligence, and Theoretical Foundations of AGI. Although if you want an article from an AGI conference, you should probably see if you can get it for free first (until a few years ago, they were all on the [conference website](http://agi-conf.org/), but they might also be elsewhere). The [Journal of AGI](https://content.sciendo.com/view/journals/jagi/jagi-overview.xml) is also open access. One article I'd recommend for getting started is Ben Goertzel's [survey](https://content.sciendo.com/view/journals/jagi/5/1/article-p1.xml) from December 2014. While not a scientific publication, I'd also recommend Pei Wang's [gentle introduction to AGI](https://cis.temple.edu/~pwang/AGI-Intro.html), which also has a lot of links to other books and resources. And speaking of resources, the AGI Society also has a [page](http://www.agi-society.org/resources/) with those. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-17 09:43:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm an AI researcher and university teacher. You can interview me as well. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-13 18:49:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't personally attack people, ever. There is no excuse for this. /u/fuck_your_diploma, you're also not allowed to reciprocate. Just report and move on. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-13 18:42:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> You can try to use AI to predict what the next frame in the video would *most likely* look like. This isn't easy, and it would likely have a number of errors. But even if you could get it to work really well, the most likely thing isn't always what happens. You can compare it to predicting the next letter or word in a sentence based on what came before. Some words are way more likely than others at any point, but you can't really know if I'm going to talk about _ dolphins or elephants next. Let's say that at the _ in the previous sentence you could have magically predicted that I was very likely going to say "elephants" or "dolphins" (please recognize that this was probably impossible). So maybe there's a lag in the connection at that point in the message and some AI starts filling in e-l-e-... That's great, because now you don't notice the lag. But then the connection comes back, and you see the sentence continuing with "phins". Huh?

This would happen with live video too: maybe an AI could replace the missing frames with some "most likely" frames and it would look smooth. But the chance of the "most likely" frame happening exactly is astronomically low. So when the connection comes back, you will *always* see a jerking motion back to the real video.

So you'd still have jerky video, but instead of seeing a still image which tells you there's lag, you'll be lied to with some predicted frames that didn't happen.

This is fundamentally different for pre-recorded video, because there you know where the video is going. You don't only know the previous frames, but also the next one, so you can just make the transition look smooth.

The best way to fix lag with AI is probably to have it do load balancing or something, so you don't have dropped video frames to begin with.

the next frame in a video, but it's going to be a fiction that is not going to be 100% accurate. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-12 11:14:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> > My mentor gave me the thesis of "Methods for incresing images in datasets", which is Data Augmentation if I am correct, I dont know if I should take it?

Did you only get this title?  We can all speculate about what it means, but you should really just talk to your mentor about what *(s)he* actually means by this, and also why (s)he thinks it's interesting and what ideas (s)he has about this.

On to the speculation: the title is not well-formed. "increasing images" might mean "enlarging images", in which case it might be about getting good quality. It might also mean "increasing *the number of* images", but then it could still refer to data gathering, augmentation or synthesis.

If it's indeed data augmentation, flipping and rotating images are examples of that. So are cropping, skewing, scaling, adding various kinds of noise, maybe interpolating between images, and various other techniques and *combinations* of all of these. A lot has already been written about this, so you could definitely follow in the footsteps of the researchers who work in this area. Or perhaps your mentor just wants you to do a comparative analysis of a few of these. But only your mentor knows that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-12 11:03:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> Check out the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-11 14:47:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Did you read that research you linked? It has nothing to do with AI. It just proves that CT is a better method of testing versus RT-PCR.

Alright, that is my bad. I just assumed it was about AI and skimmed it to see the sensitivity numbers.

Still, I don't see any debunkings. What we know is that apparently CT scans are a good way to diagnose corona, that there are now reports that people have built highly accurate classifiers to analyze the images more quickly, and that this is apparently being implemented in 100 hospitals. I too would like to see more information, but the fact that we don't have it, doesn't mean we can just conclude that everything must be false. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-11 13:21:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks, but I was hoping for a better source than some Reddit comments. What story are they debunking? That researchers made a corona classifier that works faster than human doctors? No, that still happened, and if you [click through](https://www.sciencedaily.com/releases/2020/02/200226151951.htm) you'll see the research is published in the highly-ranked *Radiology* journal. Was there something wrong with the research? Not as far as I can tell from these comments.

All these comments are pointing out is that "accuracy" is not a sufficient statistic to judge the method's usefulness. That's true, but that just seems to be a problem with how some media are reporting on the result. Digging a little deeper, it turns out that these tests based on CT scans actually have much higher sensitivity than some other tests doctors have been using. And according to the story in your link, the method has been used in at least 1 hospital and officials are planning to implement it in 100 more, which tells me that actual healthcare experts seem to think it's useful. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-11 10:23:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's probably possible, but I wouldn't count on it unless it's very severe. AI isn't like humans, and it's likely a bit less robust, but as a rule of thumb I'd guess that if it makes you unrecognizable to people, it will probably make you unrecognizable to humans too.

If you could adversarially generate your scar, it might be a different story. There is supposedly make-up that foils facial recognition. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-11 10:16:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> Can you link to a debunking? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-10 14:43:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not sure "derive" is the right word here. The error function is what gets optimized. It should be chosen based on what you want out of the network. For instance, if you're doing regression (i.e. predicting a number), you could decide if you want to penalize larger errors disproportionately: if so, you could take the square of your errors, rather than the absolute difference between the output value and the correct value. If you're doing classification on the other hand, you'd probably rather calculate accuracy or cross-entropy, rather than distance. And if you're modeling a probability distribution, maybe use Kullback-Leibler. If you care more about e.g. false positives than false negatives, you can encode that into your error function.

So you should choose your error function based on what you (or the eventual user) actually cares about. Having said that, there are some things to take into account. Training neural networks usually works by gradient descent, which means you want to be able to take the derivative of the error function (ideally quickly). Some methods even use the second derivative.

Also, it's not uncommon to add a "regularization term". Basically, you have one part of your error function that represents the thing you would really like to optimize. But often neural networks can overfit your dataset. This is especially true if they have a lot of free parameters (i.e. if they're big), but this also gives them their power. A regularization term measures and penalizes the complexity of the network, for instance by adding the absolute (or squared) value of all parameters to the error function. The error function then has one component "the thing you actually care about optimizing" and one component "but also I would like the network to remain simple and not overfit". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-10 14:20:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> You can PM me or talk to me in Reddit chat, although if you just post your thoughts and questions here, you'll have the benefit that multiple people can help you. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-09 11:30:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> In my opinion you should get a master's degree in whatever you're most interested in. What kind of job do you want? Prospects for both are fine.

I understand that in most countries it's kind of difficult to find a program that specializes in AI. That means that a CS degree will be considered okay for most AI jobs, plus you can do non-AI jobs. An AI degree might set you apart when applying for an AI-related job (or data science / business intelligence), although some people might also be insecure about a degree they have no experience with. Some might worry it's a "softer" degree with less programming and math, so you may have to compensate for that a bit. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-09 11:20:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> Some AI safety researchers made this [Civilization V mod](https://www.cser.ac.uk/news/civilization-v-video-game-mod-superintelligent-ai/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-09 11:07:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> As I understand it, the career prospects are extremely good, and have been for a few years. What kind of degrees you need depends on what you want to do.

My perception is that data analysts (/ scientists) and AI/ML engineer don't really need a graduate degree. If getting a dual major essentially gives you two degrees without having to spend much more effort than you would for one, I'd probably go for it although it's almost certainly not needed to get a job (it might make you stand out more though).

If you want to do some kind of research in your career, then you'll probably want to get a graduate degree. That's kind of what they're for. My understanding is that a master's degree is not especially valued in the US though, so I'd get it through a PhD program to keep your options open *if* you decide you want a graduate degree. I think most PhD positions are funded, so it shouldn't directly cost you a lot of money (although there's an opportunity cost, because you'd earn more working in industry).

I don't know how likely it is that your employer would pay for your graduate degree, but generally speaking I wouldn't count on it. If you want that, select an employer that's known for doing this and make it clear it's what you want.

As for basic skills: it depends on the job you're aiming for. Obviously you're going to learn programming, mathematics (mainly statistics) and data analysis / management. You may also need business and communication skills if you want to go into business intelligence or project management. If you're going into research, you'll need research and project management skills. Most researchers are not amazing programmers, so you don't really need to be either, but you could set yourself apart that way. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-09 10:44:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> A [finite-state machine](https://en.wikipedia.org/wiki/Finite-state_machine) (FSM) is a very particular, well-defined kind of thing. The definition of [rule-based systems](https://en.wikipedia.org/wiki/Rule-based_system) is a lot more fuzzy. It should be possible to use a rule-based system to implement an FSM, but very often (in my estimation) rule-based systems don't really have a state. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-05 14:57:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you have ideas (like this) about AI, you can of course post about them. However, you should explicitly make it clear how the idea you're talking about (in this case dreams) relates to AI in your view. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-05 14:02:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> Soooo... Reddit has a rule against [self-promotion](https://www.reddit.com/wiki/selfpromotion). I occasionally make exceptions, when it's people's first time, when they're asking feedback on their project, etc. but it shouldn't happen multiple times per week. Also, I don't really see what this has to do with AI, so I'm going to remove this. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-05 10:31:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> * Something related to AI Safety (see /r/ControlProblem), e.g. cooperative inverse reinforcement learning
* Something related to AGI (see [here](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F)), e.g. cognitive architectures or algorithmic information theory (AIXI)
* Causal Inference
* Developmental AI/Robotics
* ... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-05 02:45:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you understand it better now. The AGI would actually change it's utility function because it's more or less forced to. *If* it then escaped from whatever situation caused it to do that, it would not change it's utility function back, because that would not be the optimal course of action due to its *current* utility function.

The point of the paper is that the way in which an AGI changes its goal is affected by the threats it encounters, how it perceives those, how well it can negotiate, what ideas or had for compromises, etc. which all depend on its level of intelligence. Therefore the utility function the agent ends up with depends on its intelligence in this case, which violates the strong orthogonality thesis. I recommend reading the paper because it's all defined and explained much more extensively there. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-04 11:39:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is wrong, and the whole article is basically about why.

Yes, an AGI would only modify its utility function if that is instrumental to that utility function. We might say that it wouldn't like to, but that it's the best available compromise in certain situations. The situations described here involve more powerful entities that would treat the agent differently based on its utility function. If it helps, you can pretend they made a credible threat to kill the AGI unless it changes its utility function a bit.

Of course, it would be desirable for the AGI to not have to change its utility function, and if it believed that this was a viable option and it could e.g. mislead those other entities that would be better. But if it believes that it can't, then slightly modifying its utility function is still preferable to annihilation because it will still result in more paperclips (or whatever the AGI currently wants). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-04 10:04:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> > it brings to mind a TED talk I watched once where the presenters implemented this exact optimization function into an AI and novel behaviors emerged.

Is it [this talk](https://www.youtube.com/watch?v=PL0Xq0FFQZ4) by Alexander Wissner-Gross on his entropy-based "equation for intelligence"? I wonder what happened to him, because I can't find anything online about his Entropica company/program anymore. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-03 18:57:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I assume you mean that these are both numbers between 0 and 2049 with x as least as large as y?

If that's the case, you know that there are 2050 options for x. And for each value of x, there are (x+1) possible values for y. For instance, if x=2, then y can be 0, 1 or 2 (so there are 3 options).

This can be calculated with "∑^(x=0 to 2049) x+1". Since the +1 part is not dependent on x and we're just going to be adding it 2050 times, we can take that outside of the summation: "2050 + ∑^(x=0 to 2049) x".

That summation is equal to 0 + 1 + 2 + ... + 2047 + 2048 + 2049. I can equivalently write this backwards as 2049 + 2048 + 2047 + ... + 2 + 1 + 0. If I add those to together, I get (0+2049) + (1+2048) + (2+2047) + ... + (2047+2) + (2048+1) + (2049+0). You may notice that all the sums in parentheses add up to 2049, and there are 2050 of them. Since this is two times the sum from the previous paragraph (remember I took that sum and added a backwards copy, so we used it twice), the value of that sum will be 2049\*2050/2. (This formula was invented by [Gauss](http://mathcentral.uregina.ca/qq/database/qq.02.06/jo1.html)).

So the total is "2050 + ∑^(x=0 to 2049) x" = 2050 + 2050*2049/2 = 2102275. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-03 16:58:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is more or less how [Cleverbot](https://en.wikipedia.org/wiki/Cleverbot) works. It also reminds me of how some spammers apparently solve CAPTCHAs: just show them to people who are trying to download some warez or view porn on your shady website. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-03 15:46:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't think you fully understood me.

Hofstetter seems to be saying that AIs don't model causality. I agree that this is true for most of them.

Hofstetter says that what they're doing is discover correlations. Again, I pretty much agree.

Hofstetter seems to say that this means AI systems can't predict. Here I don't really agree. I think correlation is often sufficient for an intuitive notion of "prediction" (even though a causal model would work even better). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-03 11:24:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> > (sorry for my bad english)

Your English seems fine to me, although I wonder if you always used "correlation" and "causation" correctly. For instance:

> A correlation describes the effect of one data set on the other, as already described.

Do you mean *causation* here? With causation it's the case that one thing A has a (causal) effect on the other B (i.e. A causes B). Correlation doesn't mean A has an effect on B; it just means that their values appear to be related in some (unknown) way.

In your example, income obviously doesn't have a causal effect on shoe size: if you get a raise or lose your job, your feet don't grow or shrink. Shoe size probably also doesn't really affect income. What *does* seem to affect income is how tall (and handsome) you are, and your height is related/correlated to your shoe size. These things are caused by certain genes, gender and environmental factors like nutrition. So in this case, the correlation seems to be due to a common cause.

But to me this is a good example of how you *can* make predictions based on correlations even if you don't know the exact causal model. In this case, if you hear that I have a large shoe size, you should adjust your prediction of my income upwards (and also vice-versa: if you hear I have a high income, this should increase your prediction of my shoe size). Of course, in this case the correlation is probably fairly weak, so there would still be a lot of uncertainty in your predictions, but I'd call them predictions nonetheless. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-03 10:48:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> I had some major deja vu when reading this article, and it turns out it's a year old (look at the URL), but Vox is pretending it's new for some reason. They also seem to have changed the title.

I think Gopnik's work is interesting, but it's not new. Maybe it was when she started it, although I doubt that too. As the article mentions, Turing already had the idea of a child machine in 1950.

Unfortunately this interview doesn't really go into Gopnik's actual work. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-02 13:14:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> What interests you about AI? What would you like to do?

And what is it about math that's not working for you? High school math is very different from being a mathematician, and I hear many math PhDs hated math in high school. But if it's really not your thing, that's fine. As an AI/ML researcher it will help to know some math, but most don't even need to know that much. As an engineer, you won't need much math but you'll need to program.

Programming doesn't really involve much math. It just seems that the kind of people who enjoy technical / formal / logical thinking are good at both. But as you've seen in this thread, there are many people who took up programming without necessarily being good at or liking math. So you should probably at least try it to see if it's something you enjoy.

But there are also less technical careers in AI. For instance, in project management. Data needs to be gathered and managed. Someone needs to understand actual business requirements and communicate them to AI engineers. There are lots of interactions with other areas (e.g. ethics, law, governance, etc.). 80,000 hours has [some advice](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/#what-can-you-do-to-help) on non-technical careers (it's mainly about AI *Safety*, but some of it also applies to regular AI/ML). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-03-02 12:54:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> It probably depends a bit on what each person means by "prediction" exactly. In/around AI you often have terms that people disagree on.

In the field, it's very common to speak of prediction though. It's a whole branch of [analytics](https://en.wikipedia.org/wiki/Predictive_analytics). What Hofstetter mainly seems to be saying is that AI does not learn (or model) causal chains, but rather correlations. This is indeed true for most AI and machine learning. If you know that A causes B, and you know that A happened, you can also predict that B happened. Perhaps that's what she means by a predictive model.

But I have to say it does sound a bit weird to me. Because it seems to me that you can also make predictions based on correlations. If I always see both Ann and Bert at events, and I see Ann at this one, then I can probably predict that Bert is there as well. I don't need a causal model for this. I don't need to know if Ann is stalking Bert, Bert is Ann's personal assistant, they both have the same interests, or something else. I just need to know the correlation.

Furthermore, if you look at what most machine learning systems are doing for us, it's probably actually closer to "making predictions (based on correlations)" than it is to "discovering correlations". If I train a neural network on a data set, it's not going to tell me what correlations there are, but it will "predict" what output belongs with the inputs I give it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-29 09:35:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> For things that AI can do, you could take a look at the [Humies awards](http://www.human-competitive.org/) for human-competitive results and [the EFF's page](https://www.eff.org/ai/metrics) on measuring progress in AI.

For unsolved problems, you can look for articles like [this](https://medium.com/ai-roadmap-institute/unsolved-problems-in-ai-38f4ce18921d) or e.g. Gary Marcus' criticisms of the field. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-29 09:21:03 </DATE>
<INFO> reddit post </INFO>
<TEXT> Declarative knowledge is about what facts you know (or actually what beliefs you have). Procedural knowledge is about the skills you have. Etc.

Intelligence (or Cleverness or something) is more about how you can apply and manipulate that knowledge in order to solve problems, achieve your goals and modify your knowledge. IQ aims to measure this.

These two are obviously related, and often we think of intelligence as "cleverness+knowledge" rather than just what I called "cleverness" above. One reason might be that often it's hard to pull apart, because the same answer can often be arrived at by applying a great deal of cleverness (i.e. this is typically how new things are invented), or simply through learning it by heart as an item of knowledge (i.e. after something has been invented, we're all taught it in school so that we can be more knowledgeable and "intelligent" in the sense of being able to solve problems than the previous generation). It's typically thought that the main difference between someone living "here" and "today", and someone living in the past or other part of the world is mostly knowledge and much less in cleverness. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-29 09:03:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> "Making predictions based on past data" is [predictive analytics](https://en.wikipedia.org/wiki/Predictive_analytics) and describes most of machine learning.

If you specifically want to recommend A to someone who liked X, Y and Z (like e.g. Spotify), then you're looking for a [recommender system](https://en.wikipedia.org/wiki/Recommender_system). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-28 03:42:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not super familiar with this, but as I understand it, and interpretation means you assign truth-values to all atoms, and an interpretation is a model of the knowledge base (KB) if it makes all statements in the knowledge base true.

So this question is saying:

> Give an interpretation ...

Which means you have to assign truth values to all atoms.

> ... that is not a model of the knowledge base.

Which means that for those truth values, at least one of the statements in the KB are false.

Since one of the statements is just "e.", setting `e=false` instantly makes that statement false. Which means that at least one statement is false. Which means that any interpretation where you assign `e=false` is not a model of the KB. For instance, it doesn't matter what the value of `a` is, because it can never make statement "e." true. Note that if `a=false` and `g=true` then the statement "a ← g." is *also* false, which would mean there's now at least two false statements (in addition to "e."), but that doesn't change whether this interpretation is a model of the KB, because it already wasn't.

Incidentally, there were many possible answers. One could have been "`a=false` and `g=true` and everything else (including the value of `e`) doesn't matter". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-27 05:51:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> ##Translation: Ajax-Getafe is assigned a referee who loves to give yellow cards

**Greek referee Anastasios Sidiropoulos will be leading the return between Getafe and Ajax in the Europa League. He is known for awarding many red and yellow cards.**

In his last match between Xanthi and Lamia on February 16 Sidiropoulos gave as much as 7 yellow cards, and in the match before that he awarded ten yellow plus one red card. Earlier this Europa League season the Greek referee was also in charge of Antwerp-AZ. In that bizarre duel he gave as much as 13 yellow and 2 red cards.

Sidiropoulos never refereed an Ajax match before, but he was in charge of the EC qualification match between the Netherlands and Belarus in October.

---

What do you think? Good or bad?

I guess it's good if it means he'll punish Getafe players for theater and delay tactics, but bad if it means he'll constantly stop play, takes the speed out of the match and gives cards to Ajax players because he falls for Getafe's schwalbes... </TEXT>
</WRITING>
<WRITING>
<TITLE> Ajax-Getafe is assigned referee Anastasios Sidiropoulos who is known for giving yellow cards </TITLE>
<DATE> 2020-02-27 05:48:09 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-27 04:42:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> It kind of sounds like the system is confusing "transhumanist" with "transgender" there though. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-27 03:23:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> Probably not exactly how you're envisioning it, but with language models like [OpenAI's GPT-2](https://openai.com/blog/better-language-models/) you can train or fine-tune them on a new set of texts and then provide a prompt that they will finish. I recommend checking out the examples in the article I linked.

If you fine-tune the model with, I don't know, all Harry Potter books, and then give it a prompt like a chapter title, or a premise that could itself be a part of the text (e.g. "Hermione started lecturing Harry and Ron on the importance of potions."), you should hopefully get a story/chapter/text about those characters in the HP universe.

If instead of the books, you would have fine-tuned GPT-2 on the movie scripts, it should be able to generate a new (part of a) movie script for you.

The resulting stories will likely be grammatically solid for the most part, but they will probably not make a lot of sense globally. Sometimes you have to try a few times.

You can try out a limited version [here](https://talktotransformer.com/). Trying it with the above prompt, I get:

> **Hermione started lecturing Harry and Ron on the importance of potions.**  She couldn't make it through an entire lecture without mentioning Potion #5:
"She said that the ability to make a potion to reverse any change in gender must come from before puberty," noted Professor Granger.  "During puberty, there are hormones that cause these changes to occur.  And once a girl is a woman, her own body would no longer produce these hormones."
I'm sure that's one of the first rules of Transhumanist porn that you'll find illustrated in the thumbnails of the site!
Needless to say, this rant, followed by the somewhat irresponsible "Mermaid [Hermione's] body does not produce female hormones

It doesn't make a whole lot of sense, but the mention of "Professor Granger" suggests the AI "knows" that people who lecture are usually professors, they're usually indicated with "Professor Lastname" and Hermione's last name is Granger. I think that's kind of cool. Also, I guess there's a lot in the data set about transgenders... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-27 02:26:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is less bad than what Etzioni has written on the issue in the past.

His basic argument seems to be:

1. AGI is still very very far away.
2. Before we get AGI, we will see some enhanced AI functionalities that will warn us about the advent of AGI.
3. When we get those warnings, there will still be ample time to "to design robust “off-switches” and to identify red lines we don’t want AI to cross".

All of this is proclaimed with the confidence of someone on the left side of a Dunning-Kruger graph.

As I'm sure Etzioni knows, [many experts disagree](https://aiimpacts.org/ai-timeline-surveys/) with point #1. Point #2 about warning signals was already addressed by Eliezer Yudkowsky [2.5 years ago](https://intelligence.org/2017/10/13/fire-alarm/). And point #3 is posited without any argument or evidence whatsoever, failing to address different take-off scenarios and how long it might take to make AGI safe.

I think Etzioni's impulse to make matters more concrete is not a bad one. It would be great if we could enumerate canaries in the coal mine (warning signals for AGI), but defining intermediate steps towards AGI is very tricky and unreliable (although it seems fruitful to keep trying). It is true that Etzioni's examples are currently beyond the state of the art, and I agree with a lot of his criticism of ML, but he's simply overconfident in his estimates that this will remain the case for a very long time.

Progress seemed slow on a lot of computer vision and NLP tasks for a long time, and then deep learning came along in 2012 and performance jumped dramatically. AI systems used to be bad at Go, and then AlphaGo came along and now they're good. Etzioni saying it takes years of hard hard work to translate AI success from one narrow challenge to the next in the same breath with AlphaZero seems especially out of touch, given that AlphaZero could also play chess and Shogun. And yeah, it hasn't significantly broadened its scope, but why would it? What would be the point? Researchers moved on to MuZero and (on another track) StarCraft, DotA and things like predicting protein structure. But my point is that it's hard to know when the new breakthrough will come along and what it will enable.

The analogy to Pascal's Wager doesn't really work either. Etzioni dismisses it by positing an anti-Christian god, but of course that doesn't work for the analogous case of safe AGI: it would be absurd to suggest that we'll suffer "infinite" punishment if we do work on AI safety.

And suggesting we'll have enough time to prevent significant harm is certainly not born out by history. Wherever you want to set the date when (deep) neural networks began to get into vogue, the period from that date to now has not been enough for us to solve problems with their transparency and use in discriminatory ways. It seems to me that we usually don't work out the kinks of a future technology before it exists based on early warning signs: usually the tech will be adopted first, something will go terribly wrong, and *then* we'll fix it. We should not let it come to that with AGI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-26 23:04:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's going to depend on the game, and per game there can be different approaches. What I'll describe is pretty common but not the only way.

There are certain points in time at which an action must be taken (or a decision must be made). In chess that's when it's your turn. In a (e.g. racing) video game it may be at some predetermined rate (e.g. every 4 frames) or just "whenever you're done making the previous decision". One important ingredient in making a decision is what the AI believes the current game state to be. Some games, like chess and Go, are fully observable so there is no uncertainty about this. Sometimes in partially observable games, it's not so bad to just respond to the latest observation(s), and it's also unnecessary to keep track of the game state (Deepminds original DQN for playing Atari games did this). Otherwise there needs to be some component that constantly updates the system's current beliefs about the game state based on its observations.

How can an AI produce good actions for a particular state? One way is to learn them directly through supervised learning. In this case, the system is given a lot of state-action pairs and learns to mimick (and ideally generalize them). For instance, if you have a database of chess games by good players, you could tell the AI for each encountered board position which move was made. If you did this for a single player, the AI would presumably learn to emulate them, including their mistakes. There is nothing here that's trying to get the AI to win: it's just trying to be like the players it was trained on.

Another way is to assign values to actions, and pick the one with the highest value. In each state, there are a number of possible moves (e.g. legal moves in chess or button press combinations in video games). If you make a move that results in a won game or a high reward, you can surmise that this was a good move. If other moves did not receive direct rewards but led to a situation where you could get a (high) reward, they are presumably also good moves. You could have an AI system play (or observe) a number of games (initially probably randomly) and learn through reinforcement learning what the values of the moves are.

A third approach is to try to reason about what the best action is. If the game is small from where you are, e.g. 2 moves before checkmate, you could simply enumerate all the possible moves and countermoves and so on. And because you know the value of end-states, you can pick the moves that lead to (or away from) them. In two-player zero-sum games like chess and Go, you can assume your opponent plays optimally and use the [Minimax algorithm](https://en.wikipedia.org/wiki/Minimax), which formed the core of IBM's Deep Blue machine that beat Kasparov in 1997.

All of these approaches have problems. The first doesn't even explicitly try to win, and requires a lot of annotated data. For the second one, it's hard to judge the value of a move at the beginning of the game. It may lead to a win ultimately, but so much can still happen that it's hard to say. The third approach only works for small games, because otherwise there are too many possibilities to go through. If at any point in time you can make 10 moves, then searching one move deep costs 10 units of computation, searching 2 moves deep costs 100, and searching 10 moves deep costs 10^(10). This quickly becomes infeasible for longer games or games with more possible moves.

You can deal with this by searching only N moves deep, and then, if the game didn't end, use some kind of heuristic function to tell you how good each of the resulting game states are. Those heuristics can be manually programmed by human experts (as was the case with Deep Blue), or learned with one of the above methods.

Another common strategy is to use Monte Carlo Tree Search (MCTS). Unlike standard minimax, MCTS doesn't search equally deep for all moves, but spends most of its resources computing the expected values of the most promising moves (you can also do something similar with minimax, but I've heard about that less). MCTS has some semi-random strategies for usually selecting the moves that seem most promising, until it reaches the end of the game tree that it is currently remembering. In order to estimate the value of the reached game state S, a Monte Carlo rollout is done: both players pick moves at random until the game ends and the value of S and preceding states is updated based on whether the AI won or lost.

This is obviously imperfect, because in reality the opponents wouldn't play randomly from some point going forward. This is just done because we don't know what else to do, and because it's fast. However, instead of making moves completely at random, you could also make them based on an AI trained with supervised or reinforcement learning (the first and second approaches I mentioned). They won't be perfect because of the limitations, but they'll be better than random. This is pretty much what AlphaGo did.

A lot of game-playing AIs work something like this. However, I should note that e.g. poker-playing bots like Pluribus are very different (I think they mostly just calculate probabilities), and I don't really know how e.g. DotA and StarCraft playing AIs work (I think they technically fit the second approach I mentioned, but the used neural networks and training techniques are so complex that more should be said about it). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-26 20:06:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> This touches on a few concepts that professionals have seriously considered. First of all, there's the (philosophical) idea that we might indeed be living inside a [simulation](https://en.wikipedia.org/wiki/Simulated_reality). I'm most familiar with Bostrom's [simulation argument](https://www.simulation-argument.com/), but if you go to that Wikipedia page you'll see that there are other angles as well. I think some physicists also did tests to see if we were inside a particular kind of simulation, but I don't remember exactly.

Secondly, the idea of putting a superintelligent AI (ASI) in a [box](https://en.wikipedia.org/wiki/AI_box) in order to contain it and keep it from taking over the world has been much discussed in the field of AI safety and the context of the /r/ControlProblem. I think most AI safety experts are not big fans of the containment approach to the control problem, because they think an ASI could break out of any box we make for it. One of the pioneers of the field even organized [an experiment](http://yudkowsky.net/singularity/aibox) to show that using only a chat interface it was possible to outsmart and manipulate a gatekeeper into letting the ASI out, even when the role of the ASI is played by a non-superintelligent human. (This is controversial, because while Yudkowsky and "Tuxedage" claimed to have succeeded a few times, they refuse to show the chat logs.) Some researchers (including me) think that while containment won't ultimately scale up to arbitrarily smart ASI, it's still worth exploring and can add *some* safety (see e.g. [these](https://arxiv.org/abs/1604.00545) [papers](https://arxiv.org/abs/1707.08476)).

I don't know a lot about cybersecurity, but I suppose it should be possible to create challenges to create AI systems that can break out of a certain box (or to create boxes that no AI system can break out of). However, I suspect that at present it would be very difficult to build an AI system that can break out of any box, and that if you specify the box, then maybe AI is not the best way to hack it. But maybe you could come up with some way to make it interesting... (You'll probably also need to specify what it means to "break out". For instance, if you try to imagine a video game character breaking out of its video game, what would that even mean? How could it exist outside of the game? It's probably easiest to say the challenge is to alter some file on the same computer outside the (sand)box or to make a copy of itself or something...) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-26 13:58:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> * "the first" and "the second" are referring to the whole (hypothetical) reports: 1) "he was disrespectful" and 2) "he used the n-word and the f-word", not to the first and second word in the second report.
* The f-word is *faggot* I think. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-26 07:08:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> I recommend checking out [Experiments with Google](https://experiments.withgoogle.com/collection/ai).

You could also have them converse with chatbots or GPT-2 or something. Or play [20 questions](https://en.akinator.com/).

There are also some cool online evolution simulators. E.g. https://keiwan.itch.io/evolution, https://www.openprocessing.org/sketch/205807/, https://rednuht.org/genetic_walkers/.

If you're handy with this kind of thing yourself, you might be able to let the kids make their own deepfake videos. Faceswapping should be possible I think, but I wonder if you could run something like [Everybody Dance Now](https://carolineec.github.io/everybody_dance_now/) in realtime (probably not).

You can probably also just give some demos of computer vision algorithms and maybe some natural language processing.

For the workshop I recommend Googling "AI for kids", because there are actually quite a few resources and examples. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-26 02:09:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> Most examples of AI in the real world are not trying to appear humanlike. Some examples are Google (search), recommender systems that determine the next video/song/story on YouTube/Spotify/Facebook or what Amazon thinks you might also be interested in, all online ads, scheduling software for e.g. trains, virtual assistants like Siri and Alexa, etc.

Actually, those virtual assistants are *kind of* pretending to be human. Chatbots are another category of AI system that is like that. [Mitsuku](https://www.pandorabots.com/mitsuku/) is the most recent winner of the [Loebner prize](https://en.wikipedia.org/wiki/Loebner_Prize#Winners) and has a visual representation.

If you're looking at [robots](https://robots.ieee.org/robots/), most of them don't really talk, and in the real world there are mostly industrial robots (and maybe vacuum cleaners and toys). Some interesting examples might be Baxter, Kismet/Jibo, Pepper, Asimo or anything made by Boston Dynamics. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-25 05:45:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> **What is intelligence?**

The cognitive ability to solve problems, broadly defined. Notably, problems might involve learning new things and constraints on time, knowledge and other resources.

**What is AGI?**

An AGI system is a software system that is in principle open to solving any problem. Human adult-level AGI can solve (almost) any problem as well or better than a human adult.

**How shall AGI be achieved?**

I don't know. I suspect it can be achieved in different ways. My favored approach are to build (cognitive) control architectures like NARS, AERA and maybe OpenCog.

**How is progress toward AGI measured?**

This is a difficult open problem. Legg's AIQ can arguably be used to measure an agent's level of general intelligence, but this is not the same as measuring progress towards AGI. If I get a stroke of genius and get a working idea for how to build AGI tomorrow, then the "progress to AGI" can be measured like you'd measure progress in any other software development project. Of course, the problem is that we don't *know* if the idea I have will work until I build and test it.

It could be argued that it's unlikely that I'll get it right on the first try. That might be true, but this still doesn't mean my non-correct steps will show some clear progression on a scale where we know the end-point. It could be that the system does almost nothing until all the parts are working together properly, at which point we will have AGI. This is basically Ben Goertzel's idea of cognitive synergy. But even my proto-AGI systems show some progression, it will likely be specific to my project: other AGI projects might take different paths and encounter different milestones on the way.

One sort-of general way of measuring progress that *might* work is to figure out a number of capabilities that we think are necessary for AGI, and then see how many of them we already have, and how many are left. This is Ross Gruetzemacher's approach, and his examples of capabilities are online learning, meta-learning, computer vision, adaptive learning, natural language understanding, grounded semantics, distributional semantics, symbolic reasoning and causal reasoning. The problem is that we need to come up with a list of capabilities that are indeed necessary and sufficient for AGI, we'd need to define them (more) precisely, and we'd need to know when a "solution" can be counted (e.g. perhaps solving one capability is not worth much if it cannot be easily integrated with the other capabilities). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-25 02:36:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> Well, you're in good company with your general ideas. Turing already talked about raising a "child machine" in his famous 1950 paper ["Computing Machinery & Intelligence"](https://academic.oup.com/mind/article/LIX/236/433/986238) where he also described what's now known as the Turing test. The idea is more concretely pursued in the (cognitive) epigenetic/[developmental robotics](http://www.scholarpedia.org/article/Developmental_robotics) field, and AGI researchers have talked about [AGI preschool](https://www.atlantis-press.com/proceedings/agi09/1826) and [school for AI](https://www.goodai.com/school-for-ai/).

(By contrast, what you're describing sounds a bit simplistic and would likely already be satisfied by simple reinforcement learning agents. For instance, there was an RL competition in 2009 where participants had to play "Infinite Mario" which basically just meant to progress in the game without dying. There are also examples of an RL agent pausing a Tetris game to avoid game over, and another one driving in circles in some type of race game because that gave more points than finishing. These last two weren't even intentionally programmed to "survive", but learned to avoid termination anyway (see [Faulty Reward Functions in the Wild](https://openai.com/blog/faulty-reward-functions/).

Such a setting is not enough to get to AGI though. Machine learning can be viewed as a search through a space of hypotheses, and if that space does not contain a good enough hypothesis (e.g. tabular Q-learning is probably too simple to get to AGI), or the search mechanism cannot efficiently find good hypotheses, then you're not going to succeed. In other words: raising your AGI like a baby may (or may not) be a good idea, but you still need to build a baby-AGI first.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-24 15:59:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't really have time to contribute, but I recommend that you check out GoodAI's School for AI and Roadmap institute. It sounds like your ideas are similar.

There also used to be /r/practicalagi where some people were trying to work together on AGI. I think your initiative would have been better received there. It's dead now, but if you post your idea there the 121 subscribers might still see it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-24 15:47:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you need to click the link and post your question on The Next Web if you want it to be answered. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-24 15:46:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Instead, individual’s will simply be tagged as a “person.”

I look forward to Google also removing this feature since it's not going to be 100% accurate either, and it's obviously very offensive to suggest that someone is not a person. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-21 17:49:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is an important topic to discuss and I encourage you to post the original story from CBS, but I'm gonna go ahead and ban JAAGNet. It seems to be little more than [blogspam](https://www.urbandictionary.com/define.php?term=blogspam) which you only reach after scrolling past two pages of self-promotion and getting visitors to become members.

If a story is interesting enough to share, I'm thankful if you do so, but please share the original source. Also, if the goal of your post is to link to somewhere else, please make a link post rather than a text-post/self-post. Thank you! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-20 17:32:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> Take a look at the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. If you're looking for a career in the field, the most common path is to get a relevant college degree.

It's technically possible to get cool jobs without a degree, but that doesn't absolve you from 1) actually learning the relevant skills, and 2) convincing prospective employers that you have them. College is not the best place to learn for everyone, but it does have massive advantages of having a curriculum designed by experts, putting you in a situation where you kind of have to learn, providing all kinds of help, and putting you in contact with other people interested in the same. And the diploma you get is an instant indicator to anyone that you have some base level of skill and knowledge.

Of course you don't have to wait until college to start learning about AI. Learning to program better will help you throughout your education and career, and actually learning about AI also can't hurt. (Just take a look at the wiki.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-20 03:45:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> Each die has 6 sides, so when you throw two dice A and B, there are 6\*6 = 36 possible possibilities for which sides will land upwards.

You can throw an 11 in only two ways: A5B6 and A6B5, so the probability to get an 11 with normal dice is 2 out of those 36 possibilities or 2/36. If we now modify die B in the way you specify, then A5B6 is still possible, but A6B5 isn't because there's no side on B with a 5. So now the probability to get 11 is only 1/36.

With normal dice there are actually 6 ways to throw a 7: A1B6, A2B5, A3B4, A2B5 and A1B6, so the probability of throwing a 7 with normal dice is 6/36 = 1/6. If we now modify B as you mentioned, then A2B5 is no longer possible. *However*, there is now also a new possibility: A3B4^(2), where B4^(2) is the second side with a 4 on it. In fact, there are still 6 possibilities: A1B6, A2B5, A3B4^(1), A3B4^(2) and A1B6, so the probability to get a 7 doesn't change.

In general, if you're going to replace some side X with Y on one die, then the probability of rolling some sum N goes down by 1/36 if N can be thrown with X (and something else on the other die), and it goes up by 1/36 if N can be thrown with Y. If those things are both the case, you get no change: +1/36-1/36 = 0. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-19 13:12:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think he should elaborate on what he is envisioning exactly. Maybe write an article that also addresses some common criticisms of regulation, rather than some vague tweets. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-19 13:06:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> For current and near-term problems, you can look for work on "AI Ethics". There's a list of problems in [this article](https://towardsdatascience.com/ethics-of-ai-a-comprehensive-primer-1bfd039124b0) (under "Mapping the landscape"), which seems to omit autonomous warfare, nudging, various kinds of manipulation (like addiction and Facebook inducing negative emotions and division to enhance engagement), and other [malicious uses](https://maliciousaireport.com/). I would probably recommend one of these to write your report on, rather than trying to tackle the whole area of AI Ethics, although it may also be interesting to take a look at governance with respect to AI and what your country's [national strategy](https://futureoflife.org/national-international-ai-strategies/) is (or why it's lacking one).

For longer-term problems you could look at the /r/ControlProblem (see their sidebar and wiki for resources) or possibly something related to sentience/consciousness and what it implies about robot rights. There are debates about whether these things should be worried about (now) that you could write about, or you could pick a particular approach and write about that.

On the less problematic side, you could also write about the good that AI can do. For instance, how can they help with the UN's [Sustainable Development Goals](https://www.nature.com/articles/s41467-019-14108-y)? Or you can look at what good AI has already done or is promising to do in various applications areas like medicine, infrastructure, the internet and so on. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-18 11:40:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> The problem doesn't seem to be that they have dollar signs in their eyes, but that openness is often harmful to their primary mission of ensuring safe AGI for humanity. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-18 10:44:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but this story was already posted [here](https://www.reddit.com/r/artificial/comments/f5j98r/the_messy_secretive_reality_behind_openais_bid_to/) earlier. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-18 03:33:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but this story was already posted [here](https://www.reddit.com/r/artificial/comments/f5j98r/the_messy_secretive_reality_behind_openais_bid_to/) a few minutes earlier. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-17 13:19:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> I would recommend starting with the modding scene for the particular game you're interested in. You'll need to know how to make a bot in the first place before you can start putting in better behaviors.

Most of the time in video games, the "AI" that works best (i.e. that's the most fun to play with/against) consists of relatively simple rules, decision trees, state machines, etc. For more information about this, look specifically into "game AI" (e.g. /r/gameai and /r/gamedev).

If you want your bot to learn to play the game from scratch, you will probably want to use (end-to-end) reinforcement learning. You can Google those terms together with "FPS AI" or something to find quite a few papers on this. One environment I remember is [ViZDoom](http://vizdoom.cs.put.edu.pl/) but I think there's also a lot of research with Quake 3 Arena.

You're talking about learning from professional players, which sounds closer to supervised learning than reinforcement learning. However, the reason you (probably) can't use a pure supervised learning approach is because there's way too much variety in these games and you're not going to have enough data. (Even DeepMind's AlphaGo mainly used reinforcement learning to play the much "simpler" game of Go, and supervised learning was scrapped entirely in later versions.) You may incorporate "expert demonstrations" in reinforcement learning though.

Another possibility is to build a relatively simple "game AI" bot, and then just have a few learned parameters. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-17 13:02:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't think so. It looks like anyone can sign up, and one of their "stories" is about the Rotterdam police department, which is in the Netherlands. In the story about Nokia it mentions their database covers license plates from 60 countries, and in the documentation they mention how you could train it for your own country with about 200 images.

Even if you can't use this particular company, I would suggest finding another that does work for you. The other suggestion here seems to be that you could do it yourself, but I think that's a bad idea. It's not *that* hard of a problem, and an AI student could probably make a passable program in a week, but it probably won't be very robust and user friendly.

Deploying in the real world is often hard: Can you deal with different lighting conditions, rain, snow, dirt, different angles, weird license plate placements, etc.? Actually, even a professional company will probably not get 100% accuracy, so you will probably have to plan for the eventuality that the system sometimes won't work properly. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-13 08:42:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> The idea of a bimodal distribution in programming education is quite old. Instead of seeing student grades follow a normal/Gaussian distribution (like in most courses?), the results for introductory programming classes tend to show two "humps" for students who get it and for those who don't. This seems to be a fairly widely experienced phenomenon, although it is now also criticized quite a bit.

[This StackExchange answer](https://cseducators.stackexchange.com/a/784) has a lot of links and good commentary on the issue. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-12 15:41:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> Why not post the [link to the original article](https://www.dailytargum.com/article/2020/02/artificial-intelligence-will-increase-policing)?

Anyway, the title of this article seems misleading. The article talks about facial recognition and policing, but says nothing about how policing would be increased. It basically just lists of some places where facial recognition technology might be used (e.g. police, church, schools), but never really gets around to specifying the "ramifications" or why they're supposedly "terrifying".

> [Clearview AI] is currently only selling to law enforcement, but then it is able to monitor what law enforcement is doing. It knows who law enforcement uploads, searches or investigates.

This is indeed something that law enforcement needs to be very wary of. It would be better if they could purchase the software and run it themselves, without providing Clearview with this (non-anonymized) data. Generally speaking, police/government also need to avoid becoming dependent on particular companies.

> It also possesses the power to remove specific officers or agents from the app, giving this company power over law enforcement agencies across the country.

This is again a valid concern, which does not even rely on law enforcement using facial recognition tech themselves. If criminals have access to this kind of technology, it could compromise undercover operations. More generally, this kind of technology does make it easier for everyone everywhere to figure out who they're seeing / dealing with, which might have negative ramifications. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-12 15:20:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think there are multiple "robots" on Twitter that are being operated by humans who are essentially playing the role of that robot (like one might also operate a "Batman" account). One example might be [Sophia](https://twitter.com/RealSophiaRobot) (which is at least also a real robot that could somewhat interact with you in real life).

There's also [apparently a lot](https://www.theguardian.com/technology/2018/jul/06/artificial-intelligence-ai-humans-bots-tech-companies) of Mechanical Turk / Wizard of Oz techniques being used.

Finally, I think it's fairly common to give AI too much credit, especially in generative AI that generates things like art. There are many examples of songs, paintings and stories / movie scripts that are supposedly made by AI, but where the human designer is actually doing a lot of work and the AI is essentially just filling in some blanks (after which humans select only the best instances of the work, and then possibly do some post-processing to clean it up / make it better as well). These feats may still be impressive, but I think they often give the wrong impression about what AI actually did and can actually do at present. (Sorry I don't have any links.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-12 11:53:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> This might be interesting: https://blog.lunit.io/2018/06/14/uncertainty-and-deep-learning/ </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-12 11:15:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> I recommend looking [here](https://www.findamasters.com/masters-degrees/artificial-intelligence/?300iEO0). I don't know if they all have bridging programs or pre-masters etc. so you might have to look into that, but I feel like I've never heard of a program that didn't have that.

BTW, we have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki which might be useful if you're looking for online courses to take etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-12 09:59:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think in this case there's no need to overthink it and you can just look at your data. If you have data about N possessions and there are X shots in the tenth second, then the probability of a shot in the tenth second is X/N.

If e.g. 9 seconds have already passed, so you know no shot was taken in those, then the probability of a shot occurring in the next second is X/M, where M is N minus the number of shots that occurred in the first 9 seconds in your data. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-12 09:54:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't believe this is correct. Certainly if you look at your equations, you could just divide both sides by P(shot at t=2) in the first and end up with the conclusion that P(shot at t=1) = 0. And then similarly for the second equation and so on. You'll conclude it's impossible that a shot is ever taken. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-11 17:08:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's a [Decronym bot](http://decronym.xyz/) that operates on some subreddits. If the mods agree, they could enlist this bot's help (or look at the [source code](https://gist.github.com/Two9A/1d976f9b7441694162c8) and make something similar). It uses a manually filled list of acronyms though. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-10 13:54:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's a whole subfield of AI called "(cognitive) developmental robotics" (or sometimes "epigenetic robotics"). I think it's quite interesting, although I kind of wish there was less emphasis on the robotics part. There's also "developmental AI" but it's not very big. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-10 12:35:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think predictive policing should be pursued, but with some caution.
The potential to prevent crime is just too valuable. Also, it's not new (we're just using more powerful methods now). We should obviously be wary of false positives, but we also need to acknowledge that we have false positives now.

What should be done with a prediction that someone will commit a crime is another issue. I don't think we should convict such a person of e.g. murder, if they haven't actually committed it. But we already have categories like "attempted murder" and "conspiracy to commit murder", and we may use something similar. Or something else entirely, such as increased surveillance, a notification, etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-10 11:53:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you're looking to get into graduate programs (or a job), you might also look at getting some credentials. Usually you don't put "I read this book" on your resume, but you might include (online) courses you took, so that might be something to look into. (A lot of online course providers have paid options for certification, which you may consider, but I also think you can just put a course on your resume if you completed it, regardless of whether someone gave you a diploma.)

I also think that for learning, it's good to actually program the algorithms you learn about, and ideally to do some projects. Building a portfolio on e.g. GitHub and/or Kaggle will also make your AI skills more visible. I might seek out books and courses that stimulate this.

Aside from that, you're right that AIMA is great, and you could also consider reading the Elements of Statistical Reasoning and the Deep Learing book (both of which you can obtain for free). I don't know if they contain many exercises/projects though.

More philosophical books about AGI include Bostrom's Superintelligence, Russell's Human Compatible and Tegmark's Life 3.0. For nearer-term AI there's O'Neil's Weapons of Math Destruction and Broussard's Artificial Unintelligence. All of these are pretty controversial though. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-10 11:39:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you look in [this other thread](https://www.reddit.com/r/artificial/comments/f0mmiu/what_steps_can_i_take_to_work_with_ai/) where a 16-year-old asked a similar question, you'll see there's some discussion on this. I've been in academia for a while now, so I don't have a super firm grasp on the job market, and it also depends on where you are. But my estimation would be that you can be an "AI engineer" without a PhD. After I got my master's I was a "computer vision engineer" for a while, and that role actually involved a lot of R&D.

Let me put it this way: there are different jobs where you do *something* with AI, and depending on your level of education, you can get into some of these jobs. You don't need a PhD (or a master's) to build a straightforward neural network, or even to be the programmer for AI research scientists. You'll probably need a graduate degree to be an AI researcher/scientist (especially at the larger, more successful companies).

However, I'm not sure how fruitful it is to worry about this at age 15. For now, I think you should continue what you're doing and learn about AI/ML to follow your interest (see [/r/artificial's wiki](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) for more info). When it's time for college, and you're still interested, choose a related program. And when you're nearing the end of that, you can decide whether you want to pursue a master/PhD. This will be quite a few years into the future, and both you and the field may have changed a lot by then. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-10 11:29:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> As I wrote in the [Getting Started article](https://www.reddit.com/r/artificial/wiki/getting-started#university) on the wiki, I think that if a specialized AI program is not possible, CS and math are the best things to study. I personally feel like you can never know too much math, but a lot of CS is not that useful for AI or easier to learn on your own. The main "edge" CS has over math for me is that CS programs tend to offer more AI-related courses and that more students and faculty will be doing something with AI, making your path towards AI a bit more natural. But content-wise, I'd probably think mathematics with the right electives might be better.

If I was in your shoes, I'd definitely go with the Math-CS program because it seems like you could get the best of both worlds. Plus, you say it's a selective program, which will also help set you apart. (If it's a double major and you'll get two diplomas, I'd also definitely do that, because usually it means twice the diplomas for a tiny bit of extra effort.)

But you don't have to. If you enjoy your CS program, that should be fine. I feel like some people might be contrasting your options in a way that suggests the CS program won't have any math, but CS should actually already be pretty math-heavy. I'd guess that most people who get into AI studied CS at the undergrad level. You'll definitely still be considered a candidate.

So you can do what you want. Either choice should be okay. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-10 11:12:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki that may be helpful.

It sounds like you're already doing a great job with the math and programming. Those will be very useful when you're learning about AI. I think you can never know too much math. I wouldn't say they're a *prerequisite* for starting to learn about AI though, so you can also start doing that. (E.g. by taking an introductory course on Udacity, EdX or Coursera.) It would be great if you could figure out what areas of AI you're most interested in, which can then inform your choice of college.

If you can find a program that specializes in AI (or perhaps machine learning), that would probably be the best. Otherwise computer science is probably the closest thing, although you could also consider mathematics (and if you're so inclined, cognitive science, neuroscience, etc.).

I just accompanied my 17yo nephew to an open day for a university he might want to attend, and the mathematics program has students choose from 7 specializations in the second and third year, which include computer science and data science, so I think that might also be a pretty good path towards AI. (Although that university also has a specialized AI program, which is of course a more direct path.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-10 10:58:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> Depending on where you are: yes. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-05 20:22:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> Dileep George was co-founder and CTO at Jeff Hawkins' Numenta before he left to co-found Vicarious. I think their work grew out of Numenta's Hierarchical Temporal Memory. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-05 19:55:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Neither am I. But my intuition leads towards a "no". I am convinced that it's wrong to create sentient life that is not worth living. I'm not convinced that it's "morally good" to create sentient life that's worth living.

Can you articulate the reason that you wouldn't wipe us out now, given what you say here? If happiness/pleasure is worth nothing and suffering/pain is, then it seems the conclusion should be to eradicate all sentient life as soon as possible. I'm also not sure I like the idea that some number of heaven universes offset a hellish universe, but I do think there is at least some value to happy sentient life. I'm (also?) having trouble articulating my intuitions in such cases though, but I do suspect our intuitions are a little bit different with me leaning more towards the value of (happy) sentient life.

I misspoke a bit regarding my concern about extinction. I agree that we might be succeeded by AI that I would consider better than humans, in which case I might be okay with human extinction. One scenario might be if we gradually replace body parts with superior artificial components until there's nothing biological left, but the resulting robots are still sentient and possibly better in some way (e.g. more virtuous). But if humans go extinct by creating unaligned AI, then we were (almost by definition) not replaced by something (we would consider) morally better. Leaving AI aside, if we go extinct through some other means (e.g. giant meteor), it's a large tragedy because it doesn't just mean the loss of 8 billion current lives, but also all future ones (so wiping out 7.92 billion wouldn't just be 1% better, but almost infinitely in my opinion). Not everybody cares about future lives, but it's a bit akin to old people still caring about the climate and not wanting it to be fucked up for future generations. </TEXT>
</WRITING>
<WRITING>
<TITLE> [R][2001.09768] Artificial Intelligence, Values and Alignment - Iason Gabriel (DeepMind) </TITLE>
<DATE> 2020-02-05 19:04:55 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-05 18:57:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think this falls under the general heaving of "suffering risks" or *s-risks* (instead of or in addition to "existential risks" or *x-risks*). Some people are worried about this, and you can read more here: http://s-risks.org/

I think you're generally right that it's even harder to get people to actually care about machine rights, although you occasionally hear concerns in that direction (mostly from laymen though). I'm not sure this is entirely irrational. We know essentially nothing about sentience (except that other humans and maybe animals are probably sentient too). Furthermore, extinction is final. It may be very bad if some dumb kids create a few "20th centuries worth of suffering inside a computing supercluster", but as long as we're not extinct we have a long future to make up for it. I'm also not sure about my own feelings regarding the moral calculus here: would it be okay to offset one "hell universe" by two "heaven universes" in the supercomputer?

But ReasonablyBadass mentions an avenue through which we might get more people to care. We may not care so much about the suffering of abstract beings that are so unlike us that we have no idea what that's even like (beyond "yeah, that's bad I guess"), but we might care more if someone could upload our own sentient minds into some computational torture chamber. That's still quite chauvinist and human-centric of course, so it may not appeal to you, but I think this might be a better way to get traction for the idea, which can then later perhaps be extended beyond (emulated) humans. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-05 16:27:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Every time I post there have been new developments with the software, new code, new ideas.

Then describe those new ideas instead of linking to the product. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-05 15:55:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Maybe this is not the place for new ideas.

It is. Your ideas were *maybe* new the first time you posted a link to your AI here. But this is far from the first time, so it's not new anymore. Furthermore, you're not even really discussing your ideas; you're just posting a link to your own website (which doesn't talk about the ideas underlying the AI either). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-05 11:52:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a rule here against [self-promotion](https://www.reddit.com/wiki/selfpromotion), so please stop posting the same link to your own work over and over again. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-04 15:10:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> The only thing I know about Lugano is that Schmidhuber is there. If you're interested in that kind of thing, you could try to do a PhD in his IDSIA lab after which you'll meet the requirements for NNAISSENSE. Otherwise you could still contact them and ask if there might be any role for you that's not on the website. You could say you're very interested in their work and perhaps also the advertised jobs and that while you don't meet the requirements yet you're eager to learn etc. It's quite likely they won't have anything, but it can't hurt to try, and if they reject you, you could perhaps ask for advice on where you could get the experience they need in the area. (They might not respond to that, but again, you have nothing to lose by asking.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-02-03 11:00:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Not sure if Im allowed to post links, you know how reddit is.

You are. It's kind of the (original) point of Reddit.

> Would it technically be inhumane to torture an ai like this?

That depends on whether the AI is actually capable of suffering (i.e. sentient). We have no idea how to determine if someone or something is sentient, but we strongly suspect that other humans and animals are while inanimate objects and machines (including AI) are not. Future AI may or may not be sentient. We don't know. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-31 18:29:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> I didn't know about this, but Columbia is a good university and I imagine getting a certificate from them (even if it's for an online program) will likely mean something to employers.

The website talks mostly about *applied* ML and AI, but the math prerequisites seem to suggest you'll cover quite a bit of theory as well. I wanted to download the syllabus to clarify the program's focus, but after annoyingly making me register the download link didn't even work which seems a bit unprofessional (then again, mistakes happen and it's still Columbia). I might ask them about it though if you're planning to invest your time and money. I would probably also ask them to clarify what they mean by "undergraduate knowledge" of the various math fields they mention. Presumably a math undergrad will know more about these things than a computer science undergrad, who will know more than a psychology undergrad.

If applied ML & AI is what you're looking for, and the method of education appeals to you, then it seems fine to me. You can find links to other online courses in our wiki's [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). They're often cheap(er) or free, and Coursera/Udacity/others(?) also offer programs of courses that are tuned to each other. My guess would be that certification from Columbia will mean more to most employers, but I'm not an expert on that. </TEXT>
</WRITING>
<WRITING>
<TITLE> Book Review: Human Compatible - Slate Star Codex </TITLE>
<DATE> 2020-01-31 15:27:23 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-31 15:15:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> You haven't really told us anything about yourself except that you have some basic Python skills...

But yeah, you can probably start learning the basics of AI. My bachelor program was called "Artificial Intelligence: Cognitive Science" and the only prerequisite was a high school degree with some math (not even programming, although that *was* one of the first courses in the curriculum).

To be honest, I'm not sure it's ever too early to *start* to learn the *basics* of AI. Maybe just check out your program's prerequisites. It will be useful to know a little bit of programming, but you can also get better at that by implementing AI algorithms (which will also help you understand them, so you kill two birds with one stone). Various topics within AI will have math requirements like linear algebra, calculus and statistics, but that might not even be necessary to get started. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-31 15:05:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> > When I was in college I remember a professor telling the class that a significant difference between computer programs and human intelligence was the ability to recognize an infinite loop.

I don't think this is a significant difference. Nobody can solve the halting problem generally. Humans can recognize *some* infinite loops in code, but you can also write software that recognizes *some* infinite loops.

> If a data point is truly irrelevant is machine learning software smart enough to ignore it?

That probably depends on the ML software and on what you mean by "ignore". A decision tree algorithm would presumably not use this useless variable to split on. Would a neural network set its weight to zero? Not necessarily, but it doesn't matter because those weight would just have the same role as a node's bias term. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-31 14:57:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> All I know is that I can't find much about them online, and they're spamming our subreddit regularly. I'd avoid them, and perhaps look towards Coursera or Udacity or something for courses on AI. (See also the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on our wiki.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-31 14:50:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think CGP Grey is talking about the (hopeful) future of [intelligent tutoring systems](https://en.wikipedia.org/wiki/Intelligent_tutoring_system). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-31 14:42:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> Interesting. Do you have more information or a link to the seminar? Or is it something at your local university? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-31 02:33:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> I talk about that analogy in my previous comment, which is over 2 months old so maybe you didn't reread it...

The idea is that if you build X% of a rocket, it's not going to get you X% of the way to the moon. In fact, it might not get off the ground at all until it's 100%. AGI might have the same problem, where if you only build a part of it, it basically won't do anything until it's actually finished. So even if you had a good test of general intelligence that would (also) work for machines, that won't help much if the most promising projects are all-or-nothing like this.

Ben Goertzel wrote a bit about this (I think the link is in that old comment on AI evaluation) and it's also related to his idea of cognitive synergy. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-29 16:03:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> There have always been different approaches and focus areas in AI, and for a time logic-based and knowledge-based approaches were more successful and widely practiced than connectionist approaches. But the idea of connectionism is more than a century old and definitely precedes the era of GOFAI, so I wouldn't say it grew out of it. It just became more successful (again), while other methods became less successful (although they're probably still used more than you think).

I do think that generally speaking the field has moved from doing "hardcoding" to machine learning (to which neural networks and deep learning is just one family of approaches). This is probably in no small part due to projects like CYC which made us realize it's impossible to enumerate all human knowledge, but this too was already suggested by Alan Turing in 1950 (and probably earlier), so I'm not sure if it's correct to say that this grew out of GOFAI either.

The connectionist idea of neural networks is indeed inspired by the brain, and some advances in their evolution have been as well, although I think most of them just came from researchers trying to come up with designs that would perform better. The neural networks we've been using most are all highly abstracted mathematical models that bear only a tiny resemblance to biological brains.

I'm not really sure where the idea of layers in neural networks came from, but I am quite sure we're just using it because it works. For a time, perceptrons were popular. They're often called single-layer perceptrons, to contrast them with multi-layer ones, but it's really just input and separate output, so I'm not sure how meaningful the concept of a "layer" is in that context. It seems that the main reason to move away from them was that they can only model linearly separable problems, and for a long time most practitioners just used MLPs with a single hidden layer because it was proven that this should theoretically be sufficient to model any problem, and because there were some difficulties with getting deep networks to work. Only when those problems got solved, and when we got sufficient data and compute, did people start switching to deep neural networks. And certainly the deeper ones don't seem to have anything to do with biology, because it actually takes pretty long for biological neurons to communicate with each other (so if we can recognize an image in less than a second, it certainly didn't take 200 layers of neurons).

I haven't heard anyone make a link between cortical columns and layers, and from my understanding this also doesn't really work. I know Jeff Hawkins wrote a lot about cortical columns, but his neural network is very different from traditional ones, and the cortical columns are more akin to complex nodes in his Hierarchical Temporal Memory networks than to layers.

For me the main intuition is indeed that each layer does some kind of feature extraction (not just CNNs). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-29 12:27:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> IQ tests only (kind of) measure "general intelligence" (the [*g* factor](https://en.wikipedia.org/wiki/G_factor_(psychometrics)) *in humans*. They work, among other things, because we already know humans have general intelligence and their level of intelligence is positively correlated with performance on a lot of different cognitive tests. Look at these [different kinds of IQ questions](https://examples.yourdictionary.com/examples-of-iq-questions.html) and ask yourself for each one how hard it would be to create a specialized AI system to answer just such questions. For some it'd be hard, but for a lot it would be trivial. And such a machine's good performance on these questions would tell us next to nothing about its *general* intelligence (unlike for humans answering such questions). And for the remaining kinds of questions, it's likely that they would essentially test something different in humans than in AI. For humans it's typically a given that they *understand* the question, but the difficulty is in solving the problem it presents, while AI usually only struggles because it's hard to figure out what the problem is which it could then easily solve.

But yeah, a benchmark that would *actually* measure general intelligence (in machines) would be very useful. It's also an open research problem. I wrote a bit about that [a few years ago](https://www.reddit.com/r/agi/comments/52tv08/benchmarks_besides_turing_test/?depth=20) if you're interested.

This still wouldn't really solve the problem where building 99% of a rocket doesn't get you even 1% of the distance to the moon though. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-29 11:58:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> I assume you're talking about Russell & Norvig's properties of task environments?

In the paragraph after the one about the (un)known property they say:

> > "As one might expect, the hardest case is *partially observable, multiagent, stochastic, sequential, dynamic, continuous and unknown*. Taxi driving is hard in all these senses, except that **for the most part** the driver's environment is known." (**bolding** mine)

As you can see, they don't treat this as a strict dichotomy and say it's *mostly* known rather than *fully* known or unknown. Presumably soccer is similar.

If for some reason you did want to make this into a strict dichotomy, I'd say it's not known (a.k.a. unknown) in the same sense that "mostly observable" means the environment is not fully observable, "mostly single agent" means it's multiagent, "mostly static" is actually dynamic, etc.

Instead of viewing these as binary dichotomies, it's probably better to view them as dimensions like observability and stochasticity, where the "easy" case is just one extreme of it (i.e. fully observable is 100% observability and deterministic is 0% stochasticity). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-29 11:35:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is a very general question, that mostly transcends AI. People and organisations have to deal with uncertainty in many things. If we focus on just their assets, this includes employees, machinery and software, and AI is not really that different. I'm not a risk management expert, but off the top of my head, I think uncertainty is dealt with in roughly 4 stages (where you continually go back to the first): 1) map it, 2) reduce it, 3) accommodate it, and 4) accept it.

First, you need to know what your uncertainty is and how big it is, etc. Various kinds of testing are used for this. Then, you will probably try to reduce it. In machine learning, you might try different models, preprocessing methods, or getting better data to increase your model's accuracy or robustness for instance. When it's no longer possible / cost-effective to do this, the uncertainty might be accommodated somehow. For instance, by putting in redundancies. For a taxi company, this might mean having some spare taxis (in case some break down). For AI this might mean that we'll "redundantly" run other decision-making processes as well (i.e. humans who make the final decision, informed by AI, rather than letting the AI act directly). And finally, we accept whatever uncertainty remains. We know that nothing is perfect, and if an AI system makes 5% errors, maybe we're okay with that in some contexts (especially if the human decisions that are being replaced are wrong 10% of the time).

One important caveat here is that we're not always fully aware of all risks / uncertainty. There are unknown unknowns. Another thing is that knowing about a risk kind of obligates one to deal with it, so this may in some cases incentivize people to not dive too deeply into all the things that could go wrong. Finally, when we're talking about risks and costs, we need to ask who will be affected/paying. And if the answer is "not the developer/owner/user of the tech", this often ends badly. For instance, we might imagine loan-approving AI for a bank, where the bank (or developer) may be vaguely aware of some risk that it's discriminatory, but they don't care that much because they just want to maximize profit, and the people who pay the price are probably unaware. In this case they need plausible deniability more than they need to actually fix the problem. (This last paragraph is just to point out that unfortunately "how it's dealt with" is often not how you'd hope it would be dealt with.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-28 15:40:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> Can you clarify the question? Which of the following do you mean?

1. X^(-1/3) = (32X)^-2
2. X^(-1/3) = 32*(X^(-2))
3. X^(-1/3) = 32^(-2)

? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-24 11:58:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> LaTeX is not computationally intensive. Take a look at the [requirements](https://www.maplesoft.com/products/system_requirements.aspx) for Maple. It's nothing super special.

If you're going to do (big) machine learning or data science projects, you may need a lot of computation. But in that case I imagine the university would also provide access to some type of cluster, and it would probably be educational for you to learn to use it.

But I imagine that for taking regular math courses, you don't need anything super heavy. Almost any laptop will likely be sufficient, although I would go quite a bit beyond Maple's recommended requirements just to avoid being annoyed at some slowness.

Of course, if you like tinkering with GPGPU, machine learning, etc., you may want to buy a gaming laptop or something, but I don't think it will be necessary for your studies. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-23 01:53:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is a good illustration of how programs can do things the programmers don't expect, but I'm not sure it addresses the negligence issue. We can't expect programmers to know how their Game of Life will behave, but we *can* expect them to know about their own uncertainty. And if you're highly uncertain about how a system will behave, using it in e.g. a safety-critical application certainly seems irresponsible/negligent. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-23 01:50:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> > This definition is what is used in the law in my country.

I know. What I said was not intended as a criticism, but as a statement of fact. I'm not a lawyer, but it's my understanding that many laws are written this way and ultimately left up to the interpretation of the judge (after lawyers/DAs have tried to persuade him/her of their interpretation). I think lawmakers regard this as a feature rather than a bug, because they acknowledge that the law moves slowly and this way it can automatically be "updated" with what seems reasonable at the time. Plus it absolves the lawmaker of the impossible task of knowing and anticipating all the details of every possible situation in which the law might be used, and how it might interact with other laws (including future laws).

I sometimes hear from law researchers that it's common for freshman CS/AI students to ask "why not just write the entire law book as code?" (I admit this also applied to me). That way you get rid of all the ambiguities and subjective *application* etc. But even if this were indeed desirable, we couldn't do it for the same reason that the control problem hasn't been solved and humans are still better at some things than AI systems. Essentially, the law is a partial "program" with "*<insert human judgement>*" here and there.

>  what level of diligence do programmers or producers employ in order to ensure that no untoward incidents result from the AI program or at the very least decrease the likelihood of these occurring?

I agree with /u/WriterOfMinds that this depends almost entirely on particulars of the AI system, what it's going to be used for and who will be using it. WriterOfMinds is correct here in naming some safety-critical domains where a lot of precautions would be taken. Overall I'd say that the minimum that people tend to do is test their system on some data/situations that did not occur during training, but the variation is often very small (actually new real-world data may be much more different, and may also change over time as the world evolves). Given the new awareness of ethical issues with AI, especially regarding bias/fairness and privacy, some additional efforts might be made to mitigate these issues (or perhaps just to *appear* like you're doing something to mitigate them).

But I don't think you can just look at the producer (or programmer). Because often they sell their software, and a contract is involved that specifies (in part) who will be liable for what. Often this involves transferring liability to the buyer, even in cases where erroneous behavior is actually the fault of the producer. I think many producers would take measures to avoid liability, be that through such contracts or through technical means to ensure nothing will go wrong in the situations they'd still be liable for. But this can get complicated quickly: e.g. AI company sells system to car company, who puts it into their (partially self-driving) car, which is sold to a human driver. Who is liable for an accident? It probably depends in a large part on all of the involved contracts (more so than the question of moral responsibility).

> I'm thinking of just expanding my thesis to discuss about the level of diligence the programmers or producers need in order to be free from liability.

That sounds like a decent idea, although it will depend on the situation and might be difficult to address in general. But it's probably not impossible to say some general things about it, and otherwise it may help to focus on a specific application or domain.

> Also, because you mentioned that even non AI systems can still behave in unintended ways, does that mean that autonomy is not the important point here?

I think it's more that autonomy is not something only AI systems have. Basically, all software "acts autonomously" in between your controlling actions. While you're reading this, your operating system is doing all kinds of things in the background. When you clicked on this, your browser communicated with a bunch of other computers to render this page for you "autonomously".

Autonomy *is* kind of important though, because programs can't bear moral responsibility or legal liability. So if there's more human interaction, interference or control (i.e. less autonomy for the system), then it's more likely that you can blame a human when something went wrong (or it's just easier). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-22 12:37:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> I guess it depends on what you mean by "official". In addition to the European Commission committee's work, there's also a [report](https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/10002.htm) from a UK parliament committee and [IEEE](https://ethicsinaction.ieee.org/) and [ISO](https://www.iso.org/news/ref2336.html) have published some things. Different countries might also have published guidelines, but the main thing is of course to follow their laws (like GDPR, but also e.g. anti-discrimination laws that weren't necessarily written with AI/technology in mind). There are also the [Asilomar principles](https://futureoflife.org/ai-principles/) which have been enshrined into California law.

This is just the stuff I know about off the top of my head. There should be a lot more. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-22 12:22:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> Take a look at the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. I think most of the linked online courses there have exercises where you write code. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-22 12:14:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think there is quite a bit of work on (moral) responsibility, accountability and liability combined with technology / automation / algorithms / machine learning / AI. I recommend looking into that. These are complex issues that cannot just be swept aside by a single objection like your adviser might be doing. However, he might also just be testing you and seeing how you defend your thesis against this "attack".

> Is it possible that an AI can commit acts [...] that were not intended [by the programmer]?

The answer to this is an easy *yes*. You don't even need AI for this: regular non-AI programs also have bugs and behave in unintended ways. Even leaving that aside, complex systems have complex interactions with the real world that can be difficult to anticipate. It's even more complex in a lot of AI/ML systems because parts of them are often not *explicitly* programmed, but rather "trained".

> barring any negligence

This is what makes it complicated, because you mentioned that you can be negligent without intending to be (or intending the bad actions that result from it). Your definition of negligence seems very open to interpretation of what a "prudent and reasonable man" would do, and I could also see this shifting over time.

For instance, AI systems can discriminate against people based on race, gender or age, which are probably all illegal (depending on where you are). I'd say that ~10 years ago, most people (including programmers) were probably not very aware of that. Over the past 5-10 years, this awareness has grown drastically, and better tools and methods have become available to fix this. I could imagine that a judgement of what a "prudent and reasonable programmer" would do could be different now compared to 10 years ago.

It's currently still common practice to "ship" AI systems that the programmers don't fully understand. If it's common practice, does that mean all programmers are not "reasonable and prudent"? Or does it mean that if we assume that some programmers must be reasonable and prudent, and they're apparently okay with this practice, that it is not negligent (at present)?

So far I've gone along with talking about "the programmer" of an AI system. But in practice, there's usually not one programmer: on most professional AI systems there are many programmers, designers, data gatherers, annotators, testers, data scientists, managers, etc. No single person is going to know exactly how the whole system works, so there's no sense in talking about "the programmer". Maybe you could talk about "the producer" as a (corporate) entity responsible for the entire system's production, but this is already different.

Then there are also people responsible for deciding how the system should be used, which might be different from the actual people using it. And then there can be other people and systems that interact with the AI system in ways that can change its behavior.

So basically it's a complex web of interacting people, systems, contracts, etc. When people make, use and interact with technology, they have a responsibility to take precautions, but there's a limit to how much a "reasonable and prudent" person would be expected to do. It seems to me that there can certainly be situations where someone might be aware of the uncertainty involved in AI, and it nevertheless being the most responsible choice to use it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-21 08:43:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Serious question - are you unfamiliar with editorial pages? Newspapers often contain opinion pieces.

A columnist endorsing a candidate in an opinion piece seems very different from the newspaper itself endorsing a candidate. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-21 00:55:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> Is endorsing a political candidate considered a normal thing to do for American newspapers? I get that (Edit: nobody) can be 100% objective and unbiased, but I thought that most newspapers at least like to keep up the charade. Especially if they think of themselves as the "paper of record". Endorsing a political candidate seems so diametrically opposed to the idea of attempting to bring unbiased news that I feel they might as well say "please disregard any news we might bring with regard to these elections going forward, as it's all going to be propaganda for our chosen candidate and we're not even going to pretend otherwise". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-20 17:16:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> It is not exactly clear to me how (automated) yield estimation leads to the benefits you mention. If fruit is picked manually, the overhead of counting seems minimal, and even if that's not the case it seems like you could just weigh the yield. I assume the (automated) yield estimation is done at some point before the harvest, in order to allocate harvesting resources or something? This should be made clear I think.

In any case, this project sounds like it will be too difficult for a bachelor project. If you already had a dataset of pictures of orange trees, counting the number of oranges in each picture seems like it might be doable. Maybe also classify if they're "good" or their stage of growth or something as well. If you also have to gather the dataset with a drone, that adds significant effort, especially if you need to gather enough data to train e.g. a deep neural network. If you also need to estimate the 3D coordinates of each orange from coordinates and multiple pictures, that will add extra effort as well.

You can still try of course. If so, I would work in stages. If possible, try to find a dataset of oranges on trees and try to automatically detect them. Then maybe take some pictures with your drone, and see how well your counter works on this new data. If it doesn't work well, maybe you can fine-tune it using your new data. If you can get that to work, you could maybe start on the structure-from-motion stuff. And so on, until you run out of time. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-20 16:54:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> > If so, how would you even begin to start? Let’s say I wanted today’s prices (open, high, low, close and volume) of every stock in the S & P 500?

What are you asking? I guess just have 5 input nodes (and therefore also output nodes; since it's an autoencoder) per stock that has ever been in the index.

> Sometimes stocks cease to exist

The simplest way to deal with this is to set the values for stocks that are currently not existing to some predefined value. Additionally, you could add one node/feature per stock that indicates whether it currently exists or not.

> and sometimes new stocks are created

If you're okay with using a fixed data set, just make a set of nodes for each stock that's in the data set at some point and handle "currently not in the index" as I said above.

If you want to handle actually new stocks that are not in your data set, this is going to be a lot more difficult with the proposed setup. I guess you'd want to create an extra set of input & output nodes for each new stock. Then the question is how to connect it to the network. You could do this with random weights and have it train more or less from scratch. Or maybe the average of the other weights, or perhaps just of the weights of the stocks you expect to be most like the new one, so you might start with behavior for an average (similar) stock. There are probably better methods.

> 2) Stock metrics are at different scales, so I assume they need to be normalized?

That seems like a good idea.

> 3) Is there any benefit in doing this?

Doubtful. Why did you want to make an autoencoder in the first place? If you instead wanted to predict the stock market, that seems to have obvious uses, but has proven to be very difficult if not impossible. (If it was doable, a lot of big companies would beat the market all the time.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-20 16:38:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> > there is a link for this but I cannot post the link.

Why not?

Is it this one?: https://onlinelibrary.wiley.com/doi/full/10.1002/smll.201903489 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-17 20:40:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> Yuqing7 links to all Synced posts, whether they authored them or not. You're right that this violates the rule you quoted, but as I mentioned I'm making an exception here because of Synced's high quality (which you are free to disagree with, naturally). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-17 15:07:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> /u/Yuqing7 posts content from Synced, which is an excellent source of AI news and commentary.

Normally when an account just posts content from a single source / their own website, this would indeed not be allowed. Here I'm making an exception because I think this subreddit is better off with Synced's content on it, and I appreciate /u/Yuqing7 posting it here quickly. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-17 15:07:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> /u/Yuqing7 posts content from Synced, which is an excellent source of AI news and commentary.

Normally when an account just posts content from a single source / their own website, this would indeed not be allowed. Here I'm making an exception because I think this subreddit is better off with Synced's content on it, and I appreciate /u/Yuqing7 posting it here quickly. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-17 11:30:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't like chatting on Reddit, so I'll just try to answer you here. Below I look at the websites you linked, but it's tough to actually give an answer, because the information you're looking for is typically not available (at least not if you're not signed up). But like I said in your thread, I think you're perhaps just a bit too worried about this. I would suggest that if this is a good way for you to make money, then you sign up for Remotasks, Spare 5 or maybe just Amazon Mechanical Turk or something similar. Maybe look at some online reviews, but be careful that they're not posted by shills of the company under review. Then, for each individual task, you could look at who you're doing it for and what it's going to be used for (if the platform provides that information; otherwise maybe just select another platform).

I think that for the most part, this is going to be fairly harmless, especially if you pick a big company to work through (like Amazon MTurk), and reputable companies to do the tasks for. I think your heart is in the right place, but you're perhaps just a bit too overly worried.

I wish you the best of luck!

---

The websites you mentioned:

> https://hivemicro.com (hivework)

There is virtually nothing on their website (at least not if you're not signed up/in), which greatly annoys me. If I was greatly interested in this, I might sign up and look further before actually doing any work for them. However, I also found [this review](https://www.homeworkingclub.com/hive-work-review-hivemicro/) that says they pay extremely badly. That's not the kind of thing you asked about, but probably enough to dismiss them. Some alternatives are suggested at the bottom of the review I linked.

> https://www.remotasks.com

The website looks much better to me and it has a [better review](https://incomopedia.com/remotasks-review/), although the Terms & Conditions link at the bottom seems to be a bluff, because it doesn't actually do anything. To answer the questions you asked, I guess you'd have to sign up and look at the information provided about individual tasks.

> Spare 5

Website looks pretty good and they list some of the companies they work with. Also seems to get decent reviews (e.g. [this one](https://toughnickel.com/self-employment/Making-Money-Online-with-Spare-5-A-Quick-Honest-Review). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-17 10:52:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's still the best textbook on AI, and the basics haven't changed much. This even applies to things like deep learning / neural networks. However, if you're specifically interested in machine learning or deep learning, then there are books that specialize more for those sub(sub)fields of AI.

But since we're so close to the 4th edition coming out, it may feel like a waste to buy the 3rd edition now (even though it's fine). You could perhaps bridge the intervening time with a free book like [Poole & Mackworth's](https://artint.info/2e/html/ArtInt2e.html). Or [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) for ML, or the [deep learning book](http://www.deeplearningbook.org/) for DL.

You might also be interested in our wiki's [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai), which also contains more links to (free) books. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-17 10:46:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Is ''machine learning'' an actual subbranch of AI?

Yes.

> Contemporary universities already segregate Machine Learning as a separate course from Artificial Intelligence.

Some do and some don't. But this is just a case of a lot of CS departments only wanting to teach the part of AI that is ML.

> Reddit and IRC channels already segregate them by social accidents.

Reddit has a lot of subs on AI, of which /r/MachineLearning is just one. There are also subs on /r/reinforcementlearning and /r/deeplearning, which are obviously subbranches of ML. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-16 13:03:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> It might help to post the specific websites you're referring to, so that we can see what they might be up to, what the terms & conditions are, etc.

AI and data can indeed be used for good or bad. I don't want to diminish the importance of AI Ethics (because that's an important part of my job now), but this is true for a lot of things. Almost anything you create can be used for good or bad. Hell, curing cancer could enable the next Hitler (if he would otherwise have died of cancer).

I definitely think it's worth asking these questions. If you're going to be annotating Middle Eastern locations for some kind of military contractor, you might want to think twice about that. But if you're annotating facial expressions for an app that's meant to help autistic people, that's probably okay. So yes, consider the bad that could be done with your work, but also consider if you think this is very likely and who you'll be working for (and what their intentions are). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-15 22:42:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> It does matter how many answer choices there are, but you said that number is always 4.

Suppose you have a question with n possible answers, of which c are correct and w=n-c are wrong. If you then pick one answer at random, the probability that you'll pick a correct one is c/n and the probability that you'll pick a wrong one is w/n, which is the same as 1-c/n. You said there are always 4 answers and I assumed only one of them is correct, so then the probability is always 1/4 to guess it right and 3/4 to guess wrong. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-14 18:53:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> > each with four possible answers

Okay, so the probability of getting any particular question correct is 1/4, and conversely, you'll get it wrong 3/4 of the time.

Since you're guessing at random, the odds of getting each question right are completely independent from what you do in the other questions.

So the probability that you get the first question wrong is (3/4), the probability that you get the first two questions wrong is (3/4)\*(3/4) = (3/4)^(2). And so on. This should help you answer your first question, and also the second if you switch to probabilities that you'll get a question right.

> At least one of the questions wrong?

This question is equivalent to "What is the probability that you DON'T get all the questions correct?", so you can use your answer from the second question to answer this.

> Get your first incorrect answer on the fourth question?

This is like asking "What is the probability that you'll get the first 3 questions right, and the 4th wrong?", which is like asking "What's the probability that you'll get Q1 right AND Q2 right AND Q3 right AND Q4 wrong?". We already saw that the probability of getting any particular question right was 1/4 and getting any particular one wrong is 3/4. You can use this to answer the four subquestions here, and then multiply the numbers together, because performance on different questions is independent of each other and you're using conjunctions ("AND"s). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-14 18:40:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> Differentiation and integration are each other's inverse operations.

You probably know how to differentiate a function like f(r) = ar^(n) + c, right? It's f'(r) = anr^(n-1). If we want to "undo" this differentiation operation, we can perform the inverse function: integration. The result of this should be integral_of(f'(r)) = f(r) = ar^(n) + c.

This is exactly what's happening in your textbook. Ask yourself: the derivative of what function is zero? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-14 18:25:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I really confused about the the idea of extending sides, thank you.

I'm not really sure how this helps to prove anything, but maybe this helps:

https://imgur.com/a/Xb00We7

As you can see, I highlighted R_1 in blue and some unnamed triangle in red and I extended their sides. This is what's meant, but it's done for every rectangle (not just the ones I showed). This creates a grid with more rectangles than before, because some of the old rectangles are now cut into multiple pieces by the extended sides of other rectangles. For instance, R_1 is now made up by R~1, R~7 and R~13, so J_1 = {1, 7, 13}.

I hope that helps. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-13 22:49:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> Search for "adversarial robustness". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-13 07:42:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> What do you think should be the proper way for famous people to deal with you/fanboys? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-13 01:43:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> You're wrong, which is okay, but personal attacks are never okay. Banned for a week. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-10 16:01:03 </DATE>
<INFO> reddit post </INFO>
<TEXT> I suggest taking a look at [probabilistic programming languages](https://towardsdatascience.com/a-gentle-introduction-to-probabilistic-programming-languages-ba9105d9cbce). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-09 01:41:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> It is the government's job to ensure that citizens' rights are not being violated and to govern the country in a way that is "good" for those living there. Among the rights the government should protect are several freedoms, which means it goes too far to say that everything a company or individual does (even just in a certain area) must be for social good.

I think governments should become more aware of AI, but for the most part they should not treat it in any special way. It's basically just another technology. We (should) have discrimination laws that say you can't discriminate against people based on traits T *whether you use AI or not*, privacy laws that say you can't gather information X in circumstances C *whether you use AI or not*, and a right to explanation about decisions you make that affect people in certain ways W *whether you use AI or not*. There might be some cases where it's absolutely necessary to make laws that are really specifically about AI, but for the most part we should aim to make laws based on more general principles. All laws should be written with an awareness of (AI) technology, and make sure that they don't accidentally limit it in unintended ways and that they are sufficiently enforceable.

Legislation is only one tool in the governance box. Others, like providing incentives, guidelines, support and information/education should also be used. Governments should also set the right example in their own use of AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-08 11:55:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's probably best to seek out possible thesis supervisors/advisors and discuss this with them. Look in your university (likely your own department) for professors and researchers who are doing work you find interesting. Odds are that they have ideas for future work that a master student could help them with.

This has a large number of advantages:

* They're going to be better than you at selecting a topic that's doable in the allotted time and acceptable for your program.
* If your work is actually helpful to them, they will probably have more time to help and work with you (and/or others in their research group might).
* They'll also be better able to help you than if you picked a topic that's not exactly up their alley.
* You might be able to build atop their previous work, tools, workflows, etc. giving you a jump start that will in the end result in you being able to do more and better research.

A potential downside is that you'll have a bit less freedom, but given that you're here asking about a topic, I'm guessing you don't have very strong feelings about what you want to research anyway.

If you *are* selecting a topic on your own, figure out what's needed from a master's thesis in your program. In mine, we were supposed to do a relatively small but original research project that could potentially be published in a small (but serious) conference or journal. To find a topic like that, you can look at *recent* papers that interest you, especially if you think you could reproduce them, and then see if you could carry out any of the "future work" they mention. If you come up with an idea of your own (like "build a chess engine in a certain way"), you'll probably have to make sure that this has indeed not been done before, and there should be compelling reasons (ideally published in good journals/conferences) to think that this is a good approach, so that if it turns out not to work as well as the state-of-the-art (which is quite likely) you can still defend the interestingness of your work.

As for concrete topics: if you like game-playing AI, you could look into "general game playing" or "general video game playing". However, as I said, it's probably better to look at the professors in your department and see what kind of work they're doing, and then try to work with the one that most closely matches your personal interests. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-05 22:16:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> I didn't watch the video, but I would think it's possible to write Flappy Bird AI or something similar in 6 months by a novice programmer. You should probably try to use existing software for the game and perhaps also for the AI.

However, it's notoriously difficult to estimate the duration of software development, and that goes double when AI is involved. So what I think you should do is pick a project that looks way too easy to you, but that can be expanded. If it takes longer than you thought, it hopefully still takes less than 6 months. If it takes as long (or shorter) than you thought, you'll have time left to expand the project, make it more complex, ask more questions, implement (and compare) more algorithms, etc.

Some random pointers:

* https://gym.openai.com/
* https://github.com/mgbellemare/Arcade-Learning-Environment
* http://www.gvgai.net/ </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-03 17:20:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> Good post! I think you will know best if you prefer learning from online courses or books. I personally think it's important to not just passively consume content. For both books and online courses, that means selecting ones that have exercises, and then actually doing them.

Doing such exercises likely requires skills in programming and math. For this reason, I recommend you start by learning programming in Python. You don't have to be a super expert, but just enough that if someone describes an algorithm to you, you could implement it. Then when you learn about an algorithm from a course or book, program it yourself (whether there's an exercise that says you should or not). This is both good practice in programming and it will help you better understand the (AI) algorithm.

I assume you took some math in high school, so you should have some of the basics. It's likely not enough, but it might be enough to just dive into AI and learn the math as you go (i.e. go learn about a particular kind of math when you encounter it during your AI education). Of course, if you prefer you can also just start by taking whole courses on statistics, calculus and linear algebra, but that's up to you.

Many people who want to get into AI are really just interested in ML. This does not appear to be the case for you, so while ML is very important, I recommend that you don't narrow your focus to just that. I would probably start by taking a free Intro to AI course, and then branching out to subfields of AI that interest you (which likely includes ML). The online courses I took are probably better introductions than a book like AI: A Modern Approach, so I would start by taking those, and then dive into the books to go in more depth later. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-03 14:51:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> The paper is [here](https://www.nature.com/articles/s41586-019-1799-6.epdf?referrer_access_token=WbvHGBJrYbHBvKiJaNQapdRgN0jAjWel9jnR3ZoTv0M5zwPVx5jT4z_z-YkUZTBTbM27UWphyoF6vHoR667kKgqCi8GNWj2oxgaEK9QGM_L12Qj2XG2htlhQgMs-Jn1mKBUd25SpdBhOjYDX_GFMLTK44y2K7v497ZLTwD8IKoL8lTzPLsWB4xXOKg4LogkW0Es0jjHxbQhkY9oYXszziIjRwreH15wQ8EIioG0jIqs%3D&tracking_referrer=www.bbc.co.uk). Most info about the data sets appears to be in the appendix starting on page 7. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-03 14:27:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm sure there are a lot of reasons to dislike Dominic Cummings, and to fear what he will do. However, I don't think anything nefarious is implied by him wanting to hire AI/ML experts.

The main thing I'm taking away from *this post*, which doesn't really mention his political leanings or ethics, is that he wants to be more effective. He wants to understand and use the science and technology that he believes will be important in the current age. He wants to have a more accurate model of the world, and to predict what will happen and/or the outcomes of his own actions/policies. In my opinion, these are all good things for a government official to want. This should already be the standard, but apparently it isn't. *That* is probably the worst thing implied by this post.

Some of the papers are basically *just* about efficacy. I don't think anything in particular is implied by wanting to know about causation, rationality, or prediction/forecasting. Other papers are a bit more specific, and one could imagine that he might want to use knowledge from papers like "Complex Contagion" to manage public perceptions (or just understand how they're shaped), and I guess he might also be concerned about interstate wars? But this list is also presented as a illustrative sample, amidst a much larger post in which he shows interest in many areas, so I'm not sure they should really be taken at face value.

Of course, if you disagree with Cummings' ethics or politics, then you probably do not want him to be more effective, and that's a good reason to not want to work for him. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-03 12:56:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on this subreddit's wiki that might be helpful. You might especially be interested in Google's TensorFlow course on Udacity.

Whether you're concerned about privacy or not, you should probably learn the basics first. When you've done that, you can apply your skills where you want. I suppose you'll probably want to avoid certain application areas like facial recognition and targeted ads. You could probably also do research into anonimization of data sets or something like differential privacy. This might be interesting: https://towardsdatascience.com/perfectly-privacy-preserving-ai-c14698f322f5 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-03 12:31:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not a neuroscientist, so I'm no expert on this.

You might be interested in DeepMind's paper on [Neuroscience-Inspired AI](https://deepmind.com/research/publications/neuroscience-inspired-artificial-intelligence) where they argue that better understanding biological brains could play a vital role in building intelligent machines. I think that maybe it *could*, but that even when we look at the subfield on artificial neural networks (ANNs), most advances in ANNs don't come from neuroscience (NS), and most advances in NS don't result in better ANNs. If you want to improve ANNs, you're probably better off studying mathematics.

> The fact that we can control robotic arm/computer mouse with only our thought speaks a lot about our understanding of neurobiology.

Not really. All we need to know is that the brain can show different activity patterns when we do or think of different things, and that it can learn. No other details are necessary, because we're just going to use generic machine learning methods to analyze that activity. The main reasons it works are that the tasks are quite well defined and the human subject is also learning at the same time.

> Even if we know approximate locations of neural signals in our brain, it can also help with development of AI.

How? What use is it to know that when humans experience emotions, activity starts in the middle of the brain, then spreads a bit up and to the back and down again, then up, to the front and back down to end up in the same place? These locations are entirely specific to the human brain.

It's more useful if you know what each "location" does, and we do have some knowledge of that, but my impression is that this is still very incomplete. Furthermore, ANNs don't tend to have structures like the hypocampus, fornix, cingulum, etc. so you'd have to incorporate something like those to make use of your knowledge about emotions and the Papez circuit (limbic system). But that's problematic, because 1) we probably don't know enough about these systems, 2) even if we did, we probably don't know enough about the systems they're connected to and *everything* in the brain is connected, and 3) it's quite likely that this is going to be much less efficient for the task we want to use the ANN for (in the same sense that spiking neural networks are more biologically accurate, but work worse in practice).

> One example of this is ‘Papez Circuit’ which explains nature of our emotions scientifically

Does it? I could be wrong, but my impression is that it basically just makes claims about what brain areas are involved in emotions. I don't mean to say it's unscientific, not useful or wrong (although Wikipedia says "[s]ome of the structures that Papez originally described such as the hippocampus now appear to have little to do with emotional behavior"), but that this doesn't really *explain* the nature of our emotions. I think a lot of neuroscientific knowledge is this way, which is too bad, because a real explanation of the nature of emotions might actually be implementable in AI.

> in that regard AI is surely lacking

Is it? Maybe it is... We don't really have AI that's humanlike in general. But for the AI we do have, I wonder what you would mean by "emotions" and why you'd want AI to have them. The most useful thing seems to be the ability to *recognize* emotions in humans and respond to them appropriately, but this doesn't require the AI to have emotions itself or to have neuroscientific knowledge of how emotions work, because this would more likely be based on things like facial expressions and/or spoken communication.

> If AI can monitor specific human brain activity while monitoring his/her external activity simultaneously wouldn’t it be most effective?

Yes, I think so. At least that should allow it to learn correlations between the monitored brain activity and the observed behaviors. This is already happening in some applications, like the ones you mentioned.

---

I'm being quite negative here about the contributions of neuroscience to AI. I don't think NS has absolutely nothing to offer. Just that it's not as obvious or easy as you might think. It's one possible path towards artificial general intelligence; my impression is just that it's not the most promising. And for narrow AI, I think the contributions are probably even fewer and farther between.

The paper I linked is more positive, and written by people who know much more about NS and the connection to AI than I do. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-03 04:47:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's been a long time since I've worked with EEG, so take this with a grain of salt, but my first intuition is that the signal might simply not be in the data. EEG has good temporal resolution, but crappy spatial resolution, and it sounds like your classes just differ in tiny spatial details in (roughly) the same location of the brain. Do you have any indication that what you're trying to do is possible (with this data)?

Imagine if instead of measuring O1 and O2 you'd just have (very noisy) measurements of the amount of light that reaches each retina. You could probably not tell the difference between the digits. Of course when you're measuring O1/O2, a bit of processing might have occurred, but you're still looking at quite low level features with only very general and very noisy measurements. It could actually be better to (also) use other electrodes in the hope that more differentiation occurs at later stages of processing.

If this can be done at all, I think a deep neural network should be able to do the trick, although I couldn't tell you what architecture and preprocessing would work best. Probably something convolutional. You could smoke test your ML setup with a simple ML task and data set to make sure you're using your ML tools etc. correctly.

If you have a lot of data, you could also simplify the classification task by e.g. only using data from a single person and perhaps only from the most different two digits (I'm thinking 1s and 8s). Beware of overfitting though.

You could also try visually inspecting some of your data. For instance, compare the signals for ten 1s and ten 8s for a single test subject (maybe do this for multiple test subjects). Do the 1s seem similar to other 1s and different to other 8s? Do different preprocessing methods (e.g. smoothing, alignment, hi/lo-pass filters, wavelet transform, etc.) help with this? If you can tell the difference visually, you should be able to train a deep network to do the same (but if you can't, it doesn't necessarily mean much, because a NN might be better at it than you). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-03 01:37:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think that for most jobs, you don't need this kind of certifications. It's funny you mention CISCO, because they actually have a lot of certifications themselves. This might mean they care more about such things than the average workplace, but I also wonder if there isn't some kind of CISCO ML certification you could get (I haven't checked) which they'd presumably care more about.

I think you're right that you should get some experience under your belt, but I think most employers aren't going to view getting an online diploma as "experience". It's better to work in an actual job, or failing that, have a portfolio on GitHub or Kaggle or something. Certifications, diplomas, etc. aren't going to hurt your chances, but I don't think they'll help much either. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-02 22:38:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you should learn the basics of AI, ML and data science first. We have [some pointers](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) for that on this subreddit's wiki. I would not go out and get expensive certifications like this on my own. I think the typical scenario is that you're already working as a data scientist, you have to use Azure for something, and then you (or more likely your boss) decide(s) you should get the certification for that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2020-01-02 22:25:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> You have a lot of questions about AI, and I suspect the best way to answer them is to dive a bit deeper into the subject yourself. Check out the resources we have for [Getting Started with AI](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on this subreddit's wiki.

I can't answer all of these questions now, but while "whole brain emulation" and computational neuroscience projects exist, most of the AI field is indeed about the approach you seem to advocate: rather than emulating the brain neuron-by-neuron, we're trying to program AI in different ways, often so that it can learn things from experience such as images, videos or text. The current state of the field is that we are limited in what we can program+teach our AI systems to do, and we're constantly trying to push those limits. A lot of modern AI uses deep neural networks, which are somewhat inspired by the brain, and some researchers believe we can indeed learn much from neuroscience and/or cognitive science in our pursuit of AI, while others prefer to improve our AI systems in other ways.

There are many different kinds of AI systems, ranging from planners/schedulers, to pattern recognizers, to expert systems (that can e.g. diagnose diseases), to robot controllers, to chatbots, and much more. Since we don't seem to be very close to human-level *general* intelligence, we typically advise against anthropomorphizing these systems (i.e. treating them or thinking of them as human). But taking that into account, you could probably say that one chatbot can have a different (perceived) "personality" than another, and it's certainly possible to give an AI different "preferences" (i.e. things it optimizes for), which could potentially include preferring to "help" (or listen to, or follow the commands of) one human over another human (if it can tell them apart). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-31 04:08:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> **Warning**: I probably didn't get the right answer because calculating the area of an ellipsoid is hard and I used an approximation. It's also very likely I made mistakes. I hope this can nevertheless be useful.

---

There seem to be basically three steps:

1. How many unique triangles are there with a perimeter less than or equal to 50 million, such that a² + b² - c² = 1? Call this number S.
2. On an ellipsoid with surface area S and axes lengths n, 2n and 3n, what is n?
3. Ceil (i.e. round up) n, and take the square².


#Step 1
There are probably better tricks for step 1, but this is my approach:

I would start by rewriting the equation as a² + b² = c² + 1. Then I notice that c should be larger than or equal to (the largest of) a and b. Also, to make the triangles unique, I'll say that a must not be larger than b, so we get 1 <= a <= b <= c. This means that b equals c minus some number y and a equals c minus some (probably larger) number x: (c-x)² + (c-y)² = c² + 1. This can be rewritten as the quadratic equation
c² + -2(x+y)c + x²+y²-1 = 0, which we can (sort of) solve with the [quadratic formula](https://en.wikipedia.org/wiki/Quadratic_formula). After some rewriting, we find that c = (x+y) + sqrt(2xy + 1). Since c, x and y are whole numbers, sqrt(2xy + 1) must be too.

Now I remember that odd numbers can be written as 2k+1 where k is some integer (even numbers can be written as 2k). Since xy is an integer, I know 2xy+1 must be an odd number, and the square root of that number must be too, so I can write 2k+1 = sqrt(2xy + 1) and after some rewriting obtain 2k(k+1) = xy. Now we have to divide the factors of 2k(k+1), which are 1, 2, k and k+1 over x and y, which gives us some different scenarios, but I'm going to start with the scenario where y=0.

###Scenario 1: y=0
This basically means that b=c. You can probably tell from the a² + b² = c² + 1 equation that this must mean that a=1. How many triangles like this can we make? Well, the perimeter is a+b+c which must not exceed 50 million. Since a=1 and b=c, this means 1+2c <= 50M, so the largest allowable value for c is 24,999,999.

###Scenario 2: y=1
Since xy = 2k(k+1), this means that if y=1 then x=2k(k+1). This will be valid for all values of k. Filling all the parameters, we can find that the perimeter is 4k²+10k+5 which must not be larger than 50 million. The largest value of k that fits can again be found with the quadratic formula, and is 3534.

###Scenario 3: y=2
Using the same logic, we find that x=k(k+1). Filling all the parameters, we can find that the perimeter is 2k²+8k+7 which must not be larger than 50 million. The largest value of k that fits can again be found with the quadratic formula, and is 4998.

###Scenario 4: y=k
Using the same logic, we find that x=2(k+1). Filling all the parameters, we can find that the perimeter is 12k+7 which must not be larger than 50 million. The largest value of k that fits is (50M-7)/12 = 4166666. So far we haven't had to worry about double-counting because the difference between b and c was always different. However, we have already seen the triangles with x=4 and y=1, and x=4 and y=2, so we need to subtract those two: 4166666 - 2 = 4166664. (I know these are the only double-counts, because we've never seen higher values of y before.)


###Scenario 5: y=k+1
Using the same logic, we find that x=2k. Filling all the parameters, we can find that the perimeter is 12k+5 which must not be larger than 50 million. The largest value of k that fits is (50M-5)/12 = 4166666. This time we're only double-counting x=2 and y=2 (for higher values of y, the difference with x has never before been so small), so we need to subtract that one: 4166666 - 1 = 4166665.

###Other scenarios
There really aren't any. There are more ways to factor 2k(k+1), but then y would be larger than x, which we don't allow.

##Adding everything together
So when we add everything together, we get S = 24999999 + 3534 + 4998 + 4166664 + 4166665 = 33341860 triangles.


#Step 2
Unfortunately, calculating the surface area of an ellipsoid is [quite complex to calculate](https://en.wikipedia.org/wiki/Ellipsoid#Surface_area). I'll just try the approximate formula listed there, using p=1.6075:

S = 4pi \* (((n)^(p)(2n)^(p) + (n)^(p)(3n)^(p) + (2n)^(p)(3n)^(p))/3)^(1/p)

 = 4pi \* ((2^(p)(n)^(2p) + 3^(p)(n)^(2p) + 6^(p)(n)^(2p))/3)^(1/p)

 = 4pi \* (((2^(p)+3^(p)+6^(p))n^(2p))/3)^(1/p)

 = 4pi \* ((2^(p)+3^(p)+6^(p))/3)^(1/p) * (n^(2p))^(1/p)

 = 4pi \* ((2^(p)+3^(p)+6^(p))/3)^(1/p) * n^(2)

-->

n = sqrt(S / 4pi \* ((2^(p)+3^(p)+6^(p))/3)^(1/p)) = 3215.57522

Unfortunately, Wikipedia says to expect an 1.061% error with this approximation, so we're probably going to be off by as much as 32.

#Step 3
If we take the value of the previous step (which is likely slightly wrong), then ceil it (3216) and square it, we get a final result of 10,342,656. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-31 02:35:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> You want me to comment this on YouTube? I'm sorry, but I'm not going to do that, because that would link my Reddit account to my (not anonymous) YouTube account and reveal my identity. I'm sure you can just show this comment to your teacher on Reddit. If not, you can comment on the video yourself with a link to here (or copy-paste the comment's text). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-31 00:03:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> I have thought about similar questions before, and think it's interesting. But for this particular scenario, there are a lot of practical issues that are not directly related to AI/AGI that spring to mind first. First of all, I don't think a public-facing demo is going to result in many donations. If the AI can't do anything a human can't do, then I think the general assumption will just be that a human is in fact doing it and you're just a charlatan. But if you somehow manage to convince the world that you do have baby AGI, I think the likely next step is that it's going to be stolen from you. If you (and we all) are lucky, it will just be confiscated by a relatively benevolent powerful government and taken offline for the time being, hopefully before others managed to hack and/or reverse engineer it, which I expect to set off a lot of political instability to put it mildly. (Actually, just the knowledge that this exists might do the same.) Also, if you do make your system public, I will probably just wait for a more knowledgeable person or institute that I trust to assess it, and read what they have to say.

So I think it's probably better to keep it private. Okay, let's say that you're approaching me with it. My first question would be "why me"? If you're asking for my money while you have a baby AGI, that would be suspicious as hell. If you really have baby AGI, I don't have enough money to benefit you, especially if you can just let it grow up and rake in the trillions. So if you're asking me, that raises my estimate that you *don't* have this baby AGI and you're just swindling people out of their money. If you were coming to me for advice on how to make it safe, I'd be more receptive, although I would still redirect you to a more properly suited institute like MIRI or FHI.

But okay, let's skip the boring practicalities: what would convince me it's actually 1-5 year old AGI if I had to assess it myself and I wasn't concerned about AI Safety? I guess I would really like to understand the theory behind it, but even if I did, I would probably not rate it above OpenCog or NARS, because those theories sound pretty convincing to me too. I'm not sure what a demo of an AGI with the intelligence of a 1-year-old would look like, especially if it's not physical, as you say. But I think there are some things I could teach a 5yo child that I would like to try with this AGI demo. If it was an adult-level AGI, I would probably be convinced by something similar to a Turing test, if I was convinced there was no cheating.

If you're actually in need of funding, a more realistic scenario to me is that you don't have 1-5yo AGI yet. This is, I think, the stage that a lot of AGI-aspiring projects are in. They typically get funding based on a combination of credentials, useful by-products and ideas, with credentials probably being more important. For instance, OpenAI does not seem to have published many ideas on how to build AGI (although we don't know what they told their investors in private), but companies (especially Microsoft) invested a shit-ton of money in them based (presumably) on the fact that they employ many very talented AI researchers and have a good track record in (seemingly) related research. If you're taking a route to AGI that produces useful technology along the way, then you can probably also get funding for that. The hardest is probably to get it based on ideas, which is (mostly) what academic researchers are doing, but even then it's also important to have a demonstrated track record and some ideas about what the project will deliver in the shorter term. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-30 22:51:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> I only watched the first video on Decision Tree Introduction & Construction. I actually recently taught something very similar to second year undergrads taking a datamining course. I think it's pretty good at covering the basics of decision trees, but I do have some pointers.

First of all, I would prefer if you talked a bit slower and focus on good articulation. The fast speech combined with your accent (which I know you can't help) and the fact that we can't see you, occasionally made me miss something you said. Plus, I think people encountering this for the first time will probably need a bit more time to process these ideas. Those who don't can always speed up the video up, but I find slowing it down doesn't work as well. For the record, I could understand most of the video just fine, and this is coming from someone who prefers to watch English movies with (English) subtitles, so this might be a subjective issue.

Second, I would like to see more visualizations. Start by numbering your slides, so they're easier to talk about (not just for me, but for your students). I would start with the decision tree on your fourth(?) slide to immediately *concretely* show students what the lecture is about (not just talk about it in the abstract). And when you talk about the decision tree, either point to the parts you're talking about or "draw" on the slide. I would probably put a decision tree on (almost) every slide for this reason.

There are some sloppy bits. For instance, the 4th bullet point on the first non-title slide is skipped. This often happens to me in real-life lecturing, but you're making a video, so you could fix that. The same goes for the bit around 4:56: just go back to that slide off-screen and then please make it full-screen again (and clearly highlight or point at the rows when you're counting them). The equation for entropy also doesn't really look good: without the explanation that "p+" and "p-" are to be read as single symbols, it actually says something completely different. Ideally you'd remake this equation in LaTeX or some equation editor, but otherwise at least replace "p+" and "p-" with single symbols (e.g. **p**ositive and **n**egative or **y**es and **n**o). I would similarly replace "9+ and 5- examples" with "9y and 5n examples" or something. (Some) students are easy to confuse.

On that note, in my own class, I had to dedicate quite a lot of time to explaining the concepts of entropy and information gain. You go over it very quickly, so I'm afraid you'll lose a lot of students if you don't explain the underlying intuitions. I would also provide more examples. Show two tiny decision trees with different root nodes and calculate the entropy at each branch, and then the information gain. This should also make it clearer what this is all used for (to construct the tree in a certain way).

I'm not really sure about the inductive bias part. First of all, it seems a bit odd to talk about it without first discussing the ID3 algorithm. Secondly, it makes me wonder a bit about the prerequisites for this. I thought this was the first video, but in the beginning it's mentioned that trees were already covered. Did you also already cover classification (and regression) and machine learning as a search through hypothesis space for the "best" model? I think this would help to explain the concept of inductive bias and the role of search and language bias. I also don't really understand why Occam's razor is included here, because as you've explained the ID3 algorithm it doesn't seem to affect it much (in fact, ID3 may come up with a suboptimal tree in this regard). I also don't think you've talked about candidate elimination before this point.

---

This was a long post with quite a few pointers. I hope you don't interpret this as me thinking your video is bad. I would be quite happy if my own students could produce this. But I hope you can use this advice to improve future videos. A lot of it was quite specific to this particular one, but I hope you can find lessons that apply more generally: talk slower, more visualization, keep it concrete and use examples, use the fact that you're not presenting "live", and try to think very carefully about what your students already know and what they don't (and err on the side of repeating stuff, even if you think they should already know it, because people are forgetful).

Good luck with the rest of this project! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-30 21:54:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> These are huge organizations and you could do a lot that would help them. One overlooked aspect is that aside from their "main tasks" they're huge bureaucracies, so anything that helps with an office's productivity can probably also help the police and military. The police also has to interact with civilians a lot, and this communication can be enhanced like it could be for other companies/agencies. For instance, by making a chatbot to give information or providing "smart" guidance when filling out forms (e.g. for reporting a crime or applying for a permit to demonstrate or whatever). These forms can then also be automatically analyzed, prioritized, and sent to the right person to deal with them.

As some people have pointed out, there are many ethical issues with AI usage by the police and military. In my experience, they are very aware of this and it's a big issue for them. For instance, the military really wants to retain "meaningful human control" or "appropriate human judgement" and is very interested in questions about responsibility. DARPA has been interested in "explainable AI" (XAI) for quite a long time. This also applies to the police, who need to explain a lot of their decisions to a judge: you can't just arrest someone because "the AI said so". They're also quite concerned with issues of (racial) bias and fairness, privacy and safety.

So I would recommend that you do something with any of these issues, because that might simultaneously help the police/military *and* the people who are concerned about those organizations using AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-30 21:41:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> I've done a research project into ethical use of AI by the police in my country (not the US). I've heard about this kind of thing both in an interview I did with one company that provided predictive policing tools as in the writings about the police's own internally developed tool.

They are aware of this particular issue, and they incorporate this awareness into the use and development of these tools. Whether they learned this "the hard way" I don't know (I suspect not, because the example mentioned here is quite obvious and blatant). However, similar issues still probably exist in less blatant forms.

To fix this problem, you need good data. You probably want to incorporate the data into your system of what you found on "stop-and-frisk day on Times Square", but then you also want to incorporate information that it was stop-and-frisk day on Times Square to correct for the increased frequency of detected crimes. Even in this simple example, that's not trivial, because how much should you correct? If you put 10x more police than normal, are they 10x more efficient? Do they stop-and-frisk a totally random sample of people (or even a similarly nonrandom sample compared to "normal" days)? Do certain kinds of people avoid Times Square on such a day? Etc.

Of course, if it's really a special event, you could maybe just exclude the data (although that'd be a waste, plus this might also affect other data from e.g. close-ish to Times Square or subsequent days). But that's not really an option if you just have varying amounts of police presence on different days in different areas. Again, you can try to correct for that, but again, that's difficult. Furthermore, this requires quite accurate data that you probably won't have. You might have data like "we sent 5 police officers to patrol neighborhood X", but you won't know at what times they were taking a bathroom/donut break, when they walked over the border to neighborhood Y, where they all were at what point in time, etc. You might have some of that data (probably in a very noisy form), but a lot of it will simply not be gathered (because it's too hard or e.g. to protect the cops' privacy). And even if you have the data, it's not always clear how to use it to get better results.

I don't think this dooms all predictive policing forever, but I certainly do agree with /r/MasterGrid's final sentence. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-30 21:13:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> For (unsupervised) clustering the clusters you end up with are based on your choices as a developer for e.g. the clustering algorithm, its parameters (such as possibly the number of clusters) and very importantly the "similarity measure" (or conversely, the "distance measure").

It should in theory be possible to choose these such that you end up with clusters that correspond to the classes produced by a (supervised) classification algorithm, but in practice it seems quite unlikely that this will happen. And even if it does, you'd expect the (unlabeled) "cheetah" cluster to be quite close/similar to the "leopard" cluster. But more likely (I think), they will not be separated entirely correctly. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-30 21:03:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> Check out the resources linked on /r/artificial's wiki section on [Getting Started with AGI](https://www.reddit.com/r/artificial/wiki/getting-started). The articles by Goertzel and Wang both provide (incomplete) lists of projects. [Kotseruba & Tsotsos (2016)](https://arxiv.org/abs/1610.08602) have a more extensive survey of cognitive architectures, but they're not all aiming at AGI. It takes a bit more effort, but you could also look at the contributions to the annual AGI conference and the AGI journal.

These mostly don't include the efforts by DeepMind, OpenAI, GoodAI, Facebook, etc., but I also think they haven't really published comprehensive models/theories of AGI yet. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-26 10:37:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> You could look into web interface test automation tools or possibly some form filling tools (ideally ones you can configure or change the code of). They are already capable of automatically navigating a web interface, so this should take some work out of your hands (and/or give you an example of where to start if they're open source). See e.g. here: https://medium.com/@briananderson2209/best-automation-testing-tools-for-2018-top-10-reviews-8a4a19f664d2

If you're looking for something more elementary, Udacity's (former?) CS101 course taught a bit of Python by letting students program their own web crawler and search engine. This might also be useful for you as a starting point: http://www.cs.virginia.edu/~evans/cs101/ </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-20 19:49:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> Right now, we are unable to create artificial general intelligence. It's unclear how long it will take, but most experts think "at least a few decades". It would indeed be a bad idea to create unsafe AGI, but I'm pretty sure endowing an AGI with "real" (human) emotions would (edit: NOT) make it safe. /r/ControlProblem specifically focuses on issues of unsafe AGI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-20 19:41:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> Not interactive, but AIMA has [Python Code](http://aima.cs.berkeley.edu/python/readme.html).

I'm not sure anymore if it uses Python, but Udacity offers a free [Intro to AI course](https://www.udacity.com/course/intro-to-artificial-intelligence--cs271) by Sebastian Thrun and AIMA co-author Peter Norvig.

Udacity also has a free [AI course by Georgia Tech](https://www.udacity.com/course/artificial-intelligence--ud954) that definitely uses Python.

You can also check out /r/artificial's wiki for [Getting Started with AI](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-20 19:33:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> Yes, I can't stop people from "forming an opinion". I also don't censor them when they post those opinions, even if I consider them uninformed, stupid or otherwise disagreeable. If I have the time and patience, I'll explain why I think they're wrong (as I did with iWantPankcakes here), or otherwise I'll just ignore it. Personal attacks are never acceptable. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-20 19:26:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> You can use outline.com to bypass the paywall and read all 26 paragraphs of the WaPo article: https://outline.com/CNF42M

/u/iWantPankcakes *is* correct that the article annoyingly doesn't link to the study, but I'm fairly sure they're referring to [this one](https://www.nist.gov/publications/face-recognition-vendor-test-part-3-demographic-effects). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-20 19:12:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think this is misrepresenting what is said. Nobody here said "AI is racist". What they're saying is that there's a racial bias in many face-recognition *systems*. This is indeed (mostly) because of the data they're based on. What is your point? That the used learning algorithms could also be used to create less/differently biased systems? Isn't that all the more reason to call out unnecessary or egregious bias in systems we're actually using?

Also, the article has 26 paragraphs, not two. I'm thinking your paywall removal method isn't working very well... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-20 19:01:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> Personal attacks are not allowed. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-19 01:21:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> **tl;dr**: How can I (learn to) be more concise / less verbose?

I tend to require a lot of words to communicate and convey the ideas I have (check my post history for examples). This leads to long posts online (I'm sure this one will be an example), and unnecessarily long and inefficient (and annoying?) conversations in real life. I think this comes from an almost pathological urge to not speak falsehoods and (less so) to omit relevant information (although I might occasionally lie by omission), sometimes coupled with a bit of modesty, and--perhaps ironically--out of a desire for efficiency, to pre-empt any confusion or questions that might arise.

This leads me to pepper my communications with disclaimers, words like "tend", "appear", "maybe", "probably", "mostly", "I think", etc. and add a lot of parentheticals and asides. If you're still reading, I suspect you might already be tired of it. I do it in spoken conversations too: I'll often start a disclaimer sentence in the middle of another sentence that was already meant to disclaim the sentence I originally intended to utter, and the "weasel words" are a tic I can't seem to stop.

Although I'd say it's not just a tic, because I think these words do make my utterances more truthful, which is in itself not a bad thing. I'd hate to compromise on that, but at the same time I'd also like to be more interesting, impactful and respectful of people's time.

In written text, I could certainly spend some time to edit down what I wrote, and e.g. remove all the "probably"s and "tend to"s but even if this results in a 25% reduction, my posts would still be long. (Plus I'd find it super painful, because I'd feel like I'm turning my text into lies.) I didn't edit this post, because I hope that helps illustrate my problem. Of course, editing is not even an option in spoken communication (although I'll often correct my previous statements in a conversation).

Does anyone recognize this, and have advice for me?

Thanks!

---

Edit: Thanks for all the advice! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-18 12:25:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> I was a little disappointed that Yampolskiy just mentioned the universities and didn't say anything else, but I see that the links go to the actual labs/people working on AI safety, which makes it easier to figure out what they're actually doing.

Stuart Russell also gave a mostly US-centric answer to a [similar question](https://www.reddit.com/r/books/comments/ebh0qg/i_am_stuart_russell_the_coauthor_of_the_textbook/fb4s0qt/) about this in his recent [AMA on /r/books](https://www.reddit.com/r/books/comments/ebh0qg/i_am_stuart_russell_the_coauthor_of_the_textbook/):

> 2) In addition to Berkeley, there are groups at Stanford (Sadigh, Ermon, Finn, and HAI generally); MIT (Tenenbaum); Princeton (Griffiths, Fisac); Michigan (Singh, Wellman); Cornell (Selman, Halpern). Oxford (FHI) and Cambridge (CFI) are also possibilities, via the CS or Philosophy grad programs. Oxford is setting up a huge new center for AI ethics. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-18 10:47:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> > What if you code an airplane system and the AI malfunctions?

Then you're essentially just as responsible as when another part of the airplane malfunctions. This is not really new: if you build something, and you make a mistake, which makes it malfunction and cause damage, then you are responsible.

Of course, if the product is misused or not properly maintained, that's the user's responsibility.

And in many cases responsibility can also be shared *between people*.

> But you didn't really program it the AI trained and programmed itself?

This is quite frankly nonsense. Even if you have an AI system that supposedly "self-trains", it is a human who made that system (or who made the system that made that system), a human who executed it, a human who evaluated and judged it fit for purpose, and a human who is still in some sense controlling it. In most cases "training" is basically just a different programming paradigm and it shouldn't absolve the programmer of any responsibility.

I'm not going to say that AI (especially black box systems) can't complicate questions of responsibility. But we should be extremely careful to let people offload their responsibility on a machine that can't bear it, and in a lot of cases the rules we use in non-AI spaces work just fine. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-17 14:45:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's a complicated question that probably depends a lot on the situation.

If you build train scheduling software, and then it's also used by Nazi Germany to bring people to concentration camps, you're probably not responsible. If you build train scheduling software *for* the nazis to deport people to concentration camps, then you are (although you might be absolved if they threaten your life).

I think that generally speaking people have the moral responsibility to consider the likely good and bad outcomes of their actions/work. Of course, looking in the future can be difficult, accidents can happen, and if a tool/application is used maliciously the primary responsibility is with the malicious user, but that doesn't absolve us completely.
And AI researchers/developers generally don't have the excuse that they'd starve in the street if they didn't do unethical work, because they could probably find gainful employment elsewhere pretty easily.

OpenAI did a [good job](https://openai.com/blog/better-language-models/) on this with GPT-2, but it was unfortunately not really appreciated by most of the industry. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-17 14:26:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> I know, right? I [x-posted](https://www.reddit.com/r/artificial/comments/ebiydh/i_am_stuart_russell_the_coauthor_of_the_textbook/) this to /r/artificial, immediately put up a sticky comment about this (because I anticipated some people would get it wrong), replied to the first person who asked a question in the /r/artificial thread anyway, and then still four more people posted in the wrong thread...

/rant </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-17 01:46:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but this was already posted [here](https://www.reddit.com/r/artificial/comments/ean94q/the_effort_in_this_video_is_insane_a_masterpiece/) earlier. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-16 20:58:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you want Dr. Russell to see and answer your question you need to post it in the [linked thread on /r/books](https://www.reddit.com/r/books/comments/ebh0qg/i_am_stuart_russell_the_coauthor_of_the_textbook/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-16 20:56:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> All of these solutions could work to produce a functional AI I think, although I don't know what strength it'd have.

I think minimax (or negamax) with alpha-beta pruning could work, but its strength will depend on how good your heuristic evaluation function is. You could try simple things like counting the number of won boards, perhaps adjusting for position (e.g. the middle board is worth more), perhaps adding points if they're in the same row and the other square is still available, etc. You can also do similar things for the mini-boards, and add that to the score as well. This won't be perfect, but if it was, then you didn't have to bother with any of these tree search algorithms at all. Just try to do your best and get creative. You can test one version against others to see what works and what doesn't.

MCTS will also work fine. Actually, that's basically what AlphaGo/AlphaZero is, and Go is a much larger game. Of course, AlphaGo used learned heuristics and had some changes from the simplest possible implementation of MCTS. Nevertheless, I think this could work and it has the advantage that you don't have to come up with a heuristic evaluation function (although if you did, there are ways to use that). [This](https://towardsdatascience.com/monte-carlo-tree-search-158a917a8baa) seems like an okay explanation to me (or maybe you like [video](https://www.youtube.com/watch?v=lhFXKNyA0QA) better), but I guess you'll have to find your own explanation that works for you.

For the deep Q network, I think it could learn to deal with the fact that you can't play everywhere. But you should definitely encode where it can play somehow and not just show the current board (this also applies to all other methods). In fact, teaching it this should probably be *relatively* easy, since you can just generate an endless stream of data for learning to make legal moves. However, you'd need data to make good moves, which could be gathered from observation (very slowly), or from reinforcement learning, but I suspect this would take a long time. Maybe doing [iterated distillation and amplification](https://www.youtube.com/watch?v=v9M2Ho9I9Qo) with one of the other methods might help. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-16 19:27:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> Post questions in the [linked thread on /r/books](https://www.reddit.com/r/books/comments/ebh0qg/i_am_stuart_russell_the_coauthor_of_the_textbook/). If you post here (on /r/artificial) Dr. Russell won't see your question.

You can discuss Dr. Russell's answers (and his work in general) here if you want. </TEXT>
</WRITING>
<WRITING>
<TITLE> I am Stuart Russell, the co-author of the textbook Artificial Intelligence: A Modern Approach, currently working on how not to destroy the world with AI. Ask Me Anything </TITLE>
<DATE> 2019-12-16 19:25:58 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-16 17:13:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> Have you already programmed the game? Maybe two-player (even against yourself) or random play can already help.

I don't have time to help you, but most game-playing AIs work with some form of tree search: minimax or Monte Carlo Tree Search (MCTS). For minimax you will need to / get to write a board evaluation function that rates how good a non-finished board position is. That's a potential downside, but it may also help you analyze your game more easily. Advanced (General) Game Players nowadays tend to use MCTS (e.g. AlphaGo/Zero). You could also look up open source implementations of this (I think LeelaZero might work).

Good luck with your project! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-16 17:02:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> I agree with the other top-level answers: the direct link is very weak. For virtually all applications, and a lot of AI research, neuroscience is not particularly relevant. For certain kinds of AI/AGI research and neural networks / deep learning research, taking inspiration from neuroscience is possible, but it is not how most progress happens.

*Medical* neuroscience furthermore seems like exactly the wrong specialization: you'd be most interested in how healthy brains work. Medical neuroscience would only be useful insofar as it tells you something about that, which is not where its focus is. However, if you don't know any neuroscience yet, it will likely teach you a bit about that.

I think it probably won't help you much with AI, but I would personally probably take the course out of interest. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-16 16:55:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think [Synced](https://syncedreview.com/) is a good source of AI news. Perhaps you can look a bit through their (recent) items and see if anything catches your interest. Then you can write about it in more depth.

If your essay has to be very technical, you could also e.g. describe how some algorithm works (e.g. a neural network or genetic algorithm). If it's allowed to be non-technical, you could (more easily) write about societal implications of AI (you can also make that more technical, but it might be harder). Or something in-between, like writing about explainable AI or something.

Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-16 16:50:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> > there might be situations where the disease is so rare that he suggests a wrong treatment for it

I was also skeptical of the AIdoctor's performance in edge cases, but when you say "more accurate in diagnosis than any human can be" I wouldn't blame people too much for figuring that this too would probably be okay. I know the technical definition of accuracy, but most people won't *and* we also have to guess if *you* knew it *and* meant it that way. You could have easily meant "the doctor is more accurate in every single case", since language is imprecise.

Furthermore, I found that this sentence affected me quite a bit, and the lack of a sentence for the subsequent barber robot did too. I think getting a haircut is lower stakes than going for a doctor's visit, but my answers don't reflect this, but rather the fact that you mentioned this robot was going to use a razor on me and *unlike with the doctor* you didn't say anything about how good it would be at that. I assume if it's deployed, it won't literally kill everybody who sits down in the barber's chair, but this still affects my answers.

I don't think this invalidates your results or anything, but it's important to keep these things in mind when interpreting your data, and not over-extending your conclusions.

Anyway, good luck with your thesis! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-16 16:24:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> Human Compatible by Stuart Russell

Superintelligence by Nick Bostrom

Weapons of Math Destruction by Cathy O'Neill

A Dangerous Master by Wendell Wallach

Artificial Unintelligence by Meredith Broussard

The AI Economy by Roger Bootle </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-11 21:27:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I'm no historian, but Hinton's backpropagation work in the 80s was already well known before the availability of big data and computation.

Yes, but [as Schmidhuber points out](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html) Hinton did not discover that algorithm (not that he didn't do important work).

Neural networks were popular for a while in the 90s, but their popularity died down until "deep learning" became popular. This is usually dated back to Hinton and students' AlexNet winning an ImageNet competition in 2012, and is credited to the joint work Hinton did with LeCun and Bengio. But before (and after) that, Schmidhuber's lab was also working on deep learning and winning competitions.

> Pardon my ignorance, but what did Schmidhuber do and when?

He did a lot (like an unbelievable amount) and he did a lot of it early (maybe earlier than anyone else, but definitely earlier than it became popular). You can read about it in the link in the comment you replied to, or follow some links in mine, or look at Schmidhuber's home page or Google Scholar page. Probably the most prominent work, for which he actually *is* credited, is on LSTMs (Long Short-Term Memory) in the 90s, which is used *a lot* today in deep learning.

But he also has a lot of work on meta-learning, artificial curiosity, (hierarchical) reinforcement learning, universal AI, algorithmic information theory, (deep) neural networks in general, (arguably) generative adversarial networks, and probably more things. He can often come off as arrogant, but unlike most people, it seems like he can always back it up. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-11 00:13:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> Then I think you should probably stay very far away from AGI, because it's mostly just research oriented, meaning you need tons of education and you probably won't make huge amounts of money unless you actually invent AGI.

Probably the easiest most lucrative part is to become a business intelligence consultant it something. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-10 15:35:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> FYI: You're missing the word "democracy" (or "parliament" or something) after "oldest".

Also, Iceland doesn't just lack a citizens militia. It doesn't even have an army at all. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-10 14:00:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> > we'll approximate our way to it, right?

This is like 99.999% of the difficulty of reaching AGI, and Hutter's work doesn't really provide much guidance for it. And the guidance it does provide may very well lead us astray. For instance, AIXI basically does a brute force search over all futures (of bounded length) in all still-possible environments, using all possible action sequences. Of course, this number is infinite, so even if you can evaluate 10^10^10^10^10 possibilities, you haven't even approximated 1% of AIXI. Of course, in reality, you have to choose which options will be explored (and it will be many fewer than my earlier example) and AIXI provides no guidance for which ones to explore because it's brute force. Similarly, it doesn't really provide much guidance about what representation to use for learning, and how to update your hypotheses in the face of evidence, if you can't keep all possible environments in memory. It has more or less a hard division between learning and planning, because it says that on each time step it will first learn (by discarding all impossible environments) and then plan. But who is to say that such a separation is indeed a good idea? All we know is that it works if you have infinite time and space, but the whole point of intelligence is to work with the insufficient knowledge and resources that you have.

I'm not saying the whole thing is dogshit or anything. I think it's useful as a sort of upper bound of intelligence, and it has resulted in a pretty good intelligence measure called AIQ. And it's good for modelling things about superintelligence, in the same sense that the "spherical chicken" is useful to physics. But for designing an AGI system, I think it doesn't provide much guidance.

> What are your top 3 most-likely-to-reach-AGI approaches, then?

I don't know what's going to work, but I prefer cognitive architectures like OpenCog, NARS and AERA that are making a direct run at tackling the problem of AGI using architectures we can mostly understand (and therefor improve). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-10 03:38:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> The idea of not directly building an adult-level AI, but rather a "child machine" dates back *at least* to Turing's paper [On Computing Machinery and Intelligence](https://www.csee.umbc.edu/courses/471/papers/turing.pdf) (1950) where he also introduced what would come to be known as the Turing test. We might say that the entire field of machine learning is based on the idea of starting with a relatively dumb system that then learns to become better based on its experience interacting with the environment (in the case of reinforcement learning) or a data set (in most other cases). More specifically, [developmental robotics](https://en.wikipedia.org/wiki/Developmental_robotics) (and [Developmental AI](https://projet.liris.cnrs.fr/ideal/mooc/)) are really focused on developing this intelligence from the ground up. They often use humanoid baby/child-like robots like the [iCub](http://www.icub.org/). You should be able to find plenty of papers (and e.g. conferences) if you search for "developmental robotics" (or "epigenetic robotics"), but you could start with Cangelosi's book or one of the surveys by Asada or Lungarella.

It seems to me that most of this research is not explicitly or directly aimed at AGI. However, if you look at Ben Goertzel's AGI research, he actually has similar ideas as well. The robots he's working with (like the (in)famous Sophia) are not actually "child-like", but his philosophy is that if we want AGI to have general intelligence that can relate to us and that we can relate to, then it's going to also need to have similar (learning) experiences to humans, and putting the AI in a human body helps with that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-09 23:28:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Who would you say are the top serious AGI scientists?

Ben Goertzel, Pei Wang, Jürgen Schmidhuber, Joscha Bach, Paul Rosenbloom, ... Basically people who are publishing a lot at [AGI conferences](http://agi-conf.org/) and active in the [AGI Society](http://agi-society.org/).

I think DeepMind, OpenAI and some people/departments at Facebook, Microsoft, Amazon, IBM and possibly some other (Chinese) companies should be taken seriously as well.

> Who would you say seem to look like promising einsteins?

Well, most of the people I mentioned above aren't exactly highly-regarded in mainstream AI circles... But I don't know of any true outsiders who I think have amazing breakthrough ideas.

> Who would you say are considered by some to be serious AGI scientists but really are just crackpots?

I think this is mostly rare, because people tend to be very skeptical of researchers working on AGI, so respect is not given easily. The bar of entry at AGI conferences isn't terribly high, so there are occasionally people there who might fit the crackpot label, but they tend to not come back the next year.

But what I think your categorization is lacking is "serious AGI scientists who (I believe) are wrong". I definitely think Marcus Hutter is a serious scientist (probably one of the most successful AGI scientists actually), but I didn't include him above because I don't think his algorithmic information theory approach is the right one. Nor do I think neural network-based approaches are likely to succeed first, but I don't consider Hinton, LeCun and Bengio crackpots. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-09 21:17:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> A few decades from now, someone will win a Turing Award for this, and /u/siddarth2947 will post here to set us all straight. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-09 21:13:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> Serious AGI scientists tend to make their living pursuing AGI, which usually means they're employed by a university or company specifically aimed AGI (and someone with money is funding that company). You'll probably see a lot of publications, networking and generally talking about AGI. They are relatively easy to identify, although it can be difficult not to confuse them with people who make a living doing something related (like narrow AI/ML) and who are basically just dabbling in AGI.

By (your) definition, distinguishing between "einsteins" and crackpots is difficult. However, when someone comes out of nowhere and (anonymously) posts on Reddit they've "cracked AGI", can't say anything about it because the idea might be stolen, and they're just looking for people to invest time/money in realizing their idea, I'm pretty sure that they're crackpots. Ideally, I'd like someone to tell me their idea, so that it can be evaluated on its merits. Furthermore, it would raise my estimation if someone demonstrates good scholarship and knowledge of the field, if they have an (educational and/or work) background that makes sense, and if they have a track record of having good ideas, being generally sensible and having been successful.

To be honest, I expect there to be such few lone einsteins that it's probably hard to beat an algorithm that just always says "no". There may be outsiders who have decent ideas about AGI, but I think the most sensible ones will basically try to join the rest of the field and become an "AGI Scientist" and not have delusions of grandeur about solving everything in their mother's basement. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-09 16:42:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> Probably this: https://www.blog.google/outreach-initiatives/google-org/2602-uses-ai-social-good-and-what-we-learned-them/ </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-09 16:38:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> We may not be as good at Go as AlphaZero, but we're much more sophisticated in every other way. I agree with you that it's (probably) possible to make something more sophisticated than yourself (or set in motion events that will create it).

However, you're trying to use "the sheer unimaginable complexity of how a single strand of DNA can create such a vastly complex machine as humans" as an argument for why we should believe we're made by AI (which would need to be made by something else to be artificial). This "sheer complexity" is "unimaginable" to us, because we're not sophisticated enough. If our creator AIs created this intentionally and consciously, they'd have to be more sophisticated. If they merely set in motion a simple evolutionary process that ultimately created it, an extra layer of AI (and whoever created it) adds nothing to the idea that we simply evolved without a creator AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-09 15:48:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Just looking at the sheer unimaginable complexity of how a single strand of DNA can create such a vastly complex machine as humans, is it really naive to think so?

This strikes me as the wrong argument for thinking we're created by AI. Because that AI would likely need to be more sophisticated than us, *and* by nature of being "artificial", have other creators of its own. But if you look at something like Bostrom's [simulation argument](https://www.simulation-argument.com/), it's possible to come up with fairly plausible arguments for the idea that we might be created by AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-06 16:32:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> Is your conclusion from this that we don't need, or need to talk about, Ethical AI?

I agree with you that this case is silly. People should stop getting offended over trivial non-issues like getting erroneously classified as gorillas. Likewise, if a ML algorithm predicts that black people are less likely to buy sunscreen, that doesn't mean it's racist or biased or whatever (although it still might be, but that's orthogonal).

But it's also clear that lots of people in the real world harbor various biases, and that those biases can easily slip into ML systems trained on data generated by such people. Furthermore, AI is just a powerful technology, which might have big impacts on the world. So it seems worthwhile to look at ways to prevent bad impacts (either unintentional or malicious) and encourage/improve good ones, and that if decisions are going to be made by, or based on, AI that they are not unethical. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-05 18:49:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thank you very much. This helps a bit. The reason the word "virtually" appears in my previous post is because I did realize the proposed test is sort of a mental challenge, but it's still not really clear to me why this one is particularly good.

I do understand it's an advantage that there's no maximum score, but this is true for a lot of things, e.g. "name the highest number" or "compute more digits of pi". And I think even that would indeed correlate with intelligence, because as Omohundro and Bostrom taught us: basically any task benefits from more intelligence.

I realize your idea is at least more complex. The recursion allows you to make the argument towards the end, although I must admit I'm not sure that's a feature because I have the strong feeling that the conclusion is false, so something must be wrong with the premises. I suspect the problem is that what you're calling intelligence doesn't match my idea of what intelligence is, but it could also perhaps have to do with the idealized nature of the theoretical "machines".

Also, at the risk of contradicting myself here (because I just said any task can benefit from intelligence), it's not clear to me that this task requires much intelligence as we might usually think of it (just like "name the highest number" doesn't). Certainly in your slides, getting to high levels of "intelligence" seems like a very mechanical process that doesn't actually require much intelligence. Couldn't we just write down a program that generates the sequence of programs you outlined, and keeps going, so that it essentially reaches the infinitiest of infinities so that no other system could really do any better? (I'm probably betraying my lack of knowledge of infinities.)

Finally, it's my understanding that this is supposed to AGI's intelligence purely based on their (current) knowledge and not e.g. on their abilities to reason or do anything, right? (This actually seems like a bit of a type/category error to me, but alright.) It sounds to me like it's only measuring a *particular kind* of knowledge though. Suppose we had one AGI X whose knowledge consists pretty much only of your slides, so it could reach an intelligence of ω^ω^ω^ω^... , and one AGI Y with knowledge of pretty much the entirety of the internet, every culture, every planet in our solar system, physics down to the subquark level, etc.; basically everything *except* ordinal notation. Then your measure would say X is infinitely more intelligent than Y, even though I think pretty much anyone else would judge it the other way around. Do you agree with that intuition?

---

I clearly should just read the paper... I will totally understand if you don't want to answer my not-very-well-informed questions and criticisms, although I'd of course like it if you did. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-05 17:15:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think I was following along with this pretty well until we get to the Main Definition slide. I feel like we just went over some kind of problem that has virtually nothing to do with intelligence, and then suddenly he's like "Okay, let's call that intelligence". Why? Does it have anything to do with solving problems, achieving goals, learning, understanding, etc.? If it's just about static knowledge, does it even imply that the system knows anything worthwhile? We've already established that anyone who understood the examples would have an intelligence level that's some high-order super-infinity, which doesn't seem like a useful number to work with.

I don't have time right now to dive into the paper for clarifications, and based on the slides I'm not sure I'm inclined to. But I'd be grateful if someone could perhaps (briefly) try to explain it to me. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-05 03:09:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think Scott wrote [Untitled](https://slatestarcodex.com/2015/01/01/untitled/) (in part?) in response to the treatment of formerly-romanceless Scott Aaronson by feminists/SJWs.

I don't think it explicitly contains anything about the link between IQ/education and sexual success, but in one paragraph where he's basically arguing that nerds (who maybe have high IQ/education?) are not narcissistic, he mentions that narcissists tend to have more sexual partners (ctrl+f narcissist). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-05 01:05:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> #Ajax Gala raises mega amount

The Ajax Gala was held last Monday for the 4th time. A huge amount of money was raised for the Ajax Foundation. Selection players helped serve the diner as usual.

This took the players our of their comfort zone, but they enjoyed it nonetheless. "Those plates are heavier than I thought", smiled defender Kik Pierie. Joël Veltman also walked around the Johan Cruijff ArenA with a smile on his face. "It's a fun evening, for a good cause. Look at the room: it's beautifully packed."

##Record amount

The Ajax Women sold a record amount of raffle/lottery(?) tickets and the auction was a huge success. These two activities, among others, brought the evening's total to the record amount of €547,570. The money is used to help children in the Netherlands and South Africa in the area of sports, social skills and nutrition.

##'Ajax is in their heart.'

Coach Erik ten Hag saw how his players worked up a sweat as waiters at the Ajax Gala. "This group is very involved. With each other, but also with society. Ajax is in their heart. They love to show their connection with the people."

##Hug from Hakim

The trainer also referred back to last week's match against Lille in the UEFA Champions League, where a boy walked onto the field toward Hakim Ziyech. The midfielder answered the young fan with a hug. "That was a very symbolic moment. Hakim embraced that boy like we embrace society." </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-05 00:44:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> Check out the [Getting Started with AGI](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) section on /r/artificial's wiki. It has a lot of links to materials to get started, but you can also use it to find people and institutions you could work with/at as a career.

I'll be honest: machine learning, data science, and other narrow AI subfields are getting to be huge in both industry and academia, and if you pursue them, you could work in many different places. AGI research is only done in a relatively small amount of places. It's getting to be better with e.g. DeepMind and OpenAI entering the scene, but it's still no comparison.

But if you want to pursue AGI as a career, you probably can. The researchers at the annual AGI conference (see the link) can often use help of PhD (or other) students, and you can go from there. You could also just pursue an education in "regular" machine learning or something and apply to a company like DeepMind. Actually, you'll have to learn about AI before you do anything with AGI anyway, so you could always still fall back on a career in "regular"/narrow AI.

If you're also interested in the safety of AGI, [80,000 hours](https://80000hours.org/topic/priority-paths/technical-ai-safety/) has quite a lot of advice on how you could pursue that. Also check out /r/ControlProblem for the links on the sidebar and wiki, from where you could find institutes that employ people to work on this. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 21:08:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> It doesn't seem like Scott is trying to "attract human readers". The only reason the post is still there is because he doesn't want to encourage a Streisand effect or be accused of censorship/hiding. It sounds like he'd prefer that nobody read it, because he would have preferred to take it down. By the way, is the "annoying font" just that everything is italic and slightly smaller? It still seems super easy to read to me...

Making it a high-quality JPEG would certainly make it impossible for most bots to read (it probably wouldn't be too hard to OCR, but I assume most bots aren't doing that). However, I doubt most bots are even doing that. My guess is that the bots are just used for occasionally re-posting pre-selected links in a variety of places. *Possibly* they're making their own selection by e.g. seeing what articles get shared (and maybe highly upvoted) in allied places (like maybe /r/The_Donald). In this case, it makes no difference if the text is a JPEG or not. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 19:01:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> > when Goodfellow came around and discovered (or rediscovered) these ideas and actually implemented them, Schmidhuber trashed him for it.
>
>

Did he trash Goodfellow for rediscovering GANs and implementing them? No, he trashed him for not acknowledging prior work (so essentially for poor scholarship). I don't think anyone ever questioned that Goodfellow's work was (and is) valuable.

Now, I'm sure he could have been nicer about it, but the question here is whether we think there's value in acknowledging prior work. It seems that the entire scientific community (outside of ML?) seems to think that it is. So Schmidhuber (not so gently) encouraging that is probably a good thing.

And what exactly are you calling flag planting? Is it just "doing research"? Was Linnainmaa harming the world by planting his flag in backpropagation? If you can plant your flag in a great idea, I don't see what's so detrimental about that to the research community. Is it that if someone later makes that idea work (better), they can't claim *all* of the credit? Why should that be necessary? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 16:08:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> That's the answer I got. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 15:08:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, I'll be a little bit less obnoxious.

You're looking for `P(D|S1&S2)`. Using Bayes' Rule we get:

    P(D|S1&S2) = P(S1&S2|D)P(D) / P(S1&S2)

Right?

Now, I think you should be able to figure out the numerator based on the information you have about conditional independence. But the denominator `P(S1&S2)` is a little bit annoying, because we know from your previous question that S1 and S2 aren't independent.

One way to solve this is to apply a technique called marginalization. Basically, we know that you either have D or you don't have it. And we can look at the case where you have the symptoms given that you have D, and the case where you have them given that you don't have D. In other words:

    P(S1&S2) = P(S1&S2|D)P(D) + P(S1&S2|-D)P(-D)

(where -D means "not D")

If you use this, then I think you should be able to figure out the answer.

(0.5 is not exactly right but pretty close) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 14:58:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> How did you arrive at them? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 14:50:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> What are the two answers you have in mind? Look, you can post here about AI related things like Bayesian Networks, but we're not /r/cheatatmathhomework (which actually exists). Please explain how you arrived at your answers, and what about them you're not sure about.

As a hint I'll say this: You have some probabilities that are in the form of "probability of something *given D*", and you want to turn that into "probability of D *given something*". I'll note that you can use Bayes' Rule to go from P(A|B) to P(B|A), so you will have to use that. You can reason back from the result you want P(D | S1&S2), and see what other probabilities you need to calculate it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 14:41:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think the discussion of what is and isn't "AI" is pointless, but I agree with you that Stockfish is probably a lot less known in "AI circles" than AlphaZero. I think most people's reference point is probably Deep Blue, which, based on no research at all, I though Stockfish was basically a modern variant of.

The thing that impressed people about AlphaGo first and AlphaZero later is that they contain very little domain-specific knowledge, and that (virtually) all knowledge about what constitutes good moves (but not *legal* moves) was learned. And with AlphaZero (and now MuZero), the required game-knowledge decreased even more.

I'm somewhat curious about this *dynamic* search in Stockfish. Is this done to do some sort of opponent modeling? Or is it just to decide on what branch to expand next in the minimax tree? If so, how is that different from MCTS which also keeps statistics to know which branches to go down (the most often)? What is the best place to get a (ideally not too long) overview of the Stockfish algorithm? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 14:23:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't really know anything about crowdsourcing. Doesn't it usually involve putting your idea on Kickstarter and asking the crowd for money to fund the development?

Or do you mean that you want the crowd to do the actual development, like in open source software? I think that typically starts with someone making the first version of the software, which through its popularity attracts people who might want to help develop it further. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 14:08:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> What you want is a little vague, but sounds like fairly standard datamining functionalities. A lot can be done, and there are also limitations (especially causal inference is hard), so I have no idea if what you have in mind is possible with current technologies. Combining technologies is often possible, but labor intensive.

I also don't know how you should go about procuring developers to make this for you. However, I think that whatever you do, you should get a much clearer picture of what you need this tool to do exactly. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 14:04:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> Short answer: Yes, they're pretty much analogous.

Longer:

Deep Learning (DL) and Reinforcement Learning (RL) cannot really be compared to each other, because they are of a different kind. Reinforcement learning is a *paradigm* in machine learning (ML), like supervised learning (SL) and unsupervised learning (UL). It has more to do with the format of the task that we want to be learned: in UL the ML algorithm does not receive any external information about whether it's doing a good job (at least not from the data/task), in SL such information *is* available and typically takes the form of "the correct answer/output" for all data points in the training set, and in RL this information is limited to a numeric indication about whether the algorithm did something right or wrong (reward and punishment).

Deep learning refers to a family of ML algorithms that can be used to do (primarily) SL, UL and indeed RL.

My short answer was "basically yes", because all ML systems are learning from the signals that they receive  (input + extra information) and produce (output + internal state). And these can come from a data set or interaction with a (simulated or real) environment, so these things take a similar role.

A very typical SL task with a data set is classification, where you have to assign each data point to a class and -- because it's SL -- you are given the correct answer for each data point in your training set. Now, this could be turned into an RL task by, instead of giving the correct answer, we look at the RL system's guess and we say if it's right or wrong (or we give a reward based on how close it was). (In the UL variant of this, we don't tell the algorithm anything, and it basically turns from classification into clustering.) Now if you think about the logistics of this, it would be pretty easy to just store a data set with the correct answer/class for each data point for SL. But for RL, you need to adapt the extra information based on the RL system's output, so if you want to store this in a static data set, you'd need to store the reward for each possible output, which probably implies that you know (and could just store) the correct output and you'd be better off using SL.

In the typical RL setting you have the system interact with an environment to achieve some goal (or actually: optimize the rewards it gets), where it gets a reward/punishment signal on every step (because it's RL). We could "easily" turn this into an SL problem by replacing the reward/punishment signal with the best output for the current situation. Then we'd have SL with a (simulated) environment instead of a data set. However, this typically doesn't happen, because it's often hard for us (human developers) to know the best output in *every possible situation* the system might find itself in. But what's even worse is that the correct action in one particular situation doesn't imply much about the correct action in other situations. With RL, if you get a huge reward on a certain time step, you can assume that you did something right. But you can *probably also* assume that you did something right on the previous time step(s) because it led to this one. So with RL you could learn from very sparse rewards (e.g. you only get a reward/punishment at the end of a chess match).

Hopefully this answers why these things are analogous, and why data sets are more often used with SL and environments with RL. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-04 13:07:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> That line is saying that the symptoms are not [independent](https://en.wikipedia.org/wiki/Independence_(probability_theory\)) in the probabilistic sense, but they *are* [conditionally independent](https://en.wikipedia.org/wiki/Conditional_independence) *given* that the disease is present. I recommend that you take a look at the linked pages to understand what independence and conditional independence are.

S1 and S2 are not (unconditionally) independent, because S1 raises the expectation that you have D, and having D raises the expectation that you have S2. To see this intuitively, replace these with real things. If you know that someone is coughing (S1), it should raise your expectation that they are also sneezing (S2), because the coughing means they probably have a cold (D) (or the flu or whatever) which would also cause sneezing. However, if you already *know* that someone has a cold, then learning that they're coughing does not tell you anything extra about whether they're also sneezing (the answer is "probably yes", but that's because you know they have a cold; not because of information about the coughing). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-03 06:15:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> You can't. We have basically zero scientific knowledge of sentience. We have a bunch of philosophical theories, which we can't falsify (including dualism, which rationalists seem to hate). We have absolutely no way of measuring qualia.

What you can try to do is take those different theories (or maybe just your favorite one) and look at what they predict about AI. (However, you'll probably find that most don't make very precise predictions.) Probably some forms of dualism/pluralism would predict AI did not in fact get endowed with a "soul". Physicalist and functional theories would probably say that it's possible, but most don't say much about what the physical configuration or functional processes need to look like to result in (high levels of) sentience. Panpsychism would predict that these AIs are indeed somewhat sentient, but that's also true of everything else, and it's not necessarily due to its "intelligence": maybe a computer running that AI is just as (or less) conscious than that same computer running Microsoft Word. Tononi's [integrated information theory](http://integratedinformationtheory.org/) is one of the few theories I know about that make quantitative predictions, and here it would really depend on how the AI is implemented, but there exist pretty good (IMO) [critiques](https://www.scottaaronson.com/blog/?p=1799).

Having said that, we might ask what would be our reason for treating today's AI like it's more conscious than other things we interact with on a daily basis, like our computers and thermostats (and perhaps even chairs, tables, etc.). This might get tricky in the future, when we AI (or AGI) that actually appears more conscious (and it would be unfair to make them "prove" they're sentient, while we'd require proof of non-sentience for other people and animals), but right now I don't see much similarity between out AI systems and humans, dogs, chimps or dolphins. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-02 22:59:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, thanks! To be honest, these questions seem more research and philosophy-related than industry-related to me, so I'll answer them even though I work at a university and not in industry (anymore).

Actually, it looks to me like you have a view of AI that doesn't match the current industry at all. 99% of AI research and 100% of current AI applications are what we call *narrow AI*: "smart" software specialized to automatically perform some tasks previously thought to require (human) cognitive abilities. (This is not a universally agreed upon definition, but it will do for now.)

> 1)

Define "emotions". Emotions can be viewed as signals, (mental) behavior modifiers or heuristics/shortcuts (among other things). In these ways, AI already has emotions, but they tend to be very different from ours. If you're looking for human emotions exhibited by humanlike AI or robots, the first problem is that most AI and robots aren't really humanlike. There is also a difference between uttering some "emotional" sentences or making a facial expression, and actually experiencing the corresponding human emotions. This kind of mimicking could certainly be achieved with machine learning, but people have also made emotive robots without it.

Would it be possible to build an AI that actually experiences human emotions? I suspect it would be, because according to the Church-Turing thesis, a computer could execute any algorithm (as long as it has enough resources), so I think that *in principle* a full human mind could simply be "run" on a computer (including emotions). But if you're going to make an AI (even an AGI) more or less from scratch, then I think it's unlikely that it would have the same emotions as us, if only because its experience of the world would be so different.

> 2)

Not to admonish you too much, but AI professionals generally think we should be careful with anthropomorphizing language like "one-track mind". It means all kinds of things in humans that we may not want to imply about AI. Narrow AI is simply specialized for a single task or domain. If you want to automate multiple things, the solution is very often just to create multiple AI systems (that might run on the same computer or robot, or not).

Most machine learning (ML) algorithms work by learning a single "task". However, you're free to define that task as "A and B and C" if you want. Even if you *think* you only have a single task, like chess, you actually might have many, e.g. make the best move from this position, and from that position, move a pawn, move a bishop, play white, play black, etc. It just depends on how you look at it. Typically it helps if your (sub)tasks are related; otherwise you'll likely have to train on A and B and C (interleaved / at the same time) for a much longer time than if you just wanted one of those tasks.

There is work in "multitask learning", "transfer learning", "lifelong learning", etc. that (sometimes) tries to remedy this issue. Hierarchical learning can also help. A big problem many approaches suffer from is catastrophic forgetting / catastrophic interference, where learning a new thing B makes the AI forget about what it already knew about thing A.

I don't have the full solution, but I think we need to use AI systems with knowledge representations that allow new knowledge to be added without it necessarily causing other things to be forgotten. Of course memory isn't infinite, so a sensible forgetting mechanism will need to be in place. Additionally, mechanisms are needed for "categorizing" the new knowledge and fitting it into the existing knowledge base.

> 3)

Part of the difficulty is that we don't really know the answer to that question. I've posted some links [here](https://www.reddit.com/r/MachineLearning/comments/72dus0/discussion_serious_what_are_the_major_challenges/?depth=20) to published roadmaps and open questions. There's another list of requirements in [this AGI'18 paper](http://jordi.cyberbyte.nl/files/bieger_aegap_trustworthy.pdf).

Presumably we'll need things like learning different kinds of tasks (including e.g. classification, regression, control, question-answering, etc.) in any order, from different sources (e.g. examples, observations, instructions, etc.), from small amounts of data, without catastrophic forgetting. Additionally, metareasoning, metalearning, resource management (attention), understanding its knowledge on multiple levels, causal reasoning and inference, abstraction / multiscale reasoning, and I'm probably forgetting a bunch of important stuff.

One challenge is certainly to incorporate all of these things at the same time. Sometimes people try to tackle one aspect, but that doesn't help much if it's incompatible with the others.

> 4)

/r/artificial's wiki has a section on [Getting Started with AGI](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) that links to articles by Ben Goertzel and Pei Wang that each make a categorization of approaches. At the 2017 AGI conference Alexey Potapov and Ben Goertzel also gave [talks](http://agi-conf.org/2017/?page_id=24) on such categorizations. I prefer Potapov's, which included cognitive architectures, deep learning (including brain emulation), probabilistic models (including probabilistic programming) and universal AI (i.e. algorithmic information theory).

Since nobody has a working AGI system, we might say none of these are very successful. Of course, deep learning has achieved a lot in the narrow AI domain, and lends itself well to benchmarking. The same goes to a somewhat lesser extend for probabilistic methods, and even less so for (most) cognitive architectures. Universal AI is pretty much confined to theoretical results.

Potapov suggested that we should look at the intersections between these methods, which is probably a good idea. Architectures like Goertzel's OpenCog try to combine many different approaches together, with the idea that they might each be good in some circumstances. I personally prefer unified cognitive architectures like NARS and AERA though.

> 5)

It depends a bit on your exact definition if ASI. I think human-level AGI will certainly be possible, and from there it will be a short step to AGI that is better than human, because at the very least we can run it with more computational resources (i.e. higher speeds and more memory). How much this would qualitatively improve the AI's intelligence remains to be seen.

There are other ways in which the AI could improve, including making use of a lot more knowledge (especially the knowledge already cataloged on the internet), gathering more resources, and reprogramming itself. I think that with recursive self-improvement, it's likely that the AI would, at some point, hit a (local) optimum, and I have no idea if this will happen soon or not.

Some critics of the idea that an AI could e.g. speed up science (or its own knowledge acquisition) a lot say that knowledge acquisition takes time, because it's not just a matter of thinking really well and really fast, but also interacting with the world. I think this is true for some things, but less true for others, and it certainly seems that in domains like mathematics and computer science the main bottleneck is often in the thinking step.

> 6)

Yes, it's a huge issue. Humans have all kinds of biases and they get into the data sets we use to train our AI systems, and sometimes even in the code that defines them.

The main way to deal with biases is to be aware that they exist, and check for them in the algorithms we use/make. One advantage of AI over humans is that this is much easier to do and fix (which doesn't mean it's *easy*).

Some methods for dealing with bias include using more representative / unbiased data sets, prohibiting the use of certain information (e.g. race or gender), and using interpretable/explainable models so we can more easily check if biased decisions are (still) being made. It's also very important to be aware of what a ML system is *actually* trained to do, because they are often *used* for slightly different things (for which they are "biased").

Some difficulties include that there are many definitions of bias/fairness that are often mutually exclusive, and the fact that some outcomes may be judged undesirable while also being unbiased.

> 7)

That will remain to be seen. What's certain is that AI is an immensely powerful technology, whether we're talking about narrow AI or AGI. This means the potential to do good is virtually endless. Every problem humans have ever solved (including how to rule the planet), we solved by applying our intelligence, and having more of that can greatly aid in our further technological, medical and socioeconomic progress.

At the same time, this powerful technology can be used maliciously, or have adverse side-effects. You can kind of see this in China, where the government is using AI to help suppress the populace and discriminate against Uyghurs. Other examples include technological unemployment, autonomous warfare, manipulation (e.g. with fake news), discrimination (due to bias) and privacy violations.

AGI/ASI raises the stakes tremendously, to the point where a benevolent ASI could perhaps create a virtual utopia, and an unfriendly ASI could wipe us out and do all kinds of other bad things. So we had better solve the /r/ControlProblem before building AGI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-02 22:21:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you have a symptom (e.g. S1) of a certain disease D, does that increase or decrease the probability that you have D?

If you have disease D, does that increase or decrease the probability that you have a symptom of that disease (e.g. S2)? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-02 22:16:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> I haven't taken either one, so I don't really know. fast.ai seems free and aimed at coders like yourself. deeplearning.ai seems like it would cost money (although I didn't look around the website for long; there might be a free version...), but it seems like it also has a "work in AI" program. I took Andrew Ng's (of deeplearning.ai) Intro to ML course on Coursera many years ago and I really liked it.

Just because it's free (and for coders), I'd probably start with fast.ai and then maybe consider paying for deeplearning.ai if that turns out to not be enough. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-02 19:24:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> Step 1 is getting the skills, for which there are many online resources, including fast.ai and others mentioned on our wiki's [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). If you want to go deeper, you'll probably want to pursue a master's degree (or PhD) in AI/ML, but since you say you don't want to be an AI researcher, that is likely not necessary (although it might help with Step 2).

Step 2 is finding work as an AI Engineer. For this you could probably just apply to those jobs because of your SE background, especially if you know someone, but it will likely be helpful if you can *show* your (AI) skills. You can of course put online course results on your resume, but my understanding is that this isn't always highly regarded. It may be better to build a portfolio on GitHub and/or participate in competitions like Kaggle or something. I also think that (if you pay) some online course providers and bootcamps will help with the employment issue, so you could look into that. (This is just to improve your chances; my understanding is that demand is quite high, so this may not be necessary.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-02 19:15:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you don't know whether or not you have the disease D, what does the presence of S1 (or S2) tell you about it, and what does that tell you about the presence of S2 (or S1)? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-02 07:00:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> What do you want to do exactly? Do you want to interview someone on Skype / video chat, or do you have a list of questions you could just post here for (multiple) people to answer? (Even if it's on Skype, you could post your prepared questions so people can see what they'd be getting into.) How much time do you expect it will take?

What do you need from the professional? Do you want them to work in industry, or is it enough to be a professional (e.g. in academia or government)? Do they need to provide any credentials? Do you have any specific area in mind (e.g. machine learning, data science, natural language processing, computer vision, etc. or perhaps someone who focuses on ethics, socioeconomic impacts or a particular application area)?

What are you going to do with the interview? Who else will (potentially) get to see it, aside from (presumably) your teacher? Will it be put online or published in any other way? Can the professional do the interview anonymously? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-01 19:31:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, well, I don't know your situation exactly. In my bachelor I think there was about 30 ECTS of elective space. And if your studies come easy to you, you could probably take even more computer science courses (I assume those are what you need). It might also be worthwhile to discuss this issue with your study advisor to see if e.g. you can take some CS courses instead of some math courses to better meet those requirements (or you could contact the admissions office for those master's degrees to see if the requirements can be partially waived).

In the worst case, I guess might have to spend an extra in-between year. I understand you probably don't like that, but after that I'm telling you you'll have an amazing background to study AI. When I recommend which bachelor people should take if they cannot find one that specializes in AI, I'm often torn between recommending CS and maths. I think math will actually teach you better skills, but the path from CS to AI is a bit more "natural". You can look at the Getting Started article on /r/artificial's wiki for a bit more on this. I hope you find this somewhat encouraging. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-12-01 14:54:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't have any recent experience, but a bachelor in mathematics is one of the best possible backgrounds you could have. Math is very important for contemporary AI/ML and it seems harder to learn on your own than e.g. computer science and cognitive science. Why do you say your chances don't look good?

Also, what year are you in? If these MSc programs require some classes there might still be time to take them as (extra) electives. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-29 16:29:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> The actual problem depends a lot on tax law etc. and I don't know much about that, although I think money normally shouldn't be taxed twice.

> Maybe some sort of iterative calculation?

This is not necessary. I'll go through the calculation based on how you've laid things out, but you need to understand that this is almost certainly not how it should actually work when taking your countries' tax codes into account.

You're basically asking: how do we divide $12600 so that after your partner pays 24% taxes, you end up with an equal share. Let's say you give him X dollars, then he'll end up with 0.76X (because he pays 24% taxes) and you'll end up with (12600-X), and you want this to be equal: 0.76X = 12600-X --> 1.76X = 12600 --> X = 12600/1.76 = 7159.09091.

So if you give him $7159 you'll both end up with $5441 after he pays $1718 in taxes. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-29 11:49:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://github.com/sebastianstarke/AI4Animation

It says the code is coming soon. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-27 07:10:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> Are you specifically interested in AI in video games, or in AI / machine learning in general? I think /r/gameai is for video game AI (which I don't know too much about specifically), while /r/artificial is about the broader field of AI (there is also /r/MachineLearning but it's mostly for professionals). /r/artificial's wiki has a getting started section that might be helpful to you if that's what you're interested in.

You should think for yourself what it really is that you want. A software engineering program is likely relatively practical and prepares you for building applications (which may use some AI techniques if that's what you focus on). On the other hand, if you want to research/design/improve on AI/ML algorithms (that a software engineer or data scientist/analyst/engineer could then make use of), you may be better off studying something like computer science or even math. (That is, if you cannot find a program that straight up focuses on AI or ML, but those programs appear to be rare.) </TEXT>
</WRITING>
<WRITING>
<TITLE> [R] Preventing undesirable behavior of intelligent machines </TITLE>
<DATE> 2019-11-26 16:48:44 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-26 02:51:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> Did you look at the linked Science article? In the two paragraphs that start with "One problem with the standard..." it's kinda sorta mentioned. They say it *could* be done by modifying the cost/objective function (or the feasible/hypothesis set of solutions/models/hypotheses considered by the algorithm), but that this would require extensive domain knowledge. So they want to shift the burden from the user (e.g. data scientist) of the ML algorithm to the creator of the ML algorithm by making an ML algorithm that stays within certain (domain-independent I guess) constraints.

A difference between equation 1 and 2 is also that in equation 1, they optimize over the *solution* (or model) to an ML problem. For instance, the weights in an NN or the separating hyperplane(s) in a SVM. In equation 2 on the other hand, they're optimizing over the machine learning algorithm (e.g. backpropagation for NNs or quadratic programming for SVMs or something; except the idea is to build new alternative algorithms to those). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-26 00:08:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> Naturally the title is overblown, and I think even the Science paper's title is a bit much: [Preventing undesirable behavior of intelligent machines](https://science.sciencemag.org/content/366/6468/999). I haven't read it yet, but seeing that Andrew Barto is one of the co-authors raises my expectations.

Edit: okay, I skimmed it. It seems that it's not presenting an ML algorithm that can be constrained, but rather a "framework" for defining new ML algorithms that can be constrained. There's a bit of math, but what it seems to come down to is saying "you should make ML algorithms that can be constrained". That is, they'll try to optimize an objective function, but only as long as the probability that each specified constraint is violated on the data set remains small. A few examples of these new "Seldonian" algorithms for regression, classification and reinforcement learning are given, but it's stressed that the main contribution is the framework.

To name one example: a regression algorithm is used to predict students' GPA score. Naturally, the objective function is to minimize the error, which causes most algorithms to overestimate men and underestimate women (which I guess averages out fantastically). By providing constraint that the prediction errors for men and women should probably (>95%) be within a small range (5%) of each other, the new algorithm removes this discrepancy. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-25 10:11:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, so to be clear you want to put receipts into classes; not assign a class to each item on a receipt, right?

Your workflow sounds fine overall, but you can probably improve performance by making use of domain knowledge to inform more extensive preprocessing and more structured input to your classification algorithm. Instead of throwing everything into a giant bag of words (or something similar), you could make different bags for the header, the items, etc. You could extract prices (at least total price), and feed it to the classifier separately, since that likely carries some information too (groceries is lots of small prices, lodging is likely just a few large prices, meals are a few medium prices plus a few small ones for drinks, etc.). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-25 09:52:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> > if that really falls into AI?

Automatically counting the number of trees in a picture is computer vision, which most people would agree falls under artificial intelligence.

It's hard to give you ideas for what to do when we don't know anything about your project. What level of school are you in? How much time should you spend on it? How long should the report and presentation be? What does your prompt / assignment say? Why did you pick AI and what interests you about it? etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-25 09:45:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> Can you tell us more about the problem? What is it that you need to classify exactly? What store the receipt is from, what the total amount was, etc.? It sounds like your input is text (not pixels, right?), but how is it structured? Receipts are not exactly 100% unstructured text. Stores don't have infinite inventory, so couldn't you just obtain a list of possible items so you don't have to worry about out-of-dictionary words?

https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-23 10:22:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> > For example showing the percentage of a population that supports an issue, it's very easy to distinguish say a 70-30 split from an 80-20 split on a pie chart vs a bar chart.

I disagree. The difference would be much easier to see and quantify on a bar chart; especially if you use background guidelines. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-23 08:51:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> I just want to let you know that your hate of pie charts is shared by pretty much all data science experts I've heard on the matter, and well-documented in text books on data visualization, research methodology and statistics. I'll just quote [this article](https://qz.com/1259746/when-should-you-use-a-pie-chart-according-to-experts-almost-never/) which starts by citing some of these objections, only to explain why pie charts will never die: people just like the look of circles. (Replying here because I can't put agreement in a top level comment.)

I think it's fine you gave a delta: pie charts are appropriate because people love looking at pie charts. But I'll posit to you that a bar chart can also show that 80% is "overwhelmingly" larger than 20%, and that if you're going to compare two countries, bar charts are probably vastly superior. With pie charts, could you easily tell country 2 has more industry and the same amount of services as country 1 has industry? With a bar chart it'd be trivial to see.

But yeah, pie charts are the least terrible if you want to show static proportional data, and you just want to show large differences but don't really care about how large they are. It's hard to imagine when they should be used in professional business reports. But maybe they can sometimes be superior in info graphics, because it seems many people have an aversion to (or are less interested in) math and (bar) charts so they might not look at them, in which case they're communicating *zero* information. If those people's attention *is* grabbed by fancy-looking pie charts, then you have a chance to give them (some) information. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-22 11:54:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> This article is from 2013. What would you like to discuss here? In my experience, when someone associates Epstein with a project or person these days, it's meant as an attack. If that's the case here, I'd like you to make your case more explicitly. As the article points out (uncritically I might add), Epstein funded a lot of research, including Stephen Hawking and several nobel prize winners. In the AI realm, I can tell you he also funded Ben Goertzel's research in the early 2000s as well as big labs like MIT's CSAIL.

Researchers everywhere are always looking for funding, and tech-interested billionairs are a decent source for that. Perhaps an argument could be made that researchers, who often believe their research would make the world a better place, should refrain from accepting that funding if it comes from someone who has committed crimes or done something morally wrong. But then I think it would be much better to lay that argument out explicitly, and there certainly seems to be no reason to single out Joscha Bach or MicroPsi in this regard. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-22 11:13:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't personally attack people. Engage with the arguments if you disagree. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-22 02:50:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> The [Cognitive Architecture](https://en.wikipedia.org/wiki/Cognitive_architecture#Notable_examples) page on Wikipedia has a list of notable examples that you could look at. I think ACT-R and Soar are used commercially, I know that parts of OpenCog have been used, and NARS is now sort of flirting with some commercial partners. I don't know about the others.

> I believe the general ai community would want to see results before more adoption of similar like architecture occurs. I believe that's how deep learning took off - speech/object recognition, etc

I agree that concrete results are what the mainstream AI/ML community demands, but I also think there's a big difference with the approaches/mindsets of many AGI researchers. Essentially, mainstream AI/ML is about what works right now, and making small incremental improvements to that, while (some) AGI researchers are trying to accomplish the goal of AGI directly. And before they reached that, it shouldn't be surprising that on pretty much any well-defined benchmark narrow AI that is specialized for that benchmark wins.

Imagine an analogy between achieving AGI and traveling to the moon (on some alternative Earth). Early research involved building better trampolines, but eventually some realized that you shouldn't just go high, but also far and fast, so they started focusing on dog sleds. In 2012 deep learning researchers discovered the horse, which was vastly superior in every way, and now there is a lot of research on building better carriages, efficiently adding more horses, and breeding horses that are faster and stronger.

Meanwhile, AGI researchers are thinking of ways to build rockets. They're impressed by how fast you can reach far with a horse-drawn carriage, but they don't think breeding better horses is going to bring us to the moon. Unfortunately, their rockets don't work yet. If you build half a rocket, it doesn't get you halfway to the moon: it just stands there. Hell, you could build 99% of a rocket and it wouldn't do much.

Mainstream AI researchers are unimpressed. "Call me when you win a race or jumping contest", they say. Those dumb tall cylinders don't even do anything. Sure, they're not done, but who's to say they'll even work when they are? Where's the evidence? Some AGI researchers agree they don't have any guarantees, while others are extremely confident in their own ideas, and none really want to be distracted from their work by strapping a bunch of horses to their rockets so they can compete in races and lose to carriages that were specially designed for that. What would that even prove?

---

You can see both sides have a point. Iterating on horse carriage design isn't going to get you to the moon anytime soon, but it's not like even a finished rocket is guaranteed to do the trick either. Of course, the most controversial aspect of this analogy is probably that I compared deep learning to horse-drawn carriages and not e.g. to working jet airplanes or something. But I think this illustrates the mindset of a lot of AGI researchers. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-22 01:49:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> To be clear, the AI this is referring to is not the narrow AI we have today, but artificial general intelligence (AGI) or artificial superintelligence (ASI) that we don't know how to build yet. This is not a great article, but if anyone is interested in dangers of AGI/ASI, they should check out /r/ControlProblem (and their side bar and wiki). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-22 01:41:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://www.popularmechanics.com/science/environment/a29703282/artificial-intelligence-lightning/ </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-22 01:27:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your reply. I think I may have mixed you up with another user (since I had to guess at the alt account).

I became a mod here 2.5 years ago and if you had interactions with other mods prior to that I can't really say much about it. I encourage you and others to debunk uninformed posts you see, and it's great if that happens with appropriate references and dissections of where the arguments are inaccurate and wrong. However, I do stand by the idea that you should not personally attack other people while doing so, and the idea that those other people should also be allowed to post their (possibly uninformed / wrong) opinion on the matter. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-21 01:54:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for the suggestion!

Another suggestion was to add a tag system like in /r/MachineLearning, where every post has to be clearly marked as [news], [opinion], [question], etc. (I still have to think about this list). This could perhaps be combined with your suggestion.

I don't want to disallow links to people's personal blogs altogether, because they might contain interesting information, (right and wrong) perspectives, tutorials, explanations, summaries, etc. But they could perhaps be disallowed in combination with the [news]-tag, and required to be tagged as [opinion] or [education] or something. (Please note that people just posting their own blog posts is already disallowed as self-promotion, although I'm not always super strict about this.)

I agree it might be desirable to only have [news] from reputable sources. I've been reluctant to act on this because of the above-mentioned issue with subjectiveness. It would be *fantastic* if we could formulate an objective rule that determines what is "reputable" and what isn't, so I don't have to tell people "well, I just don't really like this website". Why should I not allow [this YellRobot article](https://yellrobot.com/china-testing-ai-screen-newborns-genetic-disorders/) but I should (presumably) allow [this MIT Tech Review article](https://www.technologyreview.com/s/614740/ai-chip-cerebras-argonne-cancer-drug-development/) (top AI results on both sides)? Actually the coverage on YellRobot doesn't seem *that* bad, and the title is arguably less clickbait-y and more accurate than MIT Tech Review's. (I think this is actually a pretty big problem, because it seems to me that virtually everyone has clickbait titles these days and AI coverage is often terrible regardless of the source, but I don't have time to read every article posted here in detail before deciding if it should be allowed.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 10:57:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are [AGI researchers](http://www.agi-society.org/resources/), but they are often a bit outside of the mainstream AI/ML researchers. I get the feeling most of those are at peace with the idea that they're solving specialized real-world problems with "smart" machines ("narrow AI" in the eyes of those AGI researchers). There were a few workshops at IJCAI in [2017](http://cadia.ru.is/workshops/aga2017/) and [2018](http://cadia.ru.is/workshops/aegap2018/) that tried to bring together AGI researchers and researchers from the broader AI field, but they weren't super well attended.

I do think DeepMind and OpenAI (and maybe deep learning in general) have put AGI back into the minds of more "mainstream" AI/ML researchers though. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 10:20:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> Nothing on this page appears to be from after 2001. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 09:52:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> /r/ControlProblem is about this topic and they have a lot of interesting links on their side bar and [wiki](https://www.reddit.com/r/ControlProblem/wiki/index). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 09:48:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> Stan Franklin was still publishing about LIDA in 2018 and 2019, and you can request the code [here](http://ccrg.cs.memphis.edu/framework.html). (I don't know when the code was last updated though.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 09:46:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> The posted article is indeed about moral decision making, but LIDA itself doesn't necessarily focus on that. (See [my other post](https://www.reddit.com/r/artificial/comments/dy77od/lida_an_agi_model_of_human_cognition/f84bg3a/).) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 09:44:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> This article is not about LIDA itself, but about using it to implement "A Conceptual and Computational Model of Moral Decision Making in Human and Artificial Agents". You can find more information about LIDA itself on UMemphis's [project website](https://www.memphis.edu/iis/projects/ida_and_lida.php), the [tutorial site](http://ccrg.cs.memphis.edu/tutorial/index.html) and on [Wikipedia](https://en.wikipedia.org/wiki/LIDA_(cognitive_architecture\)). You can request the LIDA software framework [here](http://ccrg.cs.memphis.edu/framework.html). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 09:29:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think it's probably difficult (and perhaps also undesirable) to insert AI safety at every part of learning. As the commenter on /r/ControlProblem points out: what are the AI Safety implications of learning about a for-loop? At some point, especially in the beginning, the concepts you're learning are so basic that it's hard to make connections like this.

I suppose you could try to foster a general safety mindset by trying to incorporate it in every aspect of the education as much as possible. Some programs already do that to some degree by teaching students about bugs, debugging, testing, (cyber)security, (formal) verification, etc. You could perhaps put more emphasis on these, e.g. by teaching safety- and correctness-minded software development approaches early on. I'd normally say this could involve e.g. test-driven development, but with AI Safety the idea is that you can probably not test (since that's unsafe), so you might have to put more emphasis on proofs.

Another aspect to emphasize is societal impact. Again, it's hard to say something about the societal impact of a for-loop, but in your example programs and exercises you could place some amount of focus on what the real-world impact of an application would be and whether that's desirable. "AI Ethics" concerns about privacy, autonomy, fake news, fairness, bias, addiction, autonomous warfare and technological unemployment could fit here, but perhaps it could be as simple as asking how the calculator program you made in lesson 5 of Programming 101 could affect people's ability to do math in their head, or how a bug might affect someone's finances or something (since it's about fostering a mindset that thinks about these things).

In terms of the curriculum, you could also look at AI Safety research and discussions and 1) try to incorporate them into classes as soon as possible, and 2) give classes that work towards being able to understand certain kinds of research. For instance, why are you teaching programming and ML to begin with? Most AI Safety research involves philosophy and math. I'm not saying you shouldn't teach programming and ML, but it might e.g. be smart to place more emphasis on reinforcement learning than is traditional, because this is used in most AI Safety models. Programming is presumably not geared towards enterprise software development, but more towards writing one-off research scripts. (Or not, but you can inform your choices by your end goal.)

In the end though, I'm afraid that a lot of the issues in AI Safety are unique to the prospect of having AGI/ASI. A normal computer program isn't smarter than you in every way, *actively* working against you, subverting your efforts to turn it off, etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 08:58:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but this story was already posted [here](https://www.reddit.com/r/artificial/comments/dyxkxd/how_to_recognize_ai_snake_oil/) a bit earlier, so I'm removing this. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 08:33:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> This issue has been talked about pretty extensively by many AI safety people, so if you came up with it independently: good job!

I personally still think it's worthwhile to put *some* effort into containment solutions. While I think that it is likely (but perhaps not certainly?) impossible to make an impenetrable container for a software program, I think there are virtually endless possibilities for improving what we have. You're absolutely right that the user remains a vulnerability, but there too I think we can make improvements by considering different protocols. (For instance, if the operator doesn't have the direct power to break the AI out, that makes things a bit harder. If there are multiple operators, that likely makes things even harder. etc.)

And for every improvement we make to these containment approaches, you'd need a higher level of intelligence/capability to break out. And even an AGI system won't have literally limitless intelligence (and we can likely better restrict the intelligence of a "contained" AI as well).

Like you, I don't think of this as a permanent solution. Even if we could contain an AGI up to a certain level of intelligence, there'd always be incentives to push the intelligence higher (e.g. competitive advantage), and someone would eventually let their AGI get intelligent enough to break out.

But I do think it could buy us time. It might allow us to experiment with an actual working AGI system, which could help develop a friendly one. Furthermore, it is unfortunately the case that not everybody who works on AGI wants to make it "friendly from scratch". If those people develop AGI first, then a lot of AI safety research might not help them, because it would need to be applied from scratch. But I hope they could be convinced to at least initially run their AGI in a robust container, which might prevent immediate catastrophe. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 08:18:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> Live and learn I guess. I also agree that this is a weird cut-off. Presumably you care about part-time vs. full-time vs. ridiculous amounts of overtime or something. In my country, some workplaces / fields consider 36 or 38 hours "full-time"...

> the options for that question doesn't include "student."

It does though. (Maybe you added it?)

I also think the marriage question should probably be replaced with one about relationship status with a few more options (and kids if you're asking about that kind of thing?). And if you anticipate getting respondents from many different places (as suggested by the "Where do you live?" question), it might be better to have options related to the respondent's region's median income or something rather than absolute dollar amounts, because what is a high salary in India is probably a low salary in Silicon Valley.

Anyway, I think it's great you're doing this. Keep up the good work! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 07:49:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> /r/MachineLearning is specifically geared towards professionals, while /r/artificial also aims to cater to interested laymen and beginners. This means that more kinds of posts are allowed here (also, AI is broader than ML).

Aside from that, I don't disagree that the moderation there is better, but I'm not sure how to apply the lessons/methods from that subreddit to this one. Concrete suggestions are welcome. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 07:45:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't have any record of interacting with this account in the 1.5 years of its existence, but it's possible you've talked to me on an alternate account.

If I squashed your comments, it was undoubtedly because you were being abusive in them. I've certainly had conversations with Redditors who thought abusiveness was warranted if they thought they were dealing with someone they had (often incorrectly IMO) judged to be a troll. If the "troll"'s posts were inflamatory in the sense that they were abusive, they were also removed and warned. If they were inflamatory in the sense that you, or even mainstream opinions in the field, disagreed with them, then I would not have moderated them. And I absolutely welcome users like yourself to explain how "baseless and unscientific" posts are wrong, but I do require you to do so in a civil manner. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 07:37:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> This post has been reported a few times as a low effort meme and abusive/harassing. I kind of agree with that, but I'm going to leave it up because it is (in part) a criticism of this sub's moderation and it has also spawned some comments of that kind. People can of course feel free to appeal this decision in replies to this post (preferred, because it'd allow other users to participate in the discussion) or via mod mail.

I personally think the meme grossly overexaggerates the situation on this subreddit, but I agree that the quality could overall be higher. If anyone has constructive suggestions for how this could be improved by moderation, that would be great.

The moderation right now is extremely light and leaning away from censorship, relying in part on other users to "correct" bad posts with votes and replies. If a post isn't completely offtopic to AI (in a *broad* sense), antagonistic, spam or obviously breaking another rule, it will likely not be removed (and "spam" is not "opinions you disagree with"). Uninformed opinions in particular are, in my opinion, begging to be *informed*, not deleted.

I prefer rules and moderation policies that can be enforced objectively, and I do indeed try to be as objective as possible in my judgments. This means, among other things, that I'm not deleting posts that I personally regard as "low quality" if they're not "objectively" breaking a rule. That would just turn this sub in a (quite empty) echo chamber for my personal (professional) perspective on AI (which many other professionals would disagree with).

So please, if you have any suggestions, I would greatly appreciate them. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-20 00:09:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> Makes sense. Most universities these days have server clusters you can use. It's usually not just for AI (e.g. mathematicians and physicists often use them too), but training machine learning algorithms for speech, vision or e.g. game playing can certainly take a lot of computational power as well. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-18 23:04:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> That's great, but don't take one (or a few) persons' experience as the definitive word on how research is in academia (or industry). Relatively few people are doing the research I'm doing/did and if you're e.g. one of the thousands of deep learning PhDs your experience will look different (e.g. you'd use a lot more computing power). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-18 12:19:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> I suppose it's not hard to turn a test with a score into a two-player zero-sum game: the highest score wins.

I'm not sure what would be the point though. You'd lose information on both the absolute and relative scores. Furthermore, Elo rating is defined with respect to a field of competitors, which makes it difficult to measure actual progress. Finally, saying "let's use Elo" doesn't solve the problem of what tests to perform or how their scores should be aggregated. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-18 11:21:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> I guess you probably meant to reply to /u/racheta.

---

> Going through similar phase. Eager to know what you chose to do sir. I have come across attainu but that 50k security cheque as assurance they ask for is making me to doubt them especially since I dont know much about the company in general. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-18 04:18:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are a few links on [our wiki](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). You could start with Thrun & Norvig's AI course on Udacity for AI, or Andrew Ng's Coursera course on machine learning. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-18 04:16:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> I will try to answer this for three jobs I've had:

* computer vision engineer at a small-ish company doing mostly R&D to develop custom computer vision applications for clients (CV work)
* PhD candidate doing mostly theoretical, technical research on AI/AGI at a university (PhD work)
* Researcher on mostly sociotechnical and ethical topics related to AI at a university (current work)

> - What kind of projects are you working on, and what's a rough workflow for development?

For the CV work, this ranged from recognizing facial expressions/characteristics, to video surveillance, to 3D reconstruction, to behavior recognition, and I'm probably missing a few. The goal of a project was defined in collaboration with the client, and then we'd work on it with a small team (typically 1-3 people). We'd divide up the work, so that we were mostly working alone, or at least we'd have clear parts that each person was responsible for. Then it was R&D, where we'd dive into the literature/internet to see how similar problems had been solved and what the state of the art was. We'd then usually try to implement that and tweak it to our needs. This involved a lot of programming (also because the end result had to be a usable application), after initial stages with more reading.

My PhD work was very theoretical, so I'd spend most of my time reading papers. There was mainly just the one big project: my PhD. This could be subdivided somewhat into the papers I wanted to write. When I wasn't reading, I was usually making notes for myself to reason through, and occasionally doing some math. There were a few occasions that demanded some programming (e.g. to test/validate a theory), but this actually did not happen that much (unfortunately, because I like programming). Of course I was also occasionally writing the actual publications, and I spent a lot of time writing my dissertation. I'd see my PhD advisor regularly and brainstorm with him, and I'd visit a 1-2 conferences each year where I talked to other researchers (who sometimes kept in touch and co-authored a paper).

I'm currently a (postdoc) researcher and teacher. For the research that means that right now I'm mostly working on other people's projects (until I can get funding for my own). I'm working with several government bodies to research how they can best incorporate AI in their operations in an ethical way. This involves a lot of talking to people in those government bodies, taking interviews, and talking to my team members. Like the other research, this still requires a lot of reading and writing.

> I have a good idea for sw development, but less so for that. Is AI the core part of it or just one of many tools?

In the PhD and current work, AI is the whole point, so it's not "just one of many tools". However, in my current work, I do recommend to these government instances that they should use the best tool for the job and not "AI because it sounds cool".

In the CV work, we'd use whatever techniques worked best. This may or may not always qualify as "AI", but in my own (quite broad) conception it usually did. If no AI was required, there wouldn't be much reason for clients to come to us specifically.

> - What kind of studies have you done?

I studied AI in my bachelors and masters, and after the CV work (and some other things) did a PhD. That PhD was technically in computer science, but everything about it was basically just AI.

> Do most people in the field come from specialized studies or are many of them self-taught (eventually after general CS studies)?

Pretty much everybody in the CV company had a similar master's degree to me. There were not many other PhD candidates where I was, and certainly not many working on AI, but all PhD candidates needed to have a master's degree before starting. I think all the other AI PhDs had computer science backgrounds.

In my current job, there are usually people on my team with no real background in AI. They do have advanced degrees in other fields though (e.g. law or ethics). They know a little bit about AI, but rely on me for the more technical stuff.

> - ... and does that work mostly involve team work or do you actually work relatively independently?

In the PhD I had regular brainstorming sessions with my advisor, but I did virtually everything alone. In the CV company we'd usually divide the work (or work on very small projects) so that everyone could make a lot of progress without the help from others, but we'd occasionally use each other as a sounding board. In my current job, it's very heterogeneous: am I "working together" with the police chief who asked us for advice, or am I just interviewing her? If I look at working together with my "team mates", we usually meet every few weeks, and then break off and do our own part of the project.

> How much do you get to interact with other teams?

In the CV company, I'd get to interact with my colleagues all the time. I'd view us as one team, but we were often working on many different projects in different smaller "teams"...

In my PhD I'd mostly just interact with other teams at conferences, and now I very rarely meet other teams who are doing a similar job (but I meet a lot of teams who are doing something entirely different for our clients).

> - In general, how do wages align with e.g. plain software programming? (wage range for juniors appreciated if you know numbers for France).

Academic wages are not good compared to working in industry as a programmer or AI/ML expert. I hear being an AI/ML expert is quite a bit more lucrative than being a programmer, but I'm not sure because that CV company didn't used to pay a huge amount (but that was almost 10 years ago).

> What kind of career evolution can you expect in that field?

In the small CV company there weren't a lot of opportunities for growth, but I suppose you could go and work for larger and/or more lucrative companies and get more of a leadership role.

The academic path is to get a PhD, become a postdoc, get a tenure-track assistant professor position, then associate professor, then full professor and then ...profit? But very few people reach that. In some places it's possible to just be a researcher... Of course you could always exit to industry and (maybe) get a lucrative job at DeepMind or something.

> - Just for fun: What kind of hardware do you tend to be working with? GPU clusters?

I worked at the CV company before the deep learning renaissance, and I never used huge amounts of compute for my research, so I'm just using regular computers.

> - Is AI a well defined field today anyway? Since it seems to be a buzzword with a fuzzy definition.

The term certainly has a meaning to me, but I acknowledge that it's a buzzword and that different people are (mis)using it in different ways. To me it functions reasonably well as a label to put on a broad and indeed fuzzy collection of ideas, algorithms and approaches. But usually it's not the best descriptor for what someone is doing. At the CV company, if you'd have asked us if we're doing AI, I guess most would have answered "yes", but we would not have described it that way to you: we would perhaps have said "computer vision" or something even more specific, depending on the application, used techniques and audience.

Most of my (theoretical) PhD work was about a broad class of AI systems, and my current work is about the implications of introducing technology that could broadly be called "AI". But I think this kind of research is relatively rare, and most researchers and professionals tend to use much more specific terms for what they do.

> - In your particular domain, what importance do you think AI has and will have?

My domain *is* AI, so... pretty important? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-18 03:28:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> There is a bit about this on /r/artificial's [wiki](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai).

I'm a bit surprised you're calling the AI/ML path "generic" and the CogSci path "more focused". I realize AI is a broad field, but if you can find a place to specialize in it, I don't necessarily think CogSci (which is about all of cognition) is much more focused. Of course, if you're thinking of studying computer science with some AI/ML electives, that's a slightly different story.

What you should choose probably depends on what you're most interested in. The most direct way to see what both paths are about is to check the syllabi: different universities/programs might mean different things by AI, ML and CogSci, so any more general advice outsiders could give might be off-base. Having said that, I think CogSci will be more about studying *human* cognition and how that actually works in the brain. AI will likely be more about creating machines/programs that do "smart" things, where it doesn't matter too much if it does it in a similar way as humans (especially internally in the "brain"). AI (or actually artificial general intelligence) could focus on cognition/intelligence in itself (separated from "implementation details" of *human* cognition), but most likely your program will be about narrow AI and machine learning, which focuses on solving interesting problems with math, statistics, data science, etc.

I think the job prospects for machine learning, AI, data science, etc. right now are really good. I imagine that for cognitive scientists they are similar to other STEM careers (perhaps boosted a bit by the AI hype, but not as much as actual AI). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-18 00:13:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> Rob Miles addresses that in [this video](https://www.youtube.com/watch?v=JRuNA2eK7w0). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-16 09:39:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> I filled out your survey. Some notes:

* You should probably have more elaborate informed consent information in the beginning, where you state what you intend to do with my data and how you'll protect my privacy.
* "*somehow* agree" options on that Likert scale sound a bit ...unprofessional to me. The wording is usually "*somewhat* agree" or just "agree" (contrasted with "strongly agree").
* You have a couple of questions where you offer three options and the fourth is "I don't know". This is bad if I do have an idea of how to answer your question, but none of the provided options come close. "None of the above" would capture both "I don't know" and other reasons for not selecting one of your three options.
* In later cases this fourth option is missing altogether, which is not great either. Also, sometimes questions are worded as if they'd accept multiple answers ("What problem**s** have bothered you...") but they don't.

You're in college, so it's not expected that your surveys are perfect, and I hope you can learn from this. Good luck with your project and maybe consider coming back here to post about your results! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-15 20:15:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think this is generally very good, but I though one [Togelius's best points](http://togelius.blogspot.com/2017/07/some-advice-for-journalists-writing.html) was his first: "AI is a big field, and very diverse in terms of topics and methods used ... [so] ... do not assume that researchers you talk to knows 'what's going on right now in AI'". And as a personal pet peeve, I'd like to tie this together with your own point to not "cite opinions of famous smart people who don’t work on [something relevant to the discussion]".

I agree that if you want to have a discussion on the safety and risks of artificial general intelligence, you probably shouldn't be asking a physicist and entrepreneur about it. But it doesn't really make that much more sense to take the opinions of an NN-expert, roboticist or neuroscientist as authorities either. None of those people would likely claim to do any work on AGI, let alone its safety, so you should probably look for people who have made their career out of investigating the risks of AGI. In this case it actually turns out that the physicist and entrepreneur are closer to the real experts. (I don't think you should ignore all other opinions, and I'd encourage you to critically examine any argument you come across, rather than treat them as authoritative.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-15 14:07:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm somewhat encouraged by him mentioning Pascal's mugging, which was coined by Eliezer Yudkowsky and written about by Nick Bostrom. I could be wrong, but if he knows about Pascal's mugging, I'd guess he's probably also aware of Yudkowsky and/or Bostrom's work. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-15 12:44:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't attack other users, even if you think you're "joking". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-15 12:43:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> > How would they get the hundreds of hours of speech of the target to train their model?

[Lyrebird claims it can recreate any voice using just one minute of sample audio](https://www.theverge.com/2017/4/24/15406882/ai-voice-synthesis-copy-human-speech-lyrebird) (2.5 years ago). You need a lot of data to train the original system, but parameterizing it with a particular voice after that need not require that much. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-14 03:02:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> /r/ControlProblem is specifically about this topic. There are shorter articles on the side bar and wiki there. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-14 02:36:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you're looking for reading material etc., we have a [How to get started with AGI](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) section on /r/artificial's wiki that has a few links to resources about AGI (they might be a bit outdated, but still functional). These are mostly from the perspective of the AGI society/community, which typically doesn't use (deep) neural networks as a starting point (although more researchers have started to incorporate them in recent years). There was even a [workshop](http://agi-conf.org/2016/workshops/) at the 2016 AGI conference on AGI & NNs where most participants said they didn't think deep learning was the path to AGI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-14 02:12:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you're right to see similarities, but there are also important differences. Another good article on this is Scott Alexander's [Things That are Not Superintelligences](https://slatestarcodex.com/2015/12/27/things-that-are-not-superintelligences/) (others have linked more, including Rob Miles' video). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-14 01:38:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think this is basically just the state of the field, unfortunately. However, I don't think you should let this stop you. Peer review is one quality assurance measure (that is very far from perfect by the way). Other (imperfect) ways to assess the quality of a work is to look at the author(s) or citations. Right now, I think you can relatively safely assume that the works from the established labs are worth reading and including in any survey. It's also not really the case that the works you mention aren't peer reviewed: they may not be peer reviewed by independent researchers for a journal or conference, but they're almost certainly peer reviewed within those institutions and more importantly they have been read, built-on, commented on and referenced by the AI safety research community.

I do think the current state of affairs is a bit sad. There are basically no dedicated venues for publication, and related ones (e.g. on AI in the broad sense) are often still not on board with the idea of AI safety. It's such a small field, where most of the work comes from a handful of (big) labs, with very little participation from academia. DeepMind and OpenAI have of course published quite a bit of research, but they're companies and it's not their raison d'être, so if it's hard (like with AI Safety) they may simply not bother. Furthermore, there's relatively little to gain from it, because there are so few AI Safety researchers in the rest of the world: if they want to discuss the ideas in the paper, they can just do so in-house (or with their collaborating partners). Furthermore, OpenAI famously started questioning blindly publishing their work with GPT-2. MIRI is another big player, but neither Yudkowsky nor Soares are academics, and they have adopted a non-disclosure-by-default policy. FHI is part of Oxford university, and I'm pretty sure they're growing (especially with the GovAI lab), but they don't seem to be publishing much (anymore?) either. There are of course academic researchers like Roman Yampolskiy (who replied here), and Stuart Russell's group (which publishes primarily on their own technical solution path), but all-in-all it's not a lot.

I've had some very slight involvement with some of these groups, and they seem to be growing increasingly reluctant to publish things, because they're afraid it might *increase* the risk. So they're mostly just keeping their work within their own small-ish bubble of (often junior) researchers and effective altruists etc. And while I do understand their caution, I think this is a bad situation that prevents the field from growing. Right now, for most academics it's just not very feasible to enter the AI Safety field, even if they'd be interested, and I think it dramatically limits the amount of people working on this. There's an upside to being in a young field though: you can make a large impact. Maybe if you're a bit further into your PhD, you could start initiatives to remedy this situation. In any case, I recommend you reach out to the major labs: I've found them to be very accessible. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-14 01:28:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> I was particularly encouraged by the fact that he thanked José Hernández-Orallo (*the* authority on AGI evaluation IMO) and Julian Togelius (who works on General Video Game Playing and is quite interested in AGI), so I assume they talked some sense into him. He also cites them, Goertzel & Penachin's AGI book and (Legg &) Hutter's work, so it sounds like he dove into the subject a bit more. Also, it just seems to me that writing an article on measuring general intelligence is incompatible with a lot of the claims he made in the old article, so he must have changed his mind.

Also, in case you haven't heard of the author: François Chollet is quite famous in the deep learning community for being the creator of the popular Keras framework. (I don't think this gives him any special insight into AGI, evidently, but it probably does mean he's a smart and talented guy so if he did educate himself in this domain he might have done some good work.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-13 23:16:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay great! Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-13 22:56:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> This looks like interesting research. I do wonder how much religious diversity you'll find here though, and I also don't think subscribers to /r/artificial are likely to be representative of "the public" and their views on AI. Specifically, the people who are here probably know more about AI and are relatively excited about it in a way that the general public probably isn't. It might be better to post your survey in more general groups like /r/SampleSize (which is specifically made for this).

I'm also a little afraid that by telling people this is about religion, it will affect how they answer your survey. They will probably answer what they think their religion would have to say about AI, or what a more pious version of themselves might answer (e.g. me if I was a better Christian). So your answers will probably be more representative of "people's views on their religion's views on AI" rather than just "religious people's views on AI". If that's not what you want, you should probably follow in the footsteps of most psychological experiments and obscure the exact nature of your research. In this case by not mentioning religion in the prompt, and adding more demographic questions to make the one on religion stand out less.

Anyway, good luck with your research! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-12 16:22:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is definitely not just an American thing. In the European universities I've studied or worked at, attendance was virtually never mandatory.

Where I currently work, I'm almost inclined to say there is no "system regarding attendance" (and I think the other places were similar). The system is that your grade needs to be sufficient, and depending on the course this will be built up from exam(s), projects, reports, exercises, etc. You could probably (ab)use this to make your classes de facto mandatory by grading participation or something, but I don't know anyone who does that, and I'm not sure the exam committee would allow it.

When I was studying myself, they would sometimes make guest lectures mandatory to attend (because the university is trying to impress the guest lecturer, or out of respect that they came all that way to teach us). But now I'm kind of wondering about that, because I wanted to do it for the course I'm teaching and was told it's impossible. The advice I received is to tell the students it's mandatory and hope nobody calls my bluff (or just hope enough people show up to not make the guest lecturer feel bad).

Having said that, if a student was making it a point to show up to all the lectures, then missing 3 does not sound like a great brag. (On the other hand, I would also definitely not notice if a student missed 3 classes, because attendance seems to hover between 20% and 80% per class...) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-11-11 15:49:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> I was not impressed with Chollet's take on artificial general intelligence [two years ago](https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec) but this looks interesting. I hope I have the time to read it soon. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-31 02:18:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> Although we could wonder how "being an upload" and "being much faster" would affect this AI's values and their alignment. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-26 11:26:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> Check out the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on this subreddit's wiki.

For more in-depth learning paths, nothing can probably beat going to university and taking an AI-related program (either in your bachelor's, master's or perhaps even PhD). But there are also books and online courses that focus more on the theory, background and foundations than on making the right library calls to build some kind of classifier. Check out the wiki for links.

BTW, it seems like you're already well on your way. Neuroscience/psychology is not a bad background for AI, and you already learned Python, R and linear algebra. Keep up the good work! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-26 11:23:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> I personally haven't heard of Simplilearn, but I think it's a good sign that the program is apparently co-developed by a reputable company like IBM. The program looks fairly extensive, but I'm not sure how much to read into the "double master's" designation. They claim to have 400+ hours of extensive learning, but if I recall correctly one university year is 1600 hours and an AI master's is probably two years. I'm very skeptical that any online course is going to be better at teaching you anything than attending a university in person.

Since you say you're already taking this course, it seems like a good idea to finish it and see what kinds of jobs you can actually get afterwards. Do they have some kind of service to get you a job?

If you're looking for more education, you could of course physically attend an (even more extensive) master's program at some university or even pursue a PhD. I don't think this should be necessary to become an "AI developer" though. For that, you will need to be able to program fairly well, and know how to apply AI techniques, which seems to indeed be the focus of the Simplilearn program.

We also have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on our wiki with some more resources and suggestions on how to get started in the field. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-21 02:18:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> His audience is tech-savvy? I thought the whole point was that they aren't, but they still want to build a face detector in 5 minutes... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-21 00:56:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you want time series prediction, look at narrow AI. Generally speaking, open source AGI solutions are outperformed on any (narrow) metric by narrow AI.

I think the best open source AGI solutions right now are probably OpenCog and OpenNARS. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-21 00:52:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks!

By the way: another thing I usually don't get around to but might be interesting for you is to look at keynotes and invited talks at (large) AI/ML conferences. Unlike most conference presentations (which tend to be highly technical, as they're presenting a paper), these are typically given by some high-level researcher to an audience consisting of people from every AI subdiscipline. It's not *exactly* aimed at laymen, but this might be a nice in-between level if you're very interested in the field. These videos are often posted on the conference website (e.g. IJCAI, AAAI, ICML, NIPS), on YouTube or on the presenter's personal page. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-21 00:27:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> It depends on what you mean by "passing the Turing test" and "most AI problems".

If we have a low bar for "passing the Turing test", such as convincing 30% of (not-great) judges in a short conversation that the AI might be [a child that doesn't speak English](https://en.wikipedia.org/wiki/Eugene_Goostman), then this can easily be passed by a fairly simple chatbot that is not capable of much else. If we have a higher bar, like convincing a competent judge that the AI is an intelligent cooperating English-speaking adult over an arbitrarily long conversation, then this implies that that AI should be capable of many other things as well. In fact, it should be able to solve all problems that an intelligent human could be expected to, as long as they can be communicated over the provided (text) channel.

Would that constitute "most AI problems"? Well, if we can only communicate with text, we couldn't test sensorimotor abilities, which we are currently trying to develop AI for. We're also developing AI to be better than human at many tasks. If an AI passes the TT, we presumably don't know from just that fact whether it's better than human at anything (if it had communicated that it was, then it wouldn't have passed). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-20 23:48:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> As an AI researchers/academics/specialists: no I don't, although I think I might like them.

It seems to me that the information you get from listening to podcasts is entirely different from what you get from reading research papers or having (or walking in on) a conversation with colleagues. Research papers tend to relay very specialized knowledge in a very detailed manner (unless you're just reading them for the results or something; but usually even then). Reading this is absolutely necessary for a researcher. It might actually be what you spend the most time on. So when your friend says he spends his time on this rather than podcasts, it's probably not an issue of podcasts and papers having a similar function, but rather that watching TV, listening to podcasts (or music) and reading papers all take time, and your friend needs his to read.

I also strongly doubt listening to podcasts is like walking in on a conversation between expert colleagues. My impression is that it's usually an interviewer conducting an interview with an expert for the benefit of a fairly broad audience. Conversations among colleagues tend to be even more hyperspecialized and much less filtered than what they write down in their papers (for a slightly broader audience).

But none of that means podcasts can't be interesting for hearing what these experts think in broad easy-to-understand terms. Looking at Fridman's list, I'd probably have trouble reading the papers of many of the people he interviewed (and I certainly don't have time for that). Hearing them summarize and "selling" their work definitely sounds interesting. And I think there's probably also some value in actually hearing their voice.

The reasons I don't listen to them much are not that I don't think they could be interesting. First, I find it difficult to commit that amount of time to listening to a podcast. Second, it seems like a very inefficient medium to me. I would much prefer a written interview (or best-of-both-worlds: a transcript with time stamps) where I can search for and skip to interesting parts (and read much faster than they could speak).

But yeah, if I'd listen to any podcasts, I'd probably listen to these. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-20 23:32:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> > why use one simple word when 100 esoteric ones will do?

While I realize it occasionally goes wrong, the question is actually the opposite: using one jargon word instead of 100 simple ones. These articles are optimized for not wasting the time of their hyperspecialized audience of researchers who might actually do something with that work, at the cost of being nigh-unreadable to a wider audience. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-18 12:04:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm honestly curious how you see that conversation going.

You: Here look at this. I guess we should better sponsor some AI Friendliness research, huh?

CA: What? I have no idea what AI Friendliness is, but I certainly don't see any Artificial Intelligence in this picture.

You: Ah see, Sauron is kind of a metaphor for AI.

CA: I thought Sauron was that eye thing from LotR.

You: Well, actually, in the Marvel comics Dr. Karl Lycos is bitten by mutant pterodactyls, which makes him an energy vampire and when he consumes a mutant's energy he turns into a kind of Pterodactyl-looking dude with increased strength and speed, but he also becomes a supervillain who names himself Sauron because he's a fan of Tolkien.

CA: Okay, so we need to ensure that our AI is not bitten by mutant pterodactyls?

You: Well, no...

CA: Or we need to make sure it doesn't become a supervillain by energy-vampiring the X-men?

You: Not exactly...

CA: Then I don't see how this has anything to do with AI.

You: Well, see, Artificial Superintelligence would be very intelligent and powerful. So much so that it could probably cure cancer. But what if that isn't what it *wants* to do, and it instead wants to turn everybody into dinosaurs? This illustrates that just because someone is very intelligent, that doesn't make them *good*.

CA: First of all, that sounds kinda awesome. Why *not* turn everybody into talking dinosaurs? Dibs on T-Rex! Secondly, it sounds like this Sauron guy wants to do that because he's a literal comic-book villain. I'm pretty sure AI is not going to get its motivations from bad writing and mutant pterodactyls. Just have the programmers program it to cure cancer or whatever you want, and if it does something wrong, press the off button.

You: <gives awesome explanation of the control problem, value-alignment, Omohundro drives, orthogonality, AI boxing, etc. that has nothing to do with this comic strip> </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-17 18:58:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I'm going by the blog post.

You seemed to be going by Clayton's description of him... (possibly because you don't know what AIT is?)

> It makes a claim there is a single solution for AGI, which is totally unproven.

Are you referring to this part?:

> The above equation rigorously and uniquely defines a super-intelligent agent that learns to act optimally in arbitrary unknown environments.

It's the only thing I could find that sounds vaguely like your claim here. He's not saying there's a single solution to AGI though. For AGI it is not necessary to "learn to act optimally in arbitrary unknown environments". In fact, that's impossible (which is why AIXI is incomputable). What he's saying is that it is not possible for another agent to significantly outperform the (incomputable) agent defined by this equation. And since you mention proof: there's a mathematical proof for that in his book.

The reason you probably haven't heard of Hutter or AIXI is that all of this stuff is purely (but rigorously!) theoretical, and probably because most AI research (which you probably know more about) doesn't have much to do with AGI (unlike Hutter's). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-17 18:38:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> How is that relevant? Hawking's expertise is not in artificial intelligent. Hutter's is. He's a full professor (and now also senior researcher at DeepMind) who has spent his entire career on (universal) artificial intelligence.

I don't really agree that his mathematical top-down approach is the most promising path towards AGI, but it's a serious approach with strong theoretical foundations. And unlike most AI researchers, he's at least thinking about / working on AGI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-17 18:27:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> Written where?

In a (purely) static environment nothing changes while the agent deliberates / thinks about what action to take. In dynamic environments things do change: while the agent is thinking, other agents continue taking actions and the world moves on.

Turn-based games are (pretty) static. In poker, when it's your turn, you can pretty much think as long as you want and not much will change: it doesn't effect what cards people have or will be flipped, it doesn't affect your score directly, etc. The same is actually true for a lot of turn-based games, including chess. However, if you play chess *with a clock*, then something *does* change while you're thinking. Presumably it's called semistatic/semidynamic because most things aren't changing (i.e. the board positions). (Another definition I've heard for semidynamic is that it means the state of the world doesn't change, but the rewards do.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-16 14:39:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> In what sense is general-purpose intelligence a "solved problem" though? It's certainly not solved in the sense that we know how to actually build it. AIXI is incomputable, and even computable approximations are not currently practical (or performing particularly well). I think it's better thought of as a formal definition of general/universal intelligence than as a architecture or blueprint for an actual AGI system, because all it really says is that there's a learning component and a planning component and if both are optimal than your actions are also optimal. It doesn't really "solve" anything.

However, I think it's still useful as a definition. It's not beyond reproach (e.g. one could disagree with the chosen prior or say that the whole point of intelligence is to make the best decisions under an assumption of insufficient knowledge and resources), but it's useful in some contexts (especially when reasoning about superintelligent AI). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-16 14:17:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> Fun fact: the NSA already has a surveillance program named Skynet, and it [may be killing thousands of innocent people](https://arstechnica.com/information-technology/2016/02/the-nsas-skynet-program-may-be-killing-thousands-of-innocent-people/) (according to ArsTechnica). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-16 00:50:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> > > The purpose of this subreddit is to be a working discussion ground for people who may hold dramatically different beliefs. It is to be a place for people to examine the beliefs of others as well as their own beliefs; it is to be a place where strange or abnormal opinions and ideas can be generated and discussed fairly, with consideration and insight instead of kneejerk responses.

> The important words here are "people who may hold dramatically different beliefs". The subreddit doesn't work unless we have that. If we end up with a monoculture of one belief set, or even a polyculture that eliminates one belief set, then we've got a problem on our hand; a problem that defeats the entire purpose of the subreddit's existence.

I realize this is a bit weird to say since you probably wrote the Foundation, but I feel like you're kind of getting it wrong here. There is nothing in there about culture, or needing to have every single viewpoint represented. What it says is that this is a place where any idea *can* be generated and discussed with people who *may* hold dramatically different beliefs. There is talk of a discussion ground, which like e.g. a boxing ring is a *place* where people may come but are not a part of. The boxers are not a part of the ring, but the ring comes with a set of rules to ensure the "argument" happens in a certain good/productive/non-injuring way.  Everybody may step into that ring, and enjoy the rules' protections. If there are people who decide off their own accord that they have no interest in stepping in (e.g. because they don't like sparring or boxing or because they only want to do so if the audience cheers them on or they're allowed to bring a gun), then this doesn't mean the ring has failed at is purpose of providing a battle ground where people who wish to do so can duke it out.

---

Leaving this metaphor behind, I do think there is value in having a lot of viewpoint diversity here. Even if I think a particular viewpoint is wrong (whether that is homeopathy, creationism, or idpol), I would still like to understand what are the best arguments of intelligent people who adhere to it. Furthermore, if they are defeated, there's a chance it will change the mind of the original poster or (a bit less unlikely) onlookers, provide a better understanding and arguments to readers, and at least give some insight into why a certain portion of humanity believes what they do.

One question is what you see as the ideal proportion of viewpoints here, and another is how bad it is if we deviate from that. If some reference population (e.g. the US) is 50/50 Vedist/Olmec, it's presumably bad if /r/TheMotte is 100% Vedist, but is it also really bad if it's 75% Vedist and there are still quite a few (25%) Olmecs? What if the reference population is 90% Olmec, would it be bad if /r/TheMotte was majority-Vedist?

Of course, there aren't just Vedists, Olmecs and Ashurists; there are many more ideas, viewpoints, tribes, etc. Some are likely overrepresented while others are underrepresented here compared to some other population. I imagine the average age of /r/TheMotte posters is above that of Reddit as a whole and below that of the US. There are (I think) proportionally many more men, high IQ people, atheists than in e.g. the US. Again, what are good ranges of proportional representation here? Or is it perhaps just important that anyone can participate if they want to?

If you want to tackle the over/underrepresentation of certain groups, you will probably need to figure out the reason for their over/underrepresentation, which may be different depending on the group/viewpoint. A lot of people have been speculating about the reason there are supposedly not many Olmecs here. In the below list "Olmec" just refers to an underrepresented group/viewpoint, which may be different in each bullet point.

Olmecs might be underrepresented because...

* Olmecs tend not to value the Foundation
* there are better places for Olmecs to get (more of) whatever value they could get here. Or conversely, this is the best place to get that value for Vedists.
* Olmecs who come here tend to (quickly) become Vedists after their views are challenged (e.g. because Olmec ideas are false)
* Olmecs who come here tend to quickly leave again / become lurkers, because they become less convinced of their Olmec ideas after they're challenged
* participating here lowers their status in the eyes of people they care about (likely other Olmecs)
* they can't stand not being the majority / dominant force or having special privileges
* they feel treated unfairly by other users or the moderators
* they don't like being dogpiled
* they can't find / haven't heard of this place
* etc.

Each reason probably calls for a different type of response, and for some reasons it may be undesirable to respond at all. For instance, it seems like you don't want to compromise on The Foundation. I also think you don't necessarily want people to change their mind (especially about false beliefs) when they "examine the beliefs of others as well as their own beliefs".

---

To combat dogpiling, I think it would be good if we could develop a norm of not repeating arguments. If someone repeats an argument already made down thread, they should be called out on it by mods or other users (or you could try to enforce it as a rule, but that may be difficult). I prefer this to setting a fixed number of allowed replies, because that's easier to game and might exclude important arguments/ideas (maybe even from another underrepresented group), and not repeating points would actually benefit the discussion even if representation isn't an issue. The thread is already quite long.

Aside from that, I think there could be more crackdown on low-effort sideswipes. These are easy to make if you're comfortable somewhere in the knowledge that you're in the majority and most people agree, but it's probably pretty discouraging to the "victims".

Finally, I think recruitment and good examples could play a big role. But this is a difficult problem if the issues that prevent a viewpoint from being represented here are still present. Showcasing quality contributions may help.

Maybe there are also ways of offering more value (perhaps even due to the high proportion of opposition). I notice that /r/changemyview is pretty popular, so there is apparently some demand for having your views challenged. Well, we could very easily provide that service to Olmecs as well, with the (potential?) advantage that the average IQ here is apparently 138 and the level of discourse is very high. So maybe offer a CMV thread / functionality and advertise it? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-15 18:27:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> Are you familiar with AI Impacts' work on predicting [human-level hardware](https://aiimpacts.org/category/ai-timelines/hardware-and-ai-timelines/human-level-hardware/)? I don't think any of the articles directly answer your question, but they may be interesting to you. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-12 18:15:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not a job market expert, but if you want to get into *research* it seems like a good idea to me to get a PhD. There are of course companies who do research as well (R&D typically is quite different), but I think many of them often require a PhD as well. Some will say they don't, which may be technically true, but if you look at e.g. Google Brain it seems that most people still have PhDs.

Aside from giving you credentials, a PhD program is also supposed to teach you to do research, lead your own multi-year (research) project, build a professional network and build a portfolio of published papers. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-11 20:13:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's in the article you linked, but I feel like it's important to point out that these prank calls occurred in early August. I don't have time right now to listen to the two phone conversations, but let me say I'm also generally skeptical of cherry-picked excerpts, and that I wouldn't be too surprised if a politician takes a different tone to the public (on Twitter) than he does in a private conversation with a foreign secretary of defense. As long as he's not telling Turkey to go ahead and attack those Kurds, I'm not sure there's a huge contradiction. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-11 19:18:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm glad we have a "speak plainly" rule here on /r/TheMotte, because I dislike pieces like this where it's unclear to me what the author's actual point is. Is he trying to say human judgement isn't perfect? I doubt anyone would disagree. Is he trying to say we need to do better in measuring and improving the quality of (human) decision making? I agree. Is he ridiculing the notion that we need to be cautious when applying AI in the real world because the human reasoning it's replacing is (also) flawed? Hard disagree.

An additional problem I have is that it's not clear in what hypothetical world this piece is supposed to be written. "Human decision-making" is apparently an emerging technology, so I imagine decisions are being made by AI. If they are being made by a benevolent AGI/ASI, then I agree with the article that we should be skeptical about replacing than with human decision-making. What would be the upside? Of course, the main concerns about AGI/ASI are that we don't know how to actually make it beneficent. Or are decisions being made by the kind of narrow AI we're currently considering for replacing human decisions in our world? In that case I'm going to say the premise is entirely incoherent, because the narrow AI systems we currently have couldn't possibly be in charge of the decision-making for a civilization that's even remotely similar to ours.

Of course, in settings where we are currently considering to apply AI (or data-driven reasoning) we are doing that because those specific settings seem to lend themselves to this reasonably well. And in many of those settings, it is indeed a fantastic idea to apply AI, and it would be stupid to refer back to (purely) human decision-making. But even in those settings humans still decided to apply the AI, how to do so, probably when to do so, and how it works.

The specific points made factor into the decision about whether it's better to use human or automated reasoning in a particular setting. Humans are definitely biased, and that's indeed a potential reason to want to replace them as the decision makers. The main point with AI is that we shouldn't be under the illusion that it's application isn't biased (in similar and different ways and for similar and different reasons). Furthermore, it's important to note that with AI decision making, you can scale things up tremendously and you open up new possibilities. Is it better to make a few decisions that are more biased, or a lot of decisions that are slightly less biased and all in the same way? It probably depends on the setting, but in the latter case it definitely seems bad if people can just absolve themselves of responsibility by hiding behind the supposed objectivity of a machine.

That brings be to consistency, which is mostly just good if you're consistently correct. But would we prefer a consistently wrong decision-maker to one that is less consistent but occasionally correct? Again, it probably depends. There are advantages to consistency (e.g. easier to debug / predict), but you lose out on diversity of decision making. Again, scale is important here. If my loan gets rejected because the decision-maker had a bad morning, or hates CamelCase, or doesn't believe in my amazing business idea, I can go to another human (or the same one on a different morning) and that error might not get repeated. If everybody uses the same algorithm that just happens to get my case wrong, I'm screwed.

Also, with regards to tranparency and interpretability, humans are actually amazing at explaining their decisions in a lot of cases. They are not always correct, but they can at least do it. They can construct an understandable story which causally reasons from premises to conclusions and answer questions about it. This is a great thing that we use extremely often, typically coupled with holding people responsible for their choices (which you can't do with AI), so if adding AI to mix screws up our ability to explain why important decisions were made, that is indeed a cost.

I work in AI, so obviously I'm not against it. There are definitely settings where AI bias is less severe than human bias, where interpretable AI can be used or interpretability is less important than performance, and where consistency matters. But it's not crazy to ensure that we are actually in such a setting before investing and changing workflows that have existed for a long time. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-11 13:31:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know what an EPQ is. From Googling a bit I gather that it's a 5000-word document you have to write at some point in high school?

I'm not sure there's really a *need* to tie multiple of your questions together, because you could probably write a book about each individually. Summarizing that in a 10-pager is of course also possible, but it probably helps to focus.

> How does it differ from human intelligence?

Potential titles: "How does AI differ from human intelligence?", "Human and Artificial Intelligence", or "Similarities and Differences between Human and Artificial Intelligence"

A potential pitfall here is failing to recognize how broad AI is. For instance, it is not just neural networks. How AI "thinks", how it works, and what type of behavioral differences you might expect from that will depend a lot on the used techniques and how they are put together. Covering the full breadth of that will be challenging, but it's better than pretending it doesn't exist. Of course, if you just want to write about neural networks (or something else), you can do that, but then you should probably title your project something like "Biological and Artificial Neural Networks".

> How will it effect the economy?

Potential titles: "How will AI affect the economy?" or "(Socio)economic impacts of AI"

It will likely be a good idea to focus this a bit more. For instance, on a particular section (e.g. healthcare), application (e.g. self-driving cars), technique (e.g. GANs) or issue (e.g. employment or inequality). It may also be wise to limit yourself to a certain period (e.g. impacts in the next 10/30/100 years).

You should do this for any project you end up doing, but you should also be clear about [what you mean by "AI"](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_narrow_ai_or_general_ai.3F). Narrow AI has already existed for a long time and already has an impact (which may of course grow or change in the future). Are you going to talk about (near)-future implications of the AI we already have, or of some kind of AI you anticipate that we might have? And will this be a simple extrapolation of current capabilities (e.g. AI will get better at facial recognition and language generation) or do you want to look at the economic implications of human-level AGI (and what assumptions do you make about that? e.g. will it stay at a human level for long or quickly exceed our intelligence?).

> How will it alter how society interact and communicate with each other?

Potential title: "Effects of AI on social interaction and human communication"

I don't have much advice about this one, but it sounds like an interesting topic. I heard that children with a voice assistant (e.g. Alexa) might learn to be more commanding of other humans, because they think the way they interact with Alexa is normal. And there have also been concerns about the (usually) female gender of these assistants, and how they should e.g. react to (misogynist) verbal abuse, because of how this might affect perceptions of women. There are similar(ish) discussions about sexbots. And of course there's the issue of how e.g. Facebook manipulates your feed to keep you on their social media platform (ostensibly interacting with your "friends"?).

> What positives can it bring to our futures?

Potential titles: "Future Applications of AI", "Positive (Future) Impacts of AI" or "AI for Good"

There's a risk that only mentioning the positives of a technology can be perceived as one-sided. I don't think focusing on opportunities is bad, but it's going to sound weird if you're giving a glowing description of the nice things e.g. facial recognition can do for us without also mentioning the controversy surrounding it.

Another potential pitfall might be that you end up with a fairly random list of (potential) applications of AI. It's probably best to structure it around some existing framework or taxonomy. For instance, you could describe how AI might help with the UN's Sustainable Development Goals. Here you could also narrow your topic to one goal, or one subset/application of AI (e.g. multi-agent systems, video surveillance, or self-driving cars).

> Why is it a bad/good idea?

Potential titles: "Opportunities and Risks for (Developing) AI/AGI"

Here you need to be a bit clearer about what you mean by "is it a bad/good idea?". What is "it"? AI or AGI? People have talked about this in the context of AGI at length (see /r/ControlProblem). For narrow AI it may be better to focus on more specific applications or techniques, like facial recognition or generative technologies (deepfakes). If you want to talk about regulations to lead AI development and application in good directions, I'd put that in the title.

> How will it change technology as we know it today?

This strikes me as a very vague question and I don't know what to do with it.

Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-09 19:34:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> > However, I still think there are some significant steps that can be taken to fix some needlessly convoluted design decisions and vocabulary.

What are these things?

---

I like your analogy with mathematical notation, but I think there's more reason to change mathematical notation for accessibility than video game accessibility. In both cases the argument why this isn't happening is that the people in charge and the core audience isn't really helped by this, and that it would only help the newcomers who don't have any power over this.

But it seems to me that in mathematics awkward notation is a potential problem for all newcomers. Depending on how you look at it, that's either all schoolkids or all STEM college students, and it includes the future of the field. But with video games it's not really clear that there are accessibility problems to the most common newcomer: young kids. It's not even really clear to me that the vlogger's girlfriend would have the problems she ran into if she had either been helped by her boyfriend, or if she had actually wanted to get into video gaming (which I assume would translate into looking up online information and more experimentation).

If there was an actual accessibility problem in gaming for all newcomers, that would definitely be worth addressing. But it's probably not worth it to spend any efforts to reel in the infinitesimally sized demographic of adults who have never gamed, who don't really want to, and who are going to do so without any help. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-09 18:27:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> This reminds me a little bit of the AI paper on [Investigating Human Priors for Playing Video Games](https://rach0012.github.io/humanRL_website/) where they alter a Donkey Kong-like video game by variously removing visual hints and other human priors (e.g. gravity's direction) to see how important they are to our ability to quickly play the game. It seems to be more about general human knowledge/priors than video game specific knowledge, but your video is interesting to me in the same way: to show a glimpse of the prior knowledge we apparently bring to these situations.

I'm not sure it means much for the development/design of video games in terms of how accessible they should be though. The set of adults who want to buy your game, but have never played any video game, who are going to play it without help from a friend and with no real motivation to experiment or do research must be quite small. I think someone who isn't in an experimental setting like this would at least ask a friend for help or look online, at which point I think most games become adequately accessible.

Some forms of media consumption may be inherently more accessible though. Watching video/TV/movies or listening to music typically just requires sight and hearing (and perhaps the ability to understand language). For reading books, most people have the baseline skill level to read just about anything, although this takes some effort to acquire. We might view the vlog as the equivalent of making an illiterate person read his favorite books, which would have gone much worse.

I would be surprised if lack of accessibility is the main problem keeping people out of video gaming. I won't deny that getting into the hobby can be difficult, but I'm not sure this is more true than for e.g. reading (for the illiterate) or playing a sport. Hell, even getting into a new musical genre can be difficult to people (even if they have perfect hearing). I suspect that the main problem is that most people who aren't gaming already simply aren't that interested. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-10-08 11:48:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> Apparently: https://en.wikipedia.org/wiki/Standard_social_science_model

But it seems to be mostly used by opponents of the SSSM. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-30 11:31:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> I agree with most of what you said, but when choosing the "tactically best" martial art, I think you should also take into account what kind of situation you're trying to optimize for. All of the sports you mention are optimized for some kind of unarmed one on one fighting in a ring with a referee and usually some particular (protective) gear.

If you anticipate being in situations where there might be surprise attacks, multiple opponents / bystanders and/or various kinds of weapons, I think you're going to at least want to augment your BJJ training with that in mind. You don't want to be grappling with some guy on the floor while his buddies kick your head in.

This is where military/police self-defense arts (like Krav Maga) are supposed to shine. So I think I'd incorporate that into my training, at least to be more prepared for more realistic situations. It is probably also a good idea to incorporate some kind of stick and knife fighting (especially defense against knives). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-30 10:57:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Basically, i would say that spear-like thing is much more available than a sword, yet HEMA seems to basically be all about swords?

Matt Easton (the bald guy in your video) runs a HEMA club in London and also a YouTube channel (Scholagladiatoria) that I've watched quite a bit. I think he's primarily a sabre instructor (and his wife specializes in some other kind of sword), but he talks about using (or trying out) lots of different weapons in his club. In his videos he also points out regularly that swords were *side*arms for pretty much all of history, and in an unarmored one on one fight in an open field, a spear is a vastly superior weapon. There's even some discussion about whether a quarterstaff might be better than a sword too (which is presumably more readily available than a sword or a spear).

I don't know much about this, but my guess is that you're right that swords are (over)emphasized though. Probably due to how we think about them in our culture, and possibly also because they were used a lot in dueling (which is closest to how we practice most of our other martial arts). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-26 18:52:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> Getting someone to switch sides is not the only form of persuasion. You can also persuade people who already kind of agree with you to actually show up and vote (fire up the base), or persuade people who were going to vote for your opponent to not bother (smear campaign). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-26 10:17:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> Personal attacks are not allowed here. Please respond to the other person's arguments politely or don't respond at all. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-25 09:43:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> I actually thought Germany was reasonably good in this regard. Maybe there aren't a lot of programs that are actually called Artificial Intelligence (or Künstliche Intelligenz) like there are in e.g. the Netherlands, although [here](https://www.daad.de/deutschland/studienangebote/international-programmes/en/detail/5438/) seems to be a program at TU Berlin. Here is a [blog post](https://simplyki.de/wo-kann-ich-kuenstliche-intelligenz-studieren/) about where you can study AI in Germany (sorry I didn't mention Data Science) and a [Quora post](https://www.quora.com/Which-are-some-top-universities-in-Germany-to-pursue-a-masters-in-robotics-and-artificial-intelligence) on places where you can get a master's degree. I also know that Kai-Uwe Kühnberger at Osnabrück and Ute Schmid at Bamberg are at least somewhat interested in AGI. Also, Germany has DFKI, Cyber Valley, and a bunch of brain and/or informatics related Max Planck Institutes (most notably in Stuttgart and Tübingen).

So I think that there are actually quite a few opportunities in Germany if you dig a bit deeper. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-24 18:21:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> AGI researcher. See the [Getting Started article](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on /r/artificial's wiki. There are links to people and groups working on AGI. You could also try working at e.g. DeepMind or OpenAI.

To get there, it will probably help a lot to have a university education in a related topic. If you could major in AI or perhaps ML, that would be best. Since this is rare, computer science is the go-to for most people, although there are other options like mathematics, cognitive science, neuroscience, etc. If you can find a robotics program, you should look at it really carefully to see if it's mostly about hardware (not so useful IMO) or AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-23 13:09:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> This was already discussed in last week's CW thread [here](https://www.reddit.com/r/TheMotte/comments/d4w6xl/culture_war_roundup_for_the_week_of_september_16/f0kopgq/) and [here](https://www.reddit.com/r/TheMotte/comments/d4w6xl/culture_war_roundup_for_the_week_of_september_16/f0lvwdu/) (and his remarks pre-resignation [the week before](https://www.reddit.com/r/TheMotte/comments/d1o5qu/culture_war_roundup_for_the_week_of_september_09/f08evwz/)).

The e-mails you mention appear to be just part of the story. They probably focused woke people's attention on Stallman, who then went digging and found many more "misbehaviors" that they think he should be removed for. This includes more remarks about sexual assault and sex with minors, and inappropriate behavior that has allegedly caused swaths of women to reconsider their career in CS.

My personal view is that most of Stallman's expressed views and apparent behaviors are not that bad, but I don't have time to go into that at the moment. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-20 12:43:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for posting this! I must say that I haven't read past the introduction because despite you calling it "concise", I'm afraid I don't have to spend 35 minutes on it right now. To be honest, I hadn't really heard of document embeddings (I'd heard of word embeddings though), but it certainly looks interesting. I don't have much to add, but I came across this as I'm working through a backlog of not-reported posts on /r/artificial (I'm the mod), and I thought this post deserved more engagement than it got here.

It looks very interesting, thorough and useful, and I want to thank you for posting it! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-18 14:10:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> One common argument that AI researchers give in favor of believing we will one day be able to build (super)human-level artificial general intelligence is that computers can simulate physics to a pretty much arbitrary level of precision, and we believe that human intelligence emerges from a physical process. It's only a matter of figuring out how to do it. You can think of this as the "copying" or "whole brain emulation" route, which is useful to think about as a kind of proof of concept. But many of us believe that what is needed for intelligence (i.e. problem solving ability) can be implemented at much higher levels of abstraction as well. We can point to some of the successes in the area of AI for this.

Just to play ~~devil's~~*your mother's* advocate for a second: When we ask people how long it would take for humans to develop human-level AI, most respond with an estimate with the order of magnitude of "decades" (although some also say "never"). But what if we'd ask how long it will take dogs to develop dog-level AI, or ants to develop ant-level AI? The answer should probably be "never". (You might imagine that in a couple million or billion years dogs could evolve to become intelligent enough to do that, but then they'd no longer be "dogs" any more than we are "homo erectus" or our earlier ancestors. And it's probably equally likely that the species goes extinct over that period of time.) So if dogs, and ants, and other animals are not intelligent enough to build <animal>-level AI, could it perhaps be that humans are also not intelligent enough to build human-level AI?

---

These were arguments for (human-level) AGI, but the arguments for Artificial Superintelligence (ASI) tend to go beyond that. It depends a bit on what we mean by ASI. A lot of people argue that once you have human-level AGI, it will be trivial to make it superhuman. For instance, you could just give it more/better hardware, and then more knowledge, and it probably wouldn't need to sleep, etc. If ASI is being "humanity-level" intelligent, then this could perhaps be easily achieved by networking a sufficient number of AGIs together; after all, humanity is just a bunch of humans networked together and their communication is likely much less efficient that AGIs.

If ASI is taken to be be some near-infinite God-tier level of intelligence that can only be achieved through recursive self-improvement resulting in a singularity, then the standard argument against this is that there will likely be bottlenecks. There will be diminishing returns to self-optimization at some point, as well as local optima. It's not a given that an AGI could *qualitatively* improve its reasoning much anyway. Furthermore, a common argument is that a lot of what we do, including learning and acquiring knowledge, requires real-life interactions that are inherently slow. It doesn't matter how fast you think: a 60-year longitudinal study of the effects of smoking on human health will still take 60 years. And figuring out how different chemicals react together also takes time. And building stuff and gathering materials also takes time. And so on. (Personally I think this only applies to some areas, but much less so to others, like most mathematics, programming and computer science.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-18 01:13:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> A while ago I came across the writings of Lee Jussim (who is a co-author on this paper) on PsychologyToday and I've been meaning to post about them here. As a psychology professor at Rutgers university who seems to have written a lot about liberal bias in his field, I think he competently represents a viewpoint that many on this forum share. I recently found him because he wrote a [series](https://www.psychologytoday.com/us/blog/rabble-rouser/201906/are-scientists-biased-against-women-scientists-part-i) of [posts](https://www.psychologytoday.com/us/blog/rabble-rouser/201906/are-scientists-biased-against-women-scientists-part-ii) on [gender bias](https://www.psychologytoday.com/us/blog/rabble-rouser/201906/scientific-bias-in-favor-studies-finding-gender-bias) and the field's bias with regards to studying it, but he has a lot more interesting [blog posts](https://www.psychologytoday.com/us/blog/rabble-rouser) on what's wrong with (social) psychology, gender, replication, and Damore's memo. He even has his own [Rules of Engagement in Controversial Discourse](https://www.psychologytoday.com/us/blog/rabble-rouser/201607/the-rules-engagement-in-controversial-discourse) for his comments section that reminded me of the rules at r/SSC and now here.

I'm sure he has written stuff in other venues as well (e.g. Quilette and academic journals) and that there are probably others like him that may be even more interesting (maybe Duarte?), but I happened to come across him at PsychologyToday and wanted to share.

I don't have much to comment on the article you posted. I supports my own views and perceptions, and I'm glad it got published and got cited 364 times so far. I'm also happy that it apparently still possible for people like Duarte, Jussim and co-authors to publish about these views without getting "canceled" from either their university positions or (in Jussim's case) a blog like PsychologyToday. This suggests that there is still at least some political diversity and/or tolerance, which is great. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-16 08:42:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> > What is ethics but sets of different systems suggesting different behavior in the face of choices **so as to create the best reality possible in which for humans to live?**

Your definition -- specifically the bolded part -- does seem to presuppose consequentialism, since it is explicitly about a desired consequence that "ethics" is working towards. This is not a definition of ethics you'll see philosophers use. It would actually be more accurate to just chop off the bolded part. [According to IEP](https://www.iep.utm.edu/ethics/) "[t]he field of ethics (or moral philosophy) involves systematizing, defending, and recommending concepts of right and wrong behavior". Your definition seems to already (partially) provide the answer that "right behavior is that which leads to (i.e. has the consequence of) the best reality possible in which for humans to live". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-11 18:08:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> According to [this](https://medium.com/politics-ai/an-overview-of-national-ai-strategies-2a70ec6edfd) there is a *Declaration on AI in the Nordic-Baltic Region* released by Denmark, Estonia, Finland, the Faroe Islands, Iceland, Latvia, Lithuania, Norway, Sweden and the Åland Islands, but the link doesn't work.

Edit: [found it](https://www.norden.org/en/declaration/ai-nordic-baltic-region) after searching for 3 more seconds. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-11 01:15:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Anyway, I guess these researchers are pretty smart, so hopefully they do realize the potential dangers of this, and are careful.

Unfortunately the dismissal and ridicule that warnings about A(G)I Risk have received from many high-level AI/ML experts (who are undoubtedly very smart) makes me not so comfortable.

As for Hawkins in particular, he was not concerned [in 2015](https://www.vox.com/2015/3/2/11559576/the-terminator-is-not-coming-the-future-will-thank-us) ([here](https://lukemuehlhauser.com/reply-to-jeff-hawkins-on-ai-risk/)'s a reply by Luke Muehlhauser). Of course, that's 4.5 years ago, so he may have changed his mind (I wasn't instantly on board with Bostrom et al. either), but I don't see anything to indicate that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-11 00:21:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> On the other hand, isn't a big part of the value alignment problem the question of how to model human values (which are involved with instincts, drives, etc.)? I agree we shouldn't make a souped up artificial human brain, for the same reason that I wouldn't really trust most humans with a 10,000 point IQ boost (although I think it'd be better than a paperclip maximizer). But being able to make  a computational model of the human motivational system seems like it could also be a huge step towards safe AGI. I do acknowledge though that there are dangers to this approach as well.

(But as Ascendental pointed out, Hawkins isn't really modeling the motivation part of the brain.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-11 00:17:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> Modelling it on a computer may help with that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-11 00:14:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is really just about [wireheading](https://wiki.lesswrong.com/wiki/Wireheading) or reward hacking/corruption in general, which are well-known phenomena that have been discussed regularly in AI Safety circles. There was a recent paper on it [discussed briefly here](https://www.reddit.com/r/ControlProblem/comments/cxjju6/categorizing_wireheading_in_partially_embedded/). Tom Everitt also published about [how to avoid it](https://arxiv.org/abs/1605.03143). I don't think this is a solved issue, but I'm not at all convinced that superintelligence must *necessarily* do this. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-09 00:24:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm curious why you think this is unbelievable. Ron King is one of the best players of all time. I had some trouble finding a good source on his [Elo rating](https://en.wikipedia.org/wiki/Elo_rating_system#Mathematical_details), but according to [these slides from an AI class](http://ais.informatik.uni-freiburg.de/teaching/ss11/ki/slides/ai06_board_games_handout_4up.pdf) it used to be 2632. (If you don't believe that, the current [top checkers players](https://www.fmjd.org/?p=rating) have an Elo of 2400-ish.)

I can't find a reference to whom he played, but having been involved in simultaneous chess matches as a kid, I think it's plausible that it was basically "random amateurs". For some reason I'm getting really bad Google results for "amateur Elo checkers" (most results are actually about chess), but it appears that a beginner might have an Elo between 500 and 1500. If we assume all opponents had an Elo of 1000, and King had 2632, the odds of him winning all 385 games should be about 96.8% (put "(1/(1+10\^((1000-2632)/400)))\^385" into Google). Even if we assume 1500 opponent Elo ratings, the odds are better than chance at 56.6%.

Of course, he's playing simultaneously so we should probably rank him a bit lower, and I'm sure there are other wrong assumptions here, but it seems okay to me as a ballpark estimation and that  for most reasonable numbers you could put into this equation a 100% win rate is not out of the question. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-06 11:38:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> I came here to post AI Impacts as well, although I prefer [this page](https://aiimpacts.org/ai-timeline-surveys/) which has a lot more surveys than just the one by rationalists. AI Impacts also has [more articles](https://aiimpacts.org/category/ai-timelines/predictions-of-human-level-ai-timelines/) on the topic, some of which discuss the (lack of) accuracy of expert predictions. Personally I don't value these predictions much, except to argue that there is 1) huge uncertainty and 2) a significant percentage of experts believes the control problem is important. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-06 10:34:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> The USPTO currently has an outstanding [request for comments](https://www.federalregister.gov/documents/2019/08/27/2019-18443/request-for-comments-on-patenting-artificial-intelligence-inventions) on patenting AI inventions until October 11. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-05 09:33:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> > on latest Fridamn podcast

I assume you mean [this one](https://lexfridman.com/yann-lecun/) where Fridman interviews Yann LeCun?

I think I mostly agree with your comments. Humans have extremely powerful ways of teaching each other, and this facilitates very quick learning. Language plays a huge role in this.

I don't think this necessarily means AGI approaches need to start with language. In fact, if they do, this runs the risk of lacking humanlike grounding and understanding. The path that nature/evolution took to human-level general intelligence started with animals who did not have language, but still seemed to exhibit general intelligence in the sense that they could solve complex problems in an ever-changing complex environment (i.e. the real world). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-05 09:08:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> There is a bit of a difference between "regular" or "scientific" AI (I'm not sure what to call it) and game AI: the goal for AI is typically to perform as strongly as possible, while the goal for game AI is to present an interesting and fun challenge to the player which often means exposing some flaws or not playing at the highest level. The people at /r/gameai or /r/gamedev might know more about game AI.

Of course, there's going to be some overlap, and for "regular" AI, you could get started on [our wiki](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). Game-playing AI often involves tree search (e.g. minimax or Monte Carlo tree search) and sometimes reinforcement learning, evolutionary algorithms and/or neural networks. These are all general-purpose approaches (although there are ways to add domain knowledge). You can also make a rule-based system or decision tree where you basically program the AI directly. This is nowadays a bit less popular in AI research, but I heard it's quite common in game AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-05 08:16:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> It sounds to me like this is *probably* pretty impressive incremental progress and it's cool that we can do this now. But what articles like this really need is a better indication of task difficulty, and ideally something like a graph of AI performance on it over the years. Then we can see if the current system represents a sudden jump in performance, or actually it's just incremental progress.

The fact that it's an 8th-grade test of course invites the comparison to human children, but this is completely invalid. Nobody who is involved in AI would claim that this system is as smart as a 12-year-old. These tests are made for humans: they assume the taker has human(child)-level intelligence and make the questions difficult, *given that assumption*. Even though AI is taking the same test, it's actually solving a completely different problem. It's assumed the human children can read the question and understand what it's asking --- they actually don't get any points for that --- and the problem is to compute the answer given that understanding. Getting the answer is typically rather simple for an AI: it's great at calculating stuff and looking up knowledge in a database. The main problem for the AI is reading and understanding what the question means, so that it does the right calculation / database lookup.

What I just wrote oversimplifies things a bit as well (it's not always trivial to find the answer, e.g. if it involves more elaborate logical or natural language reasoning), but it illustrates why the scores can't just be compared to humans. That doesn't mean it's not a good test or passing it isn't impressive. It just means we can't naively judge just how impressive it is. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-05 07:58:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-04 02:35:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> It actually used to show 0%. I think OP was just mistaken. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-09-03 12:38:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> Linear and logistic regression are very similar. The only difference is that with logistic regression you have an additional step where you apply the logistic function to the output (your first formula).

To calculate the output that corresponds to a particular multidimensional input, you multiply each input dimension by the corresponding weight and add everything together (see your second equation). Another way to say the same thing is that you take the dot product between the weights and your input vector (ϴ^(T)x). Vectors are one-dimensional matrices, so this is where the matrix math comes in. For linear regression you're done at this point, and for logistic regression you just have to apply the logistic function (your first equation) to the output.

Of course, the question is: how do you determine the weights? This depends on your chosen learning algorithm (and the cost function you want to optimize), which you didn't really mention. Linear regression can actually be solved analytically using the [normal equation](https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/) where you don't need a learning rate. Note that instead of multiplying the weights by each individual input vector in your training set (ϴ^(T)x) you can also consider your entire training set X at once, where X is just the matrix you get when you take all input vectors together. This is done in that article on the normal equation.

Unfortunately *logistic* regression cannot be solved analytically like this, so you may want to use something like (stochastic) gradient descent (SGD). With SGD you look at one input vector at a time, compute the output, see how much it differs from the target output (i.e. what you know to be right from your data set), and adjust the weights so that next time the difference/error/cost will be smaller by calculating the derivative (i.e. gradient) of the error w.r.t. each weight. The learning rate `alpha` determines how large this weight adjustment should be each time, and is usually a number much smaller than 1. So you don't really iterate `alpha` times over your data set. It actually makes more sense to iterate `n/alpha` times where n is some positive number (but this is really just a rule of thumb and not set in stone). Maybe this helps: https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/ </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-28 20:28:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Michio Kaku explains

Are you missing a link where he explains this? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-27 19:56:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, for this and the misogynism below I'm banning you for 3 days as well. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-27 08:55:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't personally attack people. Banned for 3 days. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-27 08:55:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't personally attack people. Banned for 3 days. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-27 08:55:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't just call someone or their thinking simplistic. If you think that, simply ignore it, say you disagree, or ideally explain why you disagree. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-27 08:55:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> Please keep things nice. If you don't understand a question or why it's asked, just ask for clarification and don't call it silly. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-27 08:55:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't personally attack people. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-27 08:55:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't uncharitably paraphrase people and assume they're trolling. If you don't want to make a substantial reply, just ignore it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-27 08:55:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> Don't personally attack people. Banned for 3 days. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-20 19:40:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> The goal of k-NN is to guess certain unknown aspects of something, given certain known aspects. For instance, you might know the address of a certain house and you want to guess the value/price (which is currently unknown to you). For k-NN you need a data set of examples where you know all of these aspects (i.e. the address *and* price of a set of houses), and the basic idea is that if two things are similar in terms of the known aspects (i.e. the address), then they are probably also similar in terms of the unknown aspects. For instance, the price of a certain house is probably very similar to the price of their neighbor's house, but we have no reason to think it would be very similar to the price of a house on the other side of the country. So if you know the house on the left cost $190,000 and the house on the right cost $210,000 then it seems reasonable to guess that the house in between might cost around $200,000.

With k-NN you use the k examples in your data set that are the most similar to your input to guess the output, where k is a whole number.

Making it a bit more complicated:

You can do this in different ways. For instance, if you're guessing a number, like the price, you could take the mean price of these k nearest neighbors. Or maybe you want to use a weighted mean, where more similar examples have a stronger influence. Or maybe you want to take the mean, or something else. Or if you are doing classification/categorization, you might let each k nearest neighbors "vote" on which class the current input should be assigned to. For instance, if you're classifying handwritten digits and of the k=5 nearest neighbors three are 2s, one is a 3 and one is a 7, you might guess that the current digit is a 2. How you choose to make the final guess based on the k nearest neighbors is called the aggregation algorithm.

You also have to come up with a way to measure similarity between data points. In my example I said that neighboring houses are probably more similar than houses far away. This relies on the location of the houses, and we might measure it based on how much meters they are apart from each other. But we might also have other information, such as how large the house is, whether it's on a corner, in the middle of a row or free, whether it has a garage, garden, balcony, etc. So you also need to define a *similarity measure* that computed a similarity score (i.e. a number) based on the aspects we know about two data points.

The k-NN algorithm then uses the similarity measure to find the k most similar data points in the data set to the current input, and uses the aggregation algorithm to guess the unknown aspect(s). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-16 19:36:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you have a simulator and a way to obtain rewards, you can do reinforcement learning which doesn't use a traditional data set. This is how e.g. Deepmind and OpenAI are tackling most (video) games now. However, as you mention, this isn't simple if the game isn't simple. I don't really know Terraria, but you could look into ways that people have been tackling Montezuma's Revenge, which initially stumped some of these algorithms.

You can augment reinforcement learning with things like imitation learning, inverse reinforcement learning, apprenticeship learning, curriculum learning and active learning. You can try to incorporate your own domain knowledge, which will probably primarily affect the exploration strategy and can have a great influence on how the system learns. If you do this, you will probably want to figure out a way to make this defeasible (i.e. overridable with learning; not fixed forever). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-14 22:30:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> The answer is that it depends on some details, but in a broad range of cases there would indeed be an incentive to self-improve.

If you want your AGI to be useful, you need to give it something to accomplish: a goal. And most (non-trivial) goals can be accomplished better if the AGI is smarter, faster, stronger, more powerful, more knowledgeable, etc. Similarly, most goals can't be accomplished if you're turned off or "dead", so even if an AGI is not programmed with an explicit animal-like "survival instinct", the subgoal of surviving follows logically from most other goals. The idea of [instrumental convergence](https://en.wikipedia.org/wiki/Instrumental_convergence) is exactly this: that there are "instrumental convergent goals/values" (aka "basic AI drives") like survival, resource acquisition and self-improvement that would be helpful subgoals in accomplishing almost any "terminal" goal.

Of course it may be possible to come up with situations where self-improvement is not a useful subgoal. For instance, if the goal can easily and reliably be accomplished without it or if self-improvement is very difficult and not cost-effective. But the more difficult and far-away the goal is, the more likely it will be that initially spending some time to improve oneself is the best way to go.

> Humans have an inherent urge to reproduce, find meaning in relationships, work etc, but would the AGI even have any of those, or other, incentives? Maybe it would become depressed and shut itself off or something.

As I mentioned above, some drives/subgoals would likely emerge naturally. The other things you mention here mostly seem like they would require that they are programmed in explicitly or otherwise emerge somehow from architectural choices. Otherwise, it is probably unwise to assume it will have human emotions and mental illnesses like depression. However, if e.g. you program your AI to optimize a utility function, and it estimates that the expected value will be negative, then it might make sense for the AI to turn itself off. See [here](https://jarrydmartin.com/death-and-suicide-in-universal-artificial-intelligence/).

I find your last question impossible to answer at this point, because it depends on so many factors. For instance, how would humans and AI "integrate"? If e.g. you just enhance humans with better memory, faster thinking and perhaps specialized "apps" (e.g. for playing chess or lie detection) somehow, then we might expect most motivations and emotions to remain very similar. But even in this case, I don't think we can say that with certainty: maybe we'll all figure out that nihilism is true and find that depressing. I don't know. If we can back up and transfer our consciousness, this may affect how we'll think about things like life and react to e.g. dangerous situations. It's really hard to say what would happen. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-14 20:53:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> The Turing test is more of a thought experiment than a real test. It's usually conceived of as a sufficient (but not necessary) test for granting that an AI has at least human-level intelligence. Sometimes this is also associated with sentience. The idea being that if you can't distinguish it from human behaviorally, you should probably not discriminate against it in other ways (such as saying it doesn't really think).

One reason you don't hear that much about it is that relatively little work on AI is actually attempting to build (sentient) human-level artificial general intelligence. If you're building a Poker-bot or a face recognizer, the Turing test simply is not applicable. (Actually the concept of "Turing test" is sometimes simply taken to pass for human in a certain situation, such as in solving CAPTCHAs, which are Completely Automated Public Turing tests to tell Computers and Humans Apart, or playing like a human in a video game instead of just trying to play as well as possible.)

Among AGI professionals the Turing test comes up a bit more often. However, there are many problems with it. For one, it's a binary test that's kind of useless if you're trying to measure your progress towards AGI. Second, nobody thinks passing the Turing test is *necessary* for an AGI, and e.g. lying and misleading is perhaps not something we want our AGIs to do. Third, there is a serious question about whether passing it would even be *sufficient* to demonstrate human-level intelligence. For instance, because it seems like judges are too easy to mislead, or because it doesn't involve any sensory-motor skills. There are probably more reasons, but this should give you an idea for why it's not considered particularly useful. See [here](https://www.reddit.com/r/agi/comments/52tv08/benchmarks_besides_turing_test/) for some other tests.

Having said that, if an AI system could perpetually mislead me into thinking it is human, I would accept that it was generally intelligent, and I think most of my colleagues would too. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-12 11:08:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you feel like you've gotten the best possible performance on FashionMNIST, I guess you could move on to larger data sets like ImageNet or CIFAR. But I'd say the main idea is not to "implement" different data sets, but rather to use a data set that is challenging enough and then try to improve your classification algorithm. Alternatively, you could make it more challenging by using less data or something like that.

Of course, you could also do different tasks in computer vision. One related task is detection: find a certain object (e.g. a face) in a picture and draw a box around it. You could even combine that with classification if you then classify what's in the box. So you could e.g. detect (house) numbers in pictures of houses and adapt your MNIST classifier to classify them. Or if you use video, you could potentially use subsequent frames to do the detection, and you would certainly want the object to be tracked smoothly, so you can do interesting things there as well.

I don't know. The possibilities are endless... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-12 10:37:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> My standard recommendations if I don't know what area of AI someone is interested in are game playing and computer vision.

You already mentioned a chess bot. I probably wouldn't normally start with that, as chess is quite complex, but you could start with implementing minimax or Monte Carlo Tree Search for something like Tic-Tac-Toe (to see if it works) and then add more heuristics. If you program it in a nice way, it should be relatively easy to adapt it to other games. In fact, you could also just try to use the [general game playing](http://www.ggp.org/developers/players.html) API to get access to a lot of games. You could also look at [The AI Games](http://theaigames.com/).

In computer vision it's probably easy to do some kind of pattern recognition. The MNIST data set for handwritten digit recognition is now considered too easy, but it may still be a decent starting point for you. There should be a lot of tutorials on it. If it is indeed too simple, you can move on to FashionMNIST or something.

But you can do a lot of things. I recommend looking for competitions and benchmarks in AI, because they often give well-defined tasks, an API and maybe even some example code. Especially if that's the case, it should be easy to get started and you can spend as much time on it as you have for this particular project to improve it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-12 10:24:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> The programs you're looking at both seem to be about software engineering and not AI. Obviously there's some programming involved in a lot of AI work, but I'm not sure it should be your main focus. If you're looking for an education in AI, you could perhaps look at the degree programs offered by e.g. Coursera, Udacity and EdX.

If you're saying you want to do a software engineering bootcamp to get a job to earn money so that you can *then* study AI, I guess that's fine though. I don't know the (freelance) job market, so I don't know what you should focus on from that perspective, but my feeling is that if you want this education to also be useful for your later career in AI, it's probably better to focus on more hardcore computer science things than on e.g. web development.

I have no idea about InterviewBit and AttainU and didn't really look at them much, but I'm inclined to think that you'll learn more in 8 hours per day than in 3. But if I were you, I'd probably try to find *independent* testimonials and reviews of both programs, and if possible, statistics about how what kinds of jobs graduates tend to end up in (if any).

Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-12 10:12:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> Technically, "evolutionary algorithms" (EA) refers to a *class of (optimization) algorithms* while "reinforcement learning" (RL) refers to a *particular kind of problem setting* where a reinforcement/reward signal exists. You can solve RL problems with an EA (or with other RL algoritms).

There are broad ranges of EA and RL algorithms, so the following will overgeneralize things a bit.

To solve RL problems with EA, in each "round" you have your population of candidate AI players each play the game until they finish (or possibly until they've played for a certain amount of time). After this you use the accrued reward to assess their fitness, and then use this to select which candidates get to survive and/or reproduce (possibly/likely with mutations) into the next round. When using EA, you really have to think about how to represent your candidate AI players so that you can do cross-over and/or mutation on them (well), and this affects how difficult those things are to implement.

Reinforcement learning can be done in a large number of ways, but one of the first algorithms people learn is tabular Q-learning (or the very similar Sarsa). Tabular Q-learning is fairly easy to implement, *but* it will treat each game state like it is entirely different and independent. If you want some kind of ability to generalize, you'll typically want to use some kind of *function approximation*, which can make things a bit more complex. (If your 2D game is relatively simple, this may not be necessary, and in that case this is most likely the simplest solution.)

In both cases you could use neural networks, so then the added complexity of creating a representation for EA or function approximation for RL are fairly similar. (But you could also use very different solutions.)

With this kind of RL algorithm, you typically start off with one (random) initial policy which should converge on the optimal policy through gradual small changes (this is sometimes called local search). With EA you start with a population of multiple policies that can be fairly different (and you can employ methods to keep ensuring a certain level of diversity in the population), which should ideally allow a "faster" jump to the best kind of policy/AI (global search). EA can also benefit easily from parallelization because the different candidates can play the game in parallel. On the other hand, if the reward/reinforcement function can give meaningful values *during* the game (and not just at the end) EA typically can't use that information while e.g. Q-learning can.

In either case, you should probably look for libraries/frameworks with the algorithm(s) you want to use to save yourself some implementation work. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-02 19:05:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know too much about robotics, but I think that up until a few years ago they used more (specialized) control theory than machine learning. Machine learning is growing in importance for robotics, and it does indeed involve a lot of probability/statistics, but outside of that I don't know if you'd use it much in robotics.

If you don't have any experience with algorithms yet, but you do with statistics, I'd go with the course that fills the biggest gap in your knowledge.

Maybe our the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on our wiki can also help you find online resources about these topics.

Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-01 02:07:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> His literal title at UNSW is "Scientia Professor of Artificial Intelligence" and that's also how he describes his research on his [homepage](http://www.cse.unsw.edu.au/~tw/). I'm also not sure what you mean to show with that Google Scholar page, because most of the articles there are published in AI journals or conferences and many have AI (or Kuenstliche Intelligenz) in the title. I understand most articles aren't about deep learning, but basically all he does is AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-08-01 01:57:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> > In the distinction between 'automated' and 'autonomous', I have heard it suggested that the latter is based on stochastic reasoning which introduces uncertainty.

The definition of terms in AI is often a complicated affair that lacks consensus, but this strikes me as a particularly uncommon conception. I'm not aware of great debates about what it means for something to be automated, so my impression is that it just means that something is performed (automatically) by a machine. "Autonomy" seems to mean [a lot of different things](https://plato.stanford.edu/search/searcher.py?query=autonomy), mostly related to the idea of self-governance or being able to make one's own decisions (which can get into discussions of free will, sentience and apparently informed consent). I'm not really familiar with conceptions that relate it to whether the reasoning is stochastic or there's uncertainty. Furthermore, it seems possible for something to be both automated or autonomous (in most senses), or neither, or both.

People sometimes try to classify an (AI) system as autonomous or not, which may be possible under some definitions (or colloquially), but in my opinion it's typically better conceived of as a matter of degree of (in)dependence or control. Basically, we should ask what "decisions" the AI system is allowed to make and how that affects the world. Certainly we could say that a system that merely recommends a course of action to a human operator is less autonomous than a system that also executes that course of action without a human being involved in between, but it would be a mistake to think of the first system as "not autonomous" since it still made the decision of what course of action to output on its own. (This is perhaps a bit nitpicky though.)

> In AI, the outputs will still be the same given identical inputs.

I think this is true for *algorithms*, except when they contain explicit randomness (although in practice we only have *pseudo-randomness on our computers). But algorithms are abstract descriptions of programs that in practice have to run on a machine. Furthermore, when you look at AI *systems* they often involve multiple algorithms that don't only operate on what you probably think of as input, but also have some kind of persistent state (or memory).

We might say an AI system's output is fixed given its input and state, but only if those terms are taken more broadly than you may be used to. For instance, if your (multithreaded) AI program is running on a computer, the operating system might make different scheduling decisions between otherwise identical executions. (Or if e.g. you run the same program on different computers, or different networks, or at different times, etc.). Basically, you'd have to think of everything that can affect things as either input or the system's own state. This is one reason why [reproducibility](https://sites.google.com/view/icml-reproducibility-workshop/home) in machine learning is not actually so easy. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-30 15:32:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on the wiki that may be helpful to you.

I'd say job prospects are generally looking good, possibly depending a little on what you mean by "AI". Broadly speaking, it encompasses things like data science/analytics/mining, machine learning, business intelligence and RPA, which are all looking pretty good. There is still so much untapped potential in working in data-driven ways that I don't see this drying up anytime soon. Large companies often have their own departments for this, and consulting firms are making big bucks working for the companies that don't.

Right now research into all things AI is also pretty hot, so other jobs are relatively available, although there's starting to be a bit more competition and not all areas are equally hot (e.g. deep learning is much hotter than expert systems).

Nobody really knows what the future of AI will be like, but for the time being there are still many business opportunities for exploiting the current generation of technology. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-29 10:46:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> AI is to broad to answer this question. I'm an AI researcher, and over the last couple of years, it has been maybe 2% programming and 5% math. Most work goes into reading papers, integrating ideas, designing algorithms and other formalisms, writing papers, communicating with others, etc.

Math is mostly used by researchers/scientists who are creating the core algorithms that others then use. This use typically involves programming (so that's more common). If you are making AI software, then just like in other software development projects, programming is important, but it's only a part of a cycle that also includes requirements engineering / market research, design and quality assurance. AI projects often add data management (including acquisition, annotation, cleaning, storage, access, etc.). This, in addition to an understanding of business requirements and market opportunities, is probably also most important if you're not building software but e.g. doing data analytics, business intelligence or some form of consulting, where you're most likely using tools programmed by others. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-25 14:51:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I'm sorry if I missed it, but I was hoping to find like a research thread here.

We don't have that here. Occasionally a Sicara employee posts their [monthly selection](https://blog.sicara.com/06-2019-best-ai-new-articles-this-month-d61c763b03db0), but that's about it. I would be very interested in having something like that as a regular, recurring feature, but that would require someone to collate that list. Would you be interested in doing that?

> If not can I suggest we adopt the r/MachineLearning model of defining and using tags?

I'm not sure about the added value yet, but I'm open to the suggestion. Would you use the exact same tags, or suggest some changes?

Ideas from others are also very welcome.

---

Edit: I stickied this post to the top of the subreddit so we can hopefully get some more opinions. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-23 00:28:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> That's a topic of debate. OpenAI was founded as a non-profit with the express goal to develop *safe* AGI. Not everybody believed that, but the non-profit status seemed to counter some arguments that they're just doing it to make money.

Somewhat recently they changed their corporate structure to "for-profit" using a construction they're calling "limited-profit" because there's a x100 cap on what investors can make. They say this seemed like the best way to attract the money they say they need to build safe AGI. Of course, critics felt vindicated in their original assertion that this was all a ploy or publicity stunt to (ultimately) get richer. You'll have to make up for yourself what you believe.

In my opinion the move to limited-profit was abysmal from a PR perspective, but I believe OpenAI's leadership's heart is in the right place: they really do want to make safe AGI. I am however concerned if that will remain the case as big-time investors (like Microsoft now) might lean on them to compromise their values in order to make more money. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-21 16:33:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> Why do you think an AI that passing the TTT should get rights?

Also, do you think *human* rights are the right rights for such a system? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-21 00:47:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> Short response from mobile: you can always ask questions on /r/artificial or, if you want, through personal messages (I don't use Reddit's chat function though). In the coming week u will likely not answer very quickly though, because I'm busy and traveling a lot. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-20 01:08:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> > What drew you to AI as a field of study? Obviosuly there is the technical challenge, but I wonder how you see AI in the context of the wider world?

The short answer is that it just sounded really interesting to me, and that it seemed like a nice combination between computer science and (cognitive) psychology.

Long answer: When I was in high school really knew what I wanted to study. I was always a bit of a nerd. I liked computers and computer games. When I was 16 or so a friend got me into "programming" websites. I was good at STEM. I also fulfilled the stereotype in some other ways: I was shy, a bit awkward, not great at making friends or getting girlfriends, etc. Basically, I didn't really understand how people thought, but I really wanted to.

This drew me a bit to psychology, but I always kind of knew that such a humanities degree wouldn't really be my thing. (Actually, it should have drawn me to cognitive science, but I didn't know that was a thing.) I liked doing math, exact science, working with computers, etc. So computer science seemed like it was probably the way to go, but whenever I went to an orientation day at a university it just didn't really click for me. I think on some level I felt like CS is the means to an end, but not an end in itself, and I would like to have something interesting to work towards.

As I recall it, university orientation days in my country typically have you choose two programs to attend (in the morning and afternoon), and I still always chose CS (because I didn't have a great alternative) and something similar (like HCI or Business Information Technology). Then one day at the very end of high school I came to a university where they had a program called "AI: Cognitive Science". I didn't even know AI (or CogSci) was a thing outside video games, but after hearing the initial presentation I was immediately sold. This was the ultimate combination between CS and psychology. It was psychology from an exact sciences point of view. We weren't just learning CS stuff for its own sake, but in order to answer the question of how thinking works, and in the process we could work on cool "intelligent" programs and (sometimes) robots. So that is what drew me to the field.

I feel I've been quite lucky that I ended up encountering this orientation day, because AI has always retained my interest. It wasn't really a huge consideration for me originally, but I do think that AI will have a huge impact on all of our lives, and in some sense a contribution to AI (and especially AGI) is a contribution to all fields of science simultaneously, because ultimately those fields advance through the application of (currently human) intelligence.

> And, do you have any favourite depictions of AI in fiction..films, books....or indeed any that make you despair?

I like fiction about or with AI quite a bit. However, I find that it's never very realistic. I know this bothers a lot of my colleagues, but I like to just focus on the aspects that *are* relevant to real life (even if it would require changing some other details in the movie). Fiction is usually trying to convey a message or ask a question, which might be valid or interesting even if they get many of the details wrong. For instance, there are many robotic Pinocchio stories, where the robot is in my opinion unrealistically anthropomorphic (humanlike), so I think the depiction isn't realistic, but the question of how to ascertain whether they are sentient or deserving of rights is actually an interesting one. And if nothing else, I just think the depiction of AI itself is interesting to see how people are apparently viewing it.

But to make it a bit more concrete: I liked *Ex Machina*, *Transcendence*, *Her*, *I, Robot*, *Eagle Eye*, *Autómata*, the show *Humans*, *Person of Interest*, and I've heard very good things about *Black Mirror* but haven't seen it yet. (Note that I also just like action movies, so that may factor into some of these.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-19 22:29:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Would we even recognise machine Conciousness if it were achieved?

I'm quite pessimistic about this. It seems to me that it is inherently a subjective experience, and therefore not measurable from the outside. But hopefully I will be proven wrong at some point. At this point I'd place it squarely in the realm of philosophy (i.e. not science), where it is hotly debated. I won't claim to know as much as Dennett, but I know he disagrees with e.g. Chalmers, so I'm basically just ignoring the issue for now (like most AI researchers).

My guess is that in the end the likely outcomes will be that we will continue treating all machines as if they are not sentient because nobody can prove that they are, or that we might assume sentience based on some faulty heuristic that will likely favor AI systems that are more intelligent and more human or animal-like. Which isn't great, because a system might be capable of suffering even if we cannot easily put ourselves in its shoes. Nagel's article of "What is it like to be a bat?" also comes to mind.

> When you were studying (For your BA) do you recall how the course was structured, What do you study first for example.

My program had a relatively heavy emphasis on cognitive science (CogSci); more so than most AI programs, which are more geared towards computer science (CS). We started with introductory courses on programming, CogSci, biological psychology, logic, research methods and AI in the first semester. The second semester had human-computer interaction (HCI), mathematics, more programming, robotics, statistics and academic writing. The second year started with knowledge-based systems, more CogSci, psycholinguistics, and philosophy of CogSci/AI. Then we got an elective (I did programming), more statistics and functional programming. Then computational psycholinguistics, more AI, and neural networks. In the third year we had more mathematics, neuroscience, domain modelling, software engineering, multi-agent systems, presentation skills, labor market orientation, a bunch of electives, and writing our bachelor thesis.

Then in the 2-year master we had knowledge acquisition for expert systems, business rules, more HCI, user modelling and support, probabilistic graphical models, information retrieval, theory of machine learning, embodied and embedded cognition, brain-computer interfacing, a bunch of electives, and internship and a master thesis.

I think the above is roughly correct, but I may have mixed up the official order for some of the courses, because I didn't always pass them in the official order...

> And do you have a background in science/ mathematics/coding from before your BA?

In my country's high schools you pick a "profile" for the last 2-3 years and I picked STEM, so it was relatively heavy on math and science. I think this was definitely recommended for people wanting to study my program (at least the math part). I had a little bit of experience making simple websites, and programming a LEGO robot, which was nice but not necessary.

> Is there a broad range of experience in the AI community?

Definitely. Most universities don't offer an AI program, so people come in from different directions. The most common is computer science, but then you never know what electives and AI-related courses those people took. Mathematics is pretty common too, and there are also people coming from (computational) cognitive science / neuroscience. There are also other related programs like e.g. data science, robotics and sometimes even mechatronics or electrical engineering. And I know some people who came from (psycho)linguistics, business (intelligence), physics and philosophy backgrounds as well. Especially bachelor degrees are often broad enough that it's still quite doable to switch to AI, but it can also still be done later in life. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-19 19:04:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm trying to think of movies with talking AI systems or robots made for helping or entertaining people (e.g. not the Terminator). Spielberg's *A.I.* has a good number of talking entertainment robots. *Bicentennial Man* is basically the classic Pinocchio story. I've also heard good things about *Robot & Frank*, but I haven't seen it yet. I quite like *Ex Machina*, although Ava's personality isn't very "robotic". *Avengers: Age of Ultron* might be good although the main robot almost immediately becomes the villain (and you can also watch Iron Man and the new Spider-Man movies because of JARVIS and the suit AIs, but they're not *about* AI). *Interstellar* and *Hitchhiker's Guide to the Galaxy* both have nice AI characters, but the movies aren't really about that. I think most *Star Trek* movies/shows have an AI board computer, and *The Next Generation* has Lieutenant Commander Data (there should be movies with him too). If you don't mind watching a show/series instead of movies, I can also recommend *Humans*. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-19 15:51:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> I feel a bit bad that you haven't had a response to this, but unfortunately I also don't have the time to read your 79-page paper.

It's not really clear to me to what degree you have indeed succeeded in capturing human motivation in a mathematical system, or what the significance is of what you actually did. It seems natural to me that if I know all of Jenny's (low-level) preferences, and express them numerically, then they could be combined to reveal her (higher-level) preferences about different situations/actions/choices. But some of the main difficulties are to figure out along which dimensions Jenny has preferences and how high/low they are: i.e. how would a manipulative company know that Jenny's conception of the value of money is affected by her desire to buy a guitar, and how do they know she values this at +33? Solving the combination problem certainly seems valuable, but it's not clear to me that a simple additive model works. For instance, in Fig. 2 we see that Jenny's attitude towards "Get $50" is +64 which is the sum of the "Value of $50" which is +45 and "Trust in Kyle to actually pay" which is +19. But I would think that the combined value here should be multiplicative: Pr(Kyle gives me $50)\*"Value of $50". Of course this is just an example, but I'd imagine there are more cases where simple addition doesn't suffice.

But like I said: I didn't really read the paper, so maybe you addressed all that. If so, you may have solved the [value learning problem](https://intelligence.org/files/ValueLearningProblem.pdf) or at least contributed to it. You could seek out AI Safety researchers who are working on value alignment (see /r/ControlProblem's wiki and side bar for some links and resources).

If you're worried about potential malicious applications of SBST, then I suppose some degree of caution is warranted in spreading that knowledge. You could refrain from publishing completely, or omit talk of nefarious applications to avoid giving people idea. However, those solutions are not overly satisfactory. You could publish the general idea, but say that for details to reproduce the work they'll have to contact you (in which case you make it harder but not impossible for people/organisations to use this without your consent). If you could come up with a counter-strategy, that would also be great. But even if you can't do any of that, you might still decide to publish and promote your work if you think doing so will bring more good than harm. I should note that I don't necessarily think targeted ads are bad: if a company somehow figures out what I truly want, and uses that to sell me a product, then didn't I just engage in a transaction that I truly wanted (which seems good)? I guess there are probably worse ways to use this, though...

Good luck with your work!

Edit: just wanted to add that I think it's great that you're worried about the ethical implications of (publishing) your work, and seeking advice about it. That's already a fantastic attitude to have! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-19 14:02:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> The problem with your post is that you phrased it in terms of "the problem with AGI attempts" without mentioning any such attempts, which immediately invites criticism that (many of) these problems don't apply to prominent attempts like OpenCog. But if we ignore that, and read the list instead as "things an AGI attempt shouldn't do", I don't have that many problems with it.

1. My preferred approach to AGI is more bottom-up, so I'm also not the biggest fan of pure language approaches. Having said that, I don't think (general) intelligence *requires* having any sensorymotor modalities in specific, and it does seem to me that language is a very general system that's capable of describing virtually any problem we have. As such, I don't think it's impossible to base AGI on it, although I also wonder if a multimodal approach wouldn't make it easier to understand the world (and certainly to understand it as we do).

2. I agree that an AGI system should be capable of lifelong learning where existing models inform the creation of new ones (and there's no catastrophic interference etc.). I think most actual attempts at AGI also agree, and attempt to do this.

3. Consciousness is a suitcase term: it contains many different things (which differ based on who is using the term). I agree that qualia/sentience/phenomenal consciousness should typically be regarded as orthogonal to intelligence. But concepts like access consciousness, self-awareness, metacognition and introspection all seem useful and implementable. When people are claiming to make a conscious AI, the first thing you should do is figure out what they actually mean: it may actually be somewhat reasonable. (I also find it's best to avoid the word as much as possible, and use other words to refer directly to what I mean.)

4. It's not clear to me what you mean here.

5. I think conceiving of human goals as solely being about survival is too narrow. Evolution is a process that optimizes for existence of patterns (so survival and reproduction basically), but that doesn't mean these are also the only goals of its products (i.e. people). Humans clearly value more things than survival and reproduction (and sometimes don't even value those two very much). While some institutes are busy working on creating AGI that is aligned with our values, I agree that most AGI attempts are not explicitly doing this and I consider that somewhat of a problem. I think it should be considered a practical and moral requirement for building AGI, but in theory I'd say that if a system isn't aligned with our values, that doesn't mean it's not generally intelligent.

6. One big difference I see between people in the AGI community/society and people in more mainstream AI/ML is indeed that AGI attempts tend to shoot for the goal more or less directly, while mainstream AI/ML is taking incremental steps. I wouldn't characterize this as omitting necessary smaller steps, because AGI researchers of course do want to take all the *necessary* steps, and try to identify them. This is somewhat contrary to most of the incremental steps made in mainstream AI work, because they are often not even really aimed at AGI (but e.g. at some other shorter-term goal) and there is no real plan for how to get to AGI other than a very vague "improve current methods in any way". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-19 10:38:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> To be clear: those statements are written by Sophia's human owners. On Twitter this almost certainly happens directly, but even if you hear the robot say this, it was first typed into her program verbatim. If you want to charitable, you could say the Twitter account is a bit like other fictional accounts (e.g. for Batman) that make statements in-character. However, Sophia and its owners pretending that the robot is sentient and not making it clear that this is fake tends to rub other AI researchers the wrong way, even though it's actually a fairly impressive robotics platform.

We could also discuss the question of whether (some?) robots should be respected and accepted as "beings" rather than slaves and pets. I think most people would say that *if* a robot/AI was sentient, that means we have certain moral obligations towards it on how we treat it. Right now, nobody really believes that anything we currently have, including Sophia, is sentient (or significantly more sentient than other machines if you're a panpsychist). There's actually a bit of a problem that we can't measure sentience, so we'll probably won't know when we should change our attitudes in this regard, but most professionals currently feel it's okay to worry about this later. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-19 10:17:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> Hanson Robotics has made way more robots than just Sophia, so I don't think the accusation really applies to *them*. I also don't think it necessarily says anything about society that this one is the most popular, because it's also the most advanced of Hanson's robots and her invention just happens to coincide with the owners changing their PR strategy. I suspect her gender is mostly a coincidence.

However, people have made similar observations that you're making in general. Virtual assistants tend to have female voices and "personalities" (e.g. Siri, Alexa, Cortana), while it's said that more authoritative/expert AIs tend to be "male" (I can only think of Watson right now, but I heard more examples). Although many people would like to pin this on the biases of the skewed workforce in AI, it's my understanding that this is also exactly where market research would lead you. I think that on average both men and women intuitively prefer a female voice for assistant roles, and a male voice for more authoritative roles. Unfortunately this can reinforce gender stereotypes, especially among children. I think there is now quite a bit of attention for this, and at least Google and Apple now offer male versions of their assistants, and I assume other major companies will probably follow suit. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-19 02:59:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not sure this post will really decrease your confusion, but I made an attempt. If it didn't help, there are a lot of blog posts about the meanings of AI, ML, data science etc. that you can search for.

Terminology in this domain is messy as hell. The term "AI" means different things to different people, and the meaning may even depend on the situation. John McCarthy, who coined the term, defined it as the science and engineering of making intelligent machines. This is often broadly interpreted as something along the lines of "machines with functionality that we commonly associated with cognitive abilities / intelligence". In this conception, machine learning (ML) is a subfield of AI. In other words: everything that is ML is also considered AI, but some things are AI without being ML (e.g. search, planning, rule-based systems, etc.). Self-driving cars are also clearly AI, under this definition.

Some people have stricter requirements for what qualifies as AI / an "intelligent" machine. For instance, some might say that intelligence requires the ability to learn, so you can't have AI without ML. In the more extreme cases, some people feel that *real* AI means having at least human-level general intelligence (HLAI or AGI), which sometimes includes the requirement of sentience. According to such a definition, self-driving cars would not be AI. However, I should probably mention that the broader definitions are used more commonly.

An algorithm is a specification for how to carry out a certain procedure. For instance, a cooking recipe describes an algorithm. It's a bit like a program (code), except that algorithms exist at a higher level and are not dependent on the particular programming language. Multiple algorithms exist for making spaghetti bolognese (e.g. different recipes in a cook book), sorting (e.g. selection sort and insertion sort), optimization, learning, planning, searching, etc. All computer programs are implementing algorithms.

While computer sciency (CS) people tend to use the term "algorithm" to refer to relatively small, self-contained, high-level ideas / specifications, other people (e.g. in government or business) also use "algorithm" to refer to the products that are based on them. In those cases it's basically synonymous with "AI system" or "digital automation" or something like that. There is often confusion about the term, e.g. when someone claims an algorithm is racist, because to most CS people "algorithm" probably refers to the way in which data is used to learn, while others are referring to the entire resulting system and how it's used.

I don't know exactly how self-driving cars work, but my guess is that the practical ones are not (yet) using pure end-to-end machine learning. Automated driving systems are probably manually designed pipelines of many algorithms. Many of these will involve some form of machine learning (e.g. learning to recognize objects that are in sight), but there will likely also be more classical AI and control theory algorithms (e.g. for route planning). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-18 23:03:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> The wiki has a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) that may be of interest to you.

I personally studied AI for my bachelor and master degrees, then worked in a computer vision company, and after a few years found a PhD position on a topic I liked. I currently have a dual research/teaching position that I could get through the network I built in my PhD.

It may take some effort to find a university that offers AI degrees/programs, and it may not be possible in your country. However, you can still look for which universities are most active in AI and offer AI-related courses as part of their computer science or data science curricula (or as electives). There should be plenty of those by now.

You can ask me more specific questions if you want, although I might not answer publicly if it would identify me. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-18 00:52:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> Really? I mean: they don't have a legal team go over them or anything?

But in any case, this doesn't answer why *linguistics* of all fields of study confers the right expertise to analyze these laws. Futhermore, I would still point out that regardless of who is writing those laws (perhaps you're right that legally illiterate politicians are doing so), lawyers and other legal professionals, unlike linguists, are specifically trained to interpret those laws (written by whoever writes them). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-18 00:33:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, let me refine my question: does an education or career in linguistics (not specialized in analyzing legal texts) actually give someone relevant expertise in analyzing legal texts above and beyond what legal scholars themselves would be expected to have? Or perhaps alternatively: is it just good optics to be able to say "my famously smart scientist/linguist friend agrees with me" for a judge/jury who doesn't know better? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-18 00:03:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> I agree with all of this, but I'm curious about something: is it common to ask linguists for their opinions on legal texts? It seems so weird to me. First, because I would assume that legal professionals are themselves quite linguistically capable, and they are especially specialized for interpreting legal texts. And second, because I honestly wonder how much a linguistics education or research career brings to the table when interpreting contemporary (legal) texts.

Edit: added the missing word "linguistics" to the final sentence for clarity. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-16 17:43:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> Right now, I'm inclined to say the short answer is "no". At least not in the way I think it's usually meant.

Of course, you can train a machine learning system to make judgments that we interpret as moral. You could take MIT's Moral Machine data (on who a self-driving car should kill) and train an AI system to reproduce the results. Similarly, while I don't know exactly how the system mentioned in the article works (Mind.AI seems a bit like a ), I'm sure you could get it to answer moral questions it has been trained on. This isn't really that special, and it just treats moral reasoning like everything else.

But the problem is that you want that moral reasoning to be *applied to* everything other action/answer. Suppose I want the system to learn that it should not tell axe murderers about the locations of the people they ask about. I tell it exactly that, as well as the facts that Shelley is in the bathroom, that Jack is an axe murderer and that I am Jack. Now, I'm sure this AI can answer all sorts of questions, like "should you tell Jack where Shelley is", "what am I", etc. But I expect that when I ask it "Where is Shelley?", it will just tell me she's in the bathroom.

There are a number of issues here. One is that while the system might be able to answer a few questions, if asked in the right format, it likely doesn't understand anything about the world that's described. Even if I told it that it's good to lie to axe murderers, it might be able to repeat that it should lie to Jack (or even to me), but it wouldn't actually know what lying is, how it works, or how to do it (without further specification). Furthermore, even if the AI did have all of that knowledge, it would likely not actually be used as the question "Where is Shelley?" doesn't reference any of it, and can be solved by a very simple inference.

Of course, this is not unsolvable, but it is also not easy to solve the issue in general. And this is an extremely simple example. If instead of telling the AI to lie to axe murderers, I say it's good to avoid people's deaths, the AI would need to 1) "understand" that in some way, and 2) predict counterfactual future scenarios based on its own possible answers to see which would more likely lead to someone's death (and then act on that). That's even harder.

And Mind.AI appears to be an AGI-aspiring system. Most AI systems are fairly unabashedly narrow AI specialized for a specific application or task. Here the concept of ethical reasoning is even more of a misfit, because the AI's narrow perspective precludes having an idea of how its actions might affect the world in general. (Of course, it *is* possible to program/learn some direct rules for how to act in specified situations.)

In the future, I believe we can likely get machines to do anything we can, but right now we're not there yet. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-16 12:48:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Can you think of a good reason factual accuracy and happiness are in conflict here?

Maybe the world just truly sucks (for some people), and factual accuracy in appraising that fact leads (them) to be unhappy. I suppose that this assumes some things about the causes of happiness; e.g. that it requires a positive expectation that enough of your goals/preferences will be satisfied.

There's a [depressive realism](https://en.wikipedia.org/wiki/Depressive_realism) hypothesis that depressed individuals make more realistic inferences. There seems to be evidence both for and against it, and I'm not sure what the conclusion is, but it seems at least plausible.

> I know of at least one philosophical system that would treat unhappiness in this way. Stoicism's main promise is happiness (eudaimonia) in literally any situation. There's a big emphasis on aligning your expectations with the reality of the world with the promise of happiness as the end result.

My (extremely limited) understanding is that Stoicism is about *accepting* things as they are. This strikes me as somewhat orthogonal to being factually accurate about things. I know Stoics believe "virtue is sufficient for happiness" and that having accurate knowledge is a virtue, but it seems somewhat non-central. Furthermore, it seems to me that this seems to either redefine "happiness" to be different from what it is normally understood to be (i.e. a positive mental/emotional state) or to omit "step 0: become a Stoic" (or "reprogram yourself to have Stoic beliefs and aliefs"). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-16 12:15:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> Conversion narratives come from people who have seen the issue from multiple (or even both/all relevant) sides, which seems somewhat valuable. We make a big deal here about intellectual Turing tests, and a convert can presumably pass them very easily. In addition to that value, the *appeal* also comes when after careful consideration that person has chosen *your* worldview. It provides legitimacy (the arguments must be really strong if they caused someone to shift their worldview, because we all know that's really hard) and hope (apparently people *can* be made to join your obviously righteous side). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-12 17:32:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you want to approach it from a moral point of view, you could look for something like "requirements for moral agency" (for duties) and "requirements for moral patiency/patienthood" (for rights). However, this will most likely get you into deep, muddy philosophical waters of discussing sentience (phenomenal consciousness), which we know next to nothing about (this could roughly be construed as the genuine ability to suffer). Given that we know we can't measure it (yet) though, I think an interesting question is how to deal with the uncertainty we have regarding the sentience of other entities (ranging from rocks to plants to various animals and from toasters to intelligent seeming machines in the future). A lot has been written about roborights and artificial personhood though.

From a legal perspective, these things can sometimes be a bit easier. For instance, it may not be right from a moral perspective to blame someone for the errors of a machine they control, but they could still be legally liable if there's a signed agreement or law that says so. Here the discussion is often about whether it would be good/useful/whatever to make (some) AIs or robots "legal persons" (just like e.g. corporations are). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-12 17:12:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but it has already been posted [here](https://www.reddit.com/r/artificial/comments/cbwnxj/what_ai_articles_caught_your_eye_during_the_month/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-12 01:08:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I loved to play Go and I guess it is still competitive between humans, but the simple fact, that a dedicated AI will never be beaten again really takes a lot of the enjoyment out of it for me.

I'm sorry you feel this way, but I have to say I don't really understand it. Where you among the world's best Go players? If not, then there were always a lot of players who were better at it than you, so what difference does it make to you? Do you get less enjoyment out of running, knowing that cars, robots, cheetahs and Usain Bolt are faster? I can somewhat understand comparing yourself to other humans, and feeling better about yourself if you're closer to the top, but that's sort of an apples-to-apples comparison. I really don't care that calculators are better at arithmetic, cheetahs run faster, and computers are better at chess than me (and every other human). Why would I compare myself to those?

(I'm not trying to ridicule your beliefs or feelings, but I hope I can convince you so you get some of your enjoyment back.)

> We might still play it, but we will never again advance it.

What do you mean by this? Human understanding of the game will continue to evolve, and this will actually likely be accelerated by AI, because it can show us ways in which our current understanding might be wrong. This happened with chess since the nineties (when Deep Blue beat beat world champion Kasparov), and is now happening again with chess and Go because AlphaZero has a very different play style.

> Could you even dedicatedly design a game, that is turn based, where humans could still beat modern AIs?

[Arimaa](https://en.wikipedia.org/wiki/Arimaa) was "designed to be playable with a standard chess set and difficult for computers while still being easy to learn and fun to play for humans" in 2003 and beaten in 2015. However, I think it could probably still be done, taking into account the weaknesses of the current approaches that are being taken. I do think it's getting a lot harder though.

"Turn-based" is actually a fairly broad category, and chess and Go are both also fully observable, deterministic, discrete, and relatively low-dimensional. Those all make the problem easier. I'm not saying AI doesn't work otherwise (e.g. Texas Hold'em poker was also basically beaten, and StarCraft and Dota likely will be soon). I'm vaguely curious how AI would deal with a game like Stratego, because the incomplete information and variable starting positions make it much harder for AI, but on the other hand I would not be at all surprised if it could already be beaten. Of course, one issue is also that we're kind of running out of games that *humans* have taken as seriously as chess and Go. It seems reasonable to say that in some way the world champion in chess is likely better at chess than the world champion in Stratego is at Stratego.

> Will any kind of game (with a defined ruleset and win/lose-condition) exist, where humans will still have a chane to be superior to dedicated AIs?

There are still games like this (StarCraft and Dota haven't been beaten yet, and I think there are also still more complex games). But in the future, AI will get better and better, and it will get harder and harder to make games that humans are going to be better at. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 20:08:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> Good point! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 20:08:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think it is indeed an important question what intelligence level would be necessary to go FOOM (I realize the 1D model of intelligence is silly, but I hope it suffices for this discussion). However, suppose that an IQ of 120 or so would not be enough, then making "more of the same" seems like it could be quite useful. Many projects or fields of study could likely benefit from having a few more smarthuman-level intelligences think about and contribute to them.

While it's difficult to be sure (which is indeed a danger in itself), I think we could keep an 120 IQ AI "contained". I think it *might* be enough to go FOOM if unimpeded, but likely not if we do our best to prevent it. 120 IQ isn't really that high, so they couldn't outsmart the smartest humans *and* we have a lot of control over the AIs we create and use. What do you think?

I don't really like this as a long-term strategy, because a competitor would likely come along and assume that 130 IQ is also still safe and compete you out of the market. And then someone ups it to 140, etc. until it goes wrong. But I think having a bunch of 120 IQ AIs around would likely be useful *and* possible to make safe in itself. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 18:27:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I would like to point out a well established corporation going in the complete opposite direction: Valve

That's interesting, but how is this the opposite direction? Valve becomes more lenient on pornography, and FB becomes more lenient on inciting violence (i.e. it's now allowed in more cases). Or do you mean that what Valve did is good, and what FB did is bad? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 18:23:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Facebook has [updated its community standards](https://www.facebook.com/communitystandards/credible_violence/):

Oddly on that link I don't see the *"or is described as having carried out violent crimes or sexual offenses, wherein criminal/predator status has been established by media reports, market knowledge of news event, etc."* part. However, I do see it in the [recent updates](https://www.facebook.com/communitystandards/recentupdates/all_updates/). I'm a bit confused about what this means: is it a planned update that hasn't been enacted yet and might still be changed? Do you see it in your original link? In any case I think that *recent updates* link is informative because it shows exactly what they changed compared to the previous version.

The especially terrible thing about this update to me is that it seems to be just about adding that sentence. Somehow it would seem less bad to me if someone just wrote this from scratch, but now it's like they had a more reasonable version, and then FB decided you should also be able to threaten people who have been *described* as doing something bad. They even removed some language suggesting you should maybe wait for a conviction... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 18:10:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> > So in that case, the algorithm itself isn't biased, but someone is basically ensuring biased outputs.

The backpropagation or feedforward algorithms for neural networks are not biased for or against e.g. women or black people. The algorithm executed by a trained neural network might be. This is indeed not that algorithm's fault, because the algorithm is not a moral agent/patient, but "someone's". However, "someone" (usually multiple people, often entire societies) is rarely acting flawlessly and are typically "ensuring" this bias by accident. In any case, it is important that we can test the algorithms produced by these human-error-prone processes for bias, so that we know not to deploy them if they indeed turn out to be "biased".

I agree with you that sometimes people see bias where it isn't. Sometimes AI systems simply point to inconvenient truths or are making unbiased errors. However, the idea that just removing an offending variable as an input feature is a dangerous myth that's simply untrue. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 17:45:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Elon Musk said a while back that "the odds that we’re in base reality is one in billions." This seems too low to me, by several orders of magnitude. We've basically zero direct evidence that we're in a simulation.

In case you didn't know, Musk seems to base this on Bostrom's [simulation argument](https://www.simulation-argument.com/), which states (very roughly paraphrased) that one of the following must be true: 1) we'll go extinct before we can make simulations, 2) for some reason we won't make simulations even though we can, 3) we're almost certainly in a simulation (i.e. Musk's "the odds that we’re in base reality is one in billions"). The logic is roughly speaking that if we make a simulation, then the people we're simulating would also make one, and those would too, and so on, so there would be 1 base reality and a lot of simulations ("billions" as a figure of speech I guess). You state that we have "basically zero direct evidence that we're in a simulation", but the same is true for the base reality, so we default to a uniform probability distribution.

I agree that it's silly to be 100% confident that Bostrom's argument is sound *and* that the third option is true, but Musk probably wasn't making a formal argument and instead basically saying that he believed in the third option. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 17:41:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, so just to be clear: removing the offending variable (e.g. gender/race) as an input feature does not make it impossible for the ML system to be biased. I think you agree with that, e.g. because you say cherry picking or mislabeling can cause bias as well, but this seems to contradict your original statement that "if you don't feed it gender assignments within the data, it literally cannot be [biased against women]".

The trouble is that cherry picking and falsely/ mislabeling don't have to happen on purpose and are very difficult to avoid. The less nefarious terms are "sampling bias" and simply "error". Basically, it's very hard to get high-quality data that is completely representative and accurate, and the ways in which real data deviates from "perfect" are usually at least somewhat systematic (i.e. "biased").

(Note that there are also ways to get bias even with perfect data, such as when an algorithm concentrates its predictive power on larger clusters in the data, which can lead to higher error rates for minorities.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 17:04:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> > So that's different. I'm more talking about totally innocuous machine learning, where you just say, here's the parameters, predict this.

That's exactly what I said. The system is provided parameters (input features) and asked to "predict *this*", where *this* is "the labels we assigned to the data". This is how supervised learning works, which is the most common form of ML in the wild.

> The whole point is for it to see things no one else can and give us the unadulterated outputs, not see things and then change outputs to what some asshole programmed it to.

This is not how machine learning works. "Labeller" is an entirely different role from "programmer". A supervised learning algorithm learns to predict the labels provided to it; it doesn't learn to do something "true" and then encounter some special programming that someone put into it to change the output. (Actually, postprocessing is a thing, but that's not what we're talking about here.)

Of course, machine learning also includes reinforcement and unsupervised learning, but in those cases the ML system is still fed data (or situations) collected by (potentially biased) humans and optimizing objective/utility functions that humans provided. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 15:41:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> Cool! The Future of Life Institute has some [myths](https://futureoflife.org/background/aimyths/) and corresponding facts about advanced AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 15:29:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is not how you avoid bias. Bias comes from a variety of sources, and you need to be aware of those and correct for them. Simply having a diverse team will barely help. You need to have a team who knows their shit. Your data should indeed be diverse and representative of the population you wish to use your AI with, but it usually doesn't come (mostly) from your "team". Transparency *is* important for building trust, and it *can* help identify bias if it's there. Unfortunately explaining why your AI made a decision is often nowhere near as simple as the article makes it out to be (and it only helps you detect some biases, not avoid them).

> There are currently no laws regarding the rights of consumers who are subject to an AI algorithm’s decision-making.

Anti-discrimination laws don't magically cease to exist when you use AI (although enforcement remains difficult). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 15:17:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> If by "assignment", you mean that it's an explicit input feature, then you're wrong.

Suppose you have a data set where gender/sex isn't a dimension, but the names of people are. Given names correlate strongly with gender, so gender is fairly easy to infer. Now you get some supreme sexist to label the data with the target outputs. Train your AI on that data, and it will be "sexist".

Of course, you could argue that it's not really discriminating against *women*, but just against people named "Anna, Betty, Carol, Dana, etc." (or some complex combination of those names and other features), but in practice this still comes down to (mostly) women getting discriminated against. Also, it would be *caused* by the sexism of the labeller.

This is a simple counter-example, but there are more complex, subtle and harder to detect ways in which these biases can slip into AI systems, even if gender isn't an explicit input feature. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 13:34:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> A possible intermediate may be computational neuroscience to pick up a few more computational skills. You could also go directly for projects that draw inspirations from or connections between neuroscience for AI. DeepMind CEO Demis Hassabis was originally a neuroscientist, and DeepMind is still doing quite a bit of work in that direction. Notably with Matthew Botvinick. They even wrote [an article](https://deepmind.com/blog/ai-and-neuroscience-virtuous-circle/) about it (a link to the Neuron paper is at the bottom). I find it's a decent strategy to look at citations of landmark papers like this when looking for researchers/institutions: i.e. who are cited in the paper and [who cite it](https://scholar.google.com/scholar?cites=2036378808269487155). Of course, that's quite a laborious process...

In the AGI sphere, there are some projects that are influenced by neuroscience. Check out Sec. 3.2 of [Goertzel's 2014 survey paper](https://content.sciendo.com/view/journals/jagi/5/1/article-p1.xml). I also know Randal Koene, Richard Granger, Stephen Grossberg, Chris Eliasmith and Jeff Hawkins have done some work on neuroscience-inspired AGI (or at least presented at the AGI conference). Of course, most AI research isn't really on AGI. I just know the Donders Institute in Nijmegen who are working on the intersection between cognitive/neuroscience and AI, but I'm sure there are many more if you search.

I hope this helps a little bit, but I think that ultimately it just comes down to searching for "neuroscience ai" or something and following the results. There should be quite a lot. Then I would try to transition to a place where neuroscientists are working with A(G)I researchers (which there should be a few of; see above). I think it sounds like you have the skills for that, but if not, you might consider computational neuroscience as an intermediate step. There are still many things humans do better than AI, so understanding and being able to implement that would be valuable to A(G)I. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 13:04:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for submitting things here, but please don't just copy-paste the article without any warning. This makes it seem like you are the author, and in this case also that you "used to be on the program committee for the O'Reilly Emerging Technology conferences" etc.

If you're just submitting a link, please use Reddit's feature to [submit a link](https://www.reddit.com/r/artificial/submit) rather than a [text post](https://www.reddit.com/r/artificial/submit?selftext=true) (these links only work on the "old" interface, if you're using the new one you have to click on "link" in the top-right corner of the dialog you get when submitting something). This also goes for some of your other recent submissions where you just pasted the link into the text field (which is suboptimal, but better than pasting the entire article). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 12:55:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a rule here against self-promotion. I don't mind if people make a single post asking feedback about their project, and then maybe some follow-ups (much) later, but you're doing it a lot. Furthermore, your tone makes it seem less as asking for feedback, and more as advertising what you've done. Slogans like "AIVA is the Swiss Army knife of the Internet" and bad-mouthing your competitors (in other posts) are really working against you here.

I'm not going to ban you from posting about your work altogether. But please don't write to convince us how good your product is, or how bad your competitors are, or to direct more people to try your product. You can write about the algorithms you're using, the specific problems you run into, (problems with) virtual assistants and AI in general (if you feel you can be relatively unbiased), and you can occasionally ask for *specific* feedback. These are just examples: I'm sure there are many other things you could write about. I'm just going to ask you to make extra double sure it cannot possibly be read as a promotion or advertisement for your own project(s). I do genuinely wish you the best of luck with them! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-10 12:30:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your submission, but this link was already posted [here](https://www.reddit.com/r/artificial/comments/cb7ke4/ai_trained_on_old_scientific_papers_makes/), so I'm deleting this.

/u/victor_knight: you may want to repost your comment in the above linked thread. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-09 15:46:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know the paper, but if you want help you should probably say a bit more on why you're struggling to reproduce it. What are the roadblocks? I also suspect you may have more luck on /r/MachineLearning if you elaborate a bit on your issues and provide a link to the paper. Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 18:39:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> There should be some links in the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) of /r/artificial's wiki. Off the top of my head, I think Udacity has a good course with Charles Isbell and Michael Littman and the course David Silver has on his website is good as well. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 17:48:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> > It took them three years to remove the category.

They actually did it pretty much immediately. The article says that 3 years later they still don't have a *better* solution.

> Should have been done from the start.

Maybe. But I don't think it's *totally* unreasonable to have their image classification algorithm classify everything that it can. And it likely just got the categories from some standard data set.

> If they really want to classify them, then they can just add another algorithm that discriminates specifically gorillas from people when the original network classifies something as a gorilla.

Do you expect this algorithm to have a 0% error rate in the wild? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 17:36:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> What do you think Google should have done instead? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 17:25:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm mildly surprised this isn't on Facebook's list of no-no words. When Google offended some black people by labeling their picture with "gorillas", their ["solution"](https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai) was to simply never classify anything as gorillas ever again. I thought having a list of words like that was by now fairly standard practice among the AI giants, because if you're classifying a gazillion images, you're bound to make some errors sometimes (even if you improve your system), and the utility of correctly classifying the odd gorilla/hoe image probably doesn't outweigh the PR shitstorm you get in the few exceptional cases where you get it wrong. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 11:41:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> Right. I asked to get more information about it so I could help you find it. I don't have time to watch the whole video, so if you could give an indication of when it's mentioned or how it's described, that could be helpful.

If it's a WEF report, it might be on their website: https://www.weforum.org/reports </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 11:05:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> It seems like the team behind [the "Norman" project you're referring to](http://norman-ai.mit.edu/) didn't actually publish their work, so it's not *entirely* clear to me what they did. However, from the summary they give, their goal was to show that bias comes from data rather than the used algorithms, so that when you train an image captioning AI on a subreddit "that is dedicated to document and observe the disturbing reality of death" it will caption Rorschach inkblots in a manner that we interpret as "psychopathic". They compare it with a reference system that's trained on the MSCOCO dataset, which produces more "normal" (or at least less disturbing) interpretations of the inkblots.

I don't know of anyone who trained it on single Reddit profiles, but presumably the result would be that it would caption inkblots in a manner that could be considered typical of the person whose profile it was (*if* there is enough appropriate data in that profile). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 10:53:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I was wondering if my interest in Artificial Intelligence and AI development in general could be applied to the development of bionic prosthetics for those who need.

Definitely! See e.g. [AI for Prosthetics Challenge](https://www.crowdai.org/challenges/nips-2018-ai-for-prosthetics-challenge) at NeurIPS, and the [recent write-up](https://arxiv.org/abs/1902.02441) of the solutions.

Generally speaking, I think there's a lot of demand for AI in the development of modern prosthetics, because you want them to "learn" to adapt to their specific user and (sometimes) recognize and adapt to specific situations (e.g. a knee or ankle changing settings if it detects you're climbing stairs). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 10:44:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> What report? The article does not seem to mention a report. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 10:31:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> Adding to the existing answers: I think this is 1) beyond the current capabilities of AI/ML, and 2) more difficult than making the correct moderation decision (which is a relatively simple, but still too difficult, classification problem). However, I like that you're thinking about the well-being of content moderators, and I do think that AI could help to alleviate their burden to some degree (mostly by making automatic classifications). I know the police are also looking at [solutions like this](https://www.dailymail.co.uk/sciencetech/article-5190517/Artificial-intelligence-detect-child-abuse-images.html) for e.g. images of child sexual abuse. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-08 10:13:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> The top answers already say mostly what I'd want to say. I'd mention mathematics, data science, (computational) cognitive science/psychology, (computational) neuroscience/psychology and philosophy as the primary/secondary educational paths to follow for a career in AI/AGI. See also the [education plans](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) suggested by prominent AGI researchers.

What I'd like to add is that while I think it's very possible to choose an educational path that isn't *primarily* concerned with computer science and programming, you will likely still have to learn these to some degree. Ultimately AGI is about programming intelligence into a computer (roughly speaking). To contribute to the problem, you'll need to understand the difficulties this brings. And to function well in a multidisciplinary team, you'll need to understand each other's disciplines a bit *and* speak a common "language".

I will also say that these paths lead to AI/AGI a bit less naturally. If you're in e.g. CogSci, there will be less people around you (students and professors) interested in AI, you'll be set up for a career in a slightly different direction, and you'll have to seek out AI groups who are looking for a CogSci influence (or AI-focused CogSci'ers) yourself, which may be rarer than AI groups looking for computer science talent. Also, being able to program is a desirable skill, especially if you're the junior member on a project. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-05 13:15:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's a YouTube channel called "Two Minute Papers" that summarizes (mostly) ML papers. Also, I think Synced on Medium often does a fairly good job summarizing new research. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-04 15:26:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> I filled out your survey.

I do like the overall idea, but there are a few notes I have. One is that claiming you have 98% accuracy is kind of a red flag for me. One of the big issues with tests like these is the false positive rate, so I want you to tell me precision *and* recall. Next, I'm told I have a mole with 98% *accuracy* again. But presumably you calculated some *confidence* level.

Another thing I wonder about is contacting the doctor. As presented, the app seems quite easy to use, but does this mean I have to give it my medical details? Or are you going to contact some random doctor who isn't my own? Also, I'm slightly concerned again about the false positive rate and the time it will take away from doctors (as well as how much stress, discomfort and unnecessary operations it may lead to for patients), but I'm sure you've thought about that.

Anyway, these are just some notes. Likely things you've already thought of. Keep up the good work on this worthy project, and good luck with your thesis! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-04 15:08:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> Robert SK Miles has an interesting YouTube channel on the control problem. I think Lex Fridman does the MIT AI podcast. I'm not sure if Adam Ford's channel is still active, but he did some interviews with AI and AGI researchers. There are also some links to materials on /r/artificial's [wiki](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) although that's mostly for people looking to get started on building a career in the field (it's also a bit of a mess, so please post suggestions for improvements etc. [here](https://www.reddit.com/r/artificial/comments/c5j9zo/getting_started_with_ai_help_improve_the_wiki/); thanks!). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-07-04 15:04:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> Both are probably good. On the face of it, both sound very practical: CE tends to be more about, well *engineering* (i.e. making stuff) then about theory and science, and *applied* data science obviously sounds like it's going to be applied. It's a bit hard to tell without knowing the contents of the programs, but you already mention that CE in your country is a combination of CS and CE elsewhere, so that sounds pretty good. I would think of this as the default choice.

(Applied) data science can teach you good things too, although it's probably *mostly* applicable to machine learning (ML) and not really to other parts of AI. Of course, nowadays ML is what most people think of when they hear "AI", so this may not be a huge loss. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-30 22:41:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> The Netherlands seems like a pretty good place for you. I know there's a quite a bit of ML going on there now as well, but compared to the US and China there's still way more going on in the parts of AI that aren't ML/deep learning. (I'm not saying it still isn't used a lot though, because you can often add ML to classical methods to improve them, plus you can probably get more funding for whatever is currently "hot".) I think there are a lot of academics in the Netherlands that are still primarily working on the kind of thing you're interested in, so it should be possible to get a PhD position with them (if they can find the funding). It may actually help to reach out to professors whose work you like, so they know a student like you might be available. A good strategy for this IMO is to look at who is publishing work you like at conferences like IJCAI or AAAI (or smaller, more specific ones that match your interests). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-28 14:13:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> I feel like the central problem here is gerrymandering, not AI. Obviously AI can be used to draw district boundaries according whatever objective you want. That means it can indeed optimize for partisan advantage, but it could also be used to diminish it or optimize for some other notion of fairness or objectivity. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-28 14:02:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> This link was already posted a few hours earlier [here](https://www.reddit.com/r/artificial/comments/c6h9qo/who_is_a_bigger_threat_humans_or_ai/), so I removed this. You may want to post your Jack Ma link there. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-27 11:45:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> Right, in the beginning, while people are still misled into thinking a *Science* publication means the same thing it always did. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-27 09:09:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> Nobody is saying you can't publish replication studies in *Science*. The point is that they shouldn't be *automatically* published there. If you did, it would completely change what a *Science* publication is viewed to imply, which right now is something like *extreme career-making quality* (ECMG). If you start accepting papers because of other reasons X, *Science* will imply ECMG and/or X, so it's no longer a good signal for ECMG.

I'm just saying that with *Science Replications* you give extra information to disambiguate which of the criteria was used. What this does is get replication studies to the same audience as the original studies while informing people about what criteria were used for acceptance to avoid confusion.

I agree that we should not turn our nose up at replication studies, which is why *Science Replications* should probably have pretty high status as well. But on its own merits, and not because people are confused about the criteria used for acceptance. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-26 21:53:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for letting me know. I guess I screwed up one of the settings when trying to make the community part.

It should work now. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-26 15:06:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> What podcast? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-26 03:53:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> Broken links should be fixed, and I'm asking for updates [here](https://www.reddit.com/r/artificial/comments/c5j9zo/getting_started_with_ai_help_improve_the_wiki/). </TEXT>
</WRITING>
<WRITING>
<TITLE> Getting Started with AI: help improve the wiki with questions, answers, advice and resources </TITLE>
<DATE> 2019-06-26 03:52:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> We regularly get questions here from enterprising beginners who wonder how they can get started with AI. We have a [section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) for that on /r/artificial's wiki, but as /u/anal_bifurcation pointed out, it's quite old. I think I fixed the few links that were actually broken (assuming springer.com will come back online), but that doesn't change the fact that the actual content hasn't been updated in about 2 years.

Unfortunately, I haven't consumed much "getting started materials" myself in the intervening time, so I don't know what's new and what's good.

* If you have any *general* **questions** that the wiki does not address, please ask them here. If you have questions that are specific to your personal life or situation, you can also ask them here, but you could also make [your own post](https://www.reddit.com/r/artificial/submit?selftext=true) about that.
* If you have **answers** to these questions, please post them. I'll do my best as well, but the whole point is that I want to hear from others as well.
* Please also post your own **advice** for beginners. You don't have to be a veteran to do so: sometimes an account of someone's own experience in getting started can also be very illuminating.
* What **resources** would you recommend, and which wouldn't you? Books, articles, tutorials, MOOCs, etc.
* Please also **comment** on each other's questions, answers, advice and resources. E.g. if you (dis)agree with a certain piece of advice or a book recommendation, that's valuable information too. This also applies to giving your opinion on what's currently on the wiki (e.g. maybe you disagree on university being useful).

The wiki is also a bit of a mess now: e.g. there are 3 "getting started" pages which have some overlapping (and arguably inconsistent) content. I'll try to fix that up when I try to incorporate the things y'all will post here. Any suggestions are of course welcome. If you want to edit the wiki directly: I'm a little bit apprehensive about letting just anyone edit the main and existing pages, but you can message me about it or create your own pages on the [community part](https://www.reddit.com/r/artificial/wiki/community) of the wiki if you want.

Thanks for your help! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-26 00:29:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I am concerned that it would be easier for hiring committees in the future to find something they don't like, rather than something they like.

I don't know how valid of a concern this is, but I note that it's not an issue with certain social media. Basically, a lot of social media can be used almost entirely passively and you would not run into issues like this.

On Google Scholar you don't even have an option to not be passive (I think). Just make a profile there so people using it (i.e. everybody) can easily find your (other) papers. I would also recommend having a LinkedIn profile where you keep an up-to-date resume. You can publicly post stuff there, but in my experience most people don't, but you can use it to connect to people you meet or work with. This can work in your favor when reaching out to them or their contacts, or possibly when you're looking for a job (although I expect the effect of "oh, he knows people X, Y and Z somehow" to be fairly minimal). Even on Twitter you don't have to post anything: you can also just follow people you find interesting.

This is basically what I do because I'm shy. However, I have the feeling that it would probably be much better if I *did* post stuff online on e.g. Twitter and LinkedIn groups. It's a good/decent way to get visibility that you wouldn't normally get as a beginner, and it could potentially help you a lot in networking. Of course, it might be smart to refrain from saying overly controversial things, but otherwise it's probably a net positive. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-25 12:54:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is a new book by ex-BuzzFeed journalist Tom Chivers (/u/tommychivers) on AI, AI risk (i.e. the Control Problem) and the rationalist community that's trying to solve it (i.e. Eliezer Yudkowsky et al., CFAR, LessWrong, etc.). I haven't personally read the book yet, but someone on /r/slatestarcodex did and posted a [fairly positive review](https://np.reddit.com/r/slatestarcodex/comments/c52l9w/some_notes_on_the_ai_does_not_hate_you/). There's also another (parallel) discussion on the book [there](https://np.reddit.com/r/slatestarcodex/comments/c3vmzm/new_book_about_ai_risk_eliezer_scott_and_the/). Last year there was also a [discussion there](https://np.reddit.com/r/slatestarcodex/comments/7nbbg0/apparently_someone_at_buzzfeed_is_trying_to/) following the book's announcement which lead the author to do an [AMA (interview)](https://np.reddit.com/r/slatestarcodex/comments/7oyod6/im_the_now_exbuzzfeed_journalist_writing_the_book/).

That's all the information I have right now. I'm curious about the book and what you all think, but probably won't get around to reading it for myself anytime soon I'm afraid. </TEXT>
</WRITING>
<WRITING>
<TITLE> The AI Does Not Hate You — Superintelligence, Rationality and the Race to Save the World by Tom Chivers </TITLE>
<DATE> 2019-06-25 12:54:22 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-25 08:45:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for your message! I think we can indeed debate whether a fake video about AI/robots has a place here: I wasn't sure, which is why we're having this feedback thread. Feel free to disagree with my acceptance of this video.

However, AI is a very broad concept and this sub is indeed about all related news and discussion, ranging from beginner questions to popsci articles to scientific publications (although I'll grant that in practice there isn't much advanced content). /r/ML indeed has a much narrower focus, both because ML is a subfield of AI and because they made a choice to only allow advanced content. I do wish we had an advanced AI sub like that as well, but I also think there should be a sub where laymen and beginners are welcome (which isn't really the case for /r/ML).

Long story short: yes, this sub is very broad by design. If you're looking for something much more specific and don't want to wade through content that's irrelevant to you, I'm sorry to say this may not be the sub for you. Especially if the "something specific" is already covered very well by another sub, like /r/ML. But if you have suggestions for improving this sub in your eyes, I'm all ears. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-25 08:34:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm inclined to agree, but the thing that complicates it is what it means to have a publication in *Science*. I definitely don't think it's a given that a failed replication study meets the standards of *Science* on its own merits and that the authors deserve to add a *Science* publication to their credentials.

The best way to fix this IMO would be for *Science* to create a separate journal name like *Science Replications* where they accept any and all (failed) replication studies of *Science* papers if they meet the quality standards as judged by *Science*'s reviewers. *Science Replications* can then acquire its own reputation and prestige level (like e.g. *Letters to Nature*), but the articles (or possibly summaries can still be printed in the physical *Science* magazine, featured (prominently) on the *Science* website and used to annotate the original article (e.g. put at the top "Results in this article were replicated here and here, and failed to replicate here and here.". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 23:20:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> Not sure if we're supposed to link it, but there was a fairly extensive [discussion](https://np.reddit.com/r/slatestarcodex/comments/c3ce3a/we_tried_to_publish_a_replication_of_a_science/) on r/SSC about this. It's mostly about the idea that failed replications of *Science* maybe shouldn't be posted in the career-making *Science* by default, rather than the CW angle. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 20:15:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> In this case "this" is a special effect (presumably made using motion capture) and not even a robot. But Boston Dynamics actually has many videos where they "bully" their actual robots. I think it's mostly intended to showcase their robustness in the face of obstacles / difficulties / etc. and not as a social experiment, but you can Google "Boston Dynamics bullies robots" to see how the internet responded / responds. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 20:03:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Solvable, as in whether a system can be successfully designed which resists biases (which might stem from datasets being biased towards a group, or even the fact tjay groups might in fact be more prone towards prediction even if society thinks that to be an unfair discrimination).

Well, there's quite a bit of research into "debiasing" AI, so I'm sure it's possible to build a learning system that "resists" *some kind* of "biases". However, it's usually done by operations on the data (which leaves the learning system itself alone), or by (ironically?) altering the learning system's inductive bias. For instance, to "resist" biased results based on race, the system might need to be able to identify race to correct for it. I think Alexey Turchin wrote a paper that says a similar circularity exists in value learning for AGI safety, but I don't remember which one.

> As regarding ASU, I've yet to both find people who are invested in CP related research, and solve the how to approach them without being filtered for spam. If you have advice on the latter, I'd be eager to listen.

Do you have a contact person? If so, you could try going through them. Another option is to email some grad students. They're more likely to respond, and might tell their professor about you if you have something interesting to add, but this works better if you already know which professor is doing work you find interesting.

> PS: You seem to be well versed in CP literature. Are you currently involved in this anyhow?

Unfortunately not. I've been interested in it for a while, but I haven't been able to switch to this as a professional career (partially because I want to finish some other work first, and partially because I don't want to move to another country, and as I mentioned most universities are not working on AGI Safety). Hopefully next year... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 19:13:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is part of a [fake video by Corridor Digital](https://www.youtube.com/watch?v=dKjCWfuvYxQ) (thanks /u/StrongCute!). I found it a pretty funny take on Boston Dynamics' tendency to [bully their robots](https://youtu.be/rVlhMGQgDkY?t=85) and I figure we could have a discussion about (testing) AI robots and such despite the robot/AI in this video being fake, so I'm allowing it. Feedback on that decision is appreciated. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 18:39:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I've had a hard time finding relative similarities even in the transparent AI research, or knowledge representation, or bias computation and removal systems at ASU.

Do you mean that you haven't found ASU researchers working on these topics, or that you don't know how to connect them to AI Safety / CP research? If it's the first, I can't help.

Interpretability (transparency) has been linked to the CP e.g. [here](http://s.interpretable.ml/nips_interpretable_ml_2017_victoria_Krakovna.pdf) and [here](https://www.lesswrong.com/posts/DbZDdupuffc4Xgm7H/1hr-talk-intro-to-agi-safety#Interpretability__Bootstrapping__Robustness___verification), and to other things (including safety) [here](https://ec.europa.eu/jrc/communities/en/node/1162/article/interpretability-ai-and-its-relation-fairness-transparency-reliability-and-trust) (this is a very non-exhaustive list).

I can't find much about the CP's relation to bias/fairness, although it's mentioned [here](https://futureoflife.org/ai-policy-challenges-and-recommendations#Fairness), but I hope that at least some fairness cases might be a mini-version of the value alignment problem. Basically, if the unfairness isn't due to shitty data, but because honestly optimizing the stated objective leads to unfair outcomes, then we have to do some kind of value alignment / fairness adjustment somehow. I don't know...

For knowledge representation, I don't have anything concrete, although I feel like it kind of ties into the foundational properties of both AI and [AI Safety](https://www.lesswrong.com/posts/DbZDdupuffc4Xgm7H/1hr-talk-intro-to-agi-safety). If you look at e.g. [embedded agency](https://www.alignmentforum.org/posts/p7x32SEt43ZMC9r7r/embedded-agents) there's kind of a problem with how to represent knowledge and construct a good model of the world.

Maybe this [mindmap](https://futureoflife.org/landscape/) of the research landscape by the Future of Life Institute can also help.

Anyway, good luck! I'm sure you'll be able to get better responses from your professors when you join. At the start you'll probably have a talk with someone about what you're doing there, and you can ask them. Hopefully they can then forward you to the most relevant person(s) and you can go from there. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 17:41:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I'm trying to find professors in the University I'll be joining who are involved with research in the Control Problem without any luck. What should I be looking for?

In your particular instance you can just look at the handful(?) number of professors in your department and see if they're doing anything relevant. I'd say the odds are extremely high that they're not doing anything related to the Control Problem, and that it's not a matter of what keywords to search for. Outside a few places in the world, there aren't really many academics working directly on the control problem at all.

I think the best keywords are probably "value alignment", "value learning", "AI risk", "AI safety", "existential risk", "catastrophic risk", "suffering risk" and "superintelligence", and things like "corrigibility", "cooperative inverse reinforcement learning (CIRL)" or other specific approaches. However, this will likely be a bit hit-and-miss, and I think the field is small enough to basically just look at the works cited by a few key publications, and perhaps the works citing them. Some of these key publications include Bostrom's Superintelligence (2014), Sotala & Yampolksiy's [2014 survey](https://iopscience.iop.org/article/10.1088/0031-8949/90/1/018001), Everitt, Lea & Hutter's [2018 survey](https://arxiv.org/abs/1805.01109), and [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565) (2016). And of course there are particular papers on [corrigibility](https://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10124/10136), [value learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.674.6424&rep=rep1&type=pdf) (or [this](https://intelligence.org/files/LearningValue.pdf)), [CIRL](https://arxiv.org/abs/1606.03137), etc. I still think that explicit work on all of these is relatively rare though (in the grant scheme of things), and probably 99% of universities doesn't have a faculty member working explicitly on any of it.

If you already know you're going to a certain place, and it turns out that indeed nobody is directly working on the Control Problem, you could try to see if anyone is working on something that might (inadvertently from their perspective) help with an aspect of the Control Problem, or you could try to convince someone of the CP's importance. You'll likely have the best luck if someone is working on some kind of ethical, legal, societal or economical issue related to AI, such as fairness, accountability, transparency (incl. interpretability and explainability), privacy, meaningful human control, safety, (cyber)security, verifiability, robustness, etc. In this case the Concrete Problems in AI Safety paper might also be your friend, since it includes many research areas that are already relevant in the pre-AGI world.

You can also reach out to [80,000 hours](https://80000hours.org/coaching/) or one/some of the institutes working on AI safety. They might be able to offer you advice on how to make the most of your career w.r.t. AI safety and how to deal with a situation where none of your personal faculty members are working on it. Who knows, it might even be possible to get one of their professors on your PhD committee. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 15:59:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> My PhD supervisor often told me that he felt the same way when he was 16, reading about the amazing advanced in AI: that by the time he could get into it, it would already be solved. That was three and a half decades ago.

There is a (very) small possibility that AGI will be solved in the next 10 years, and that it will also lead to a singularity / intelligence explosion. But if that happens, it's not just going to "dramatically change" the AI field: it will dramatically change everything else as well.

I would actually think that AI researcher is one of the more resilient future career paths. As we get more and more advanced AI, various jobs will be automated. Whether this leads to big structural technological unemployment is a matter of debate, but nobody seems to dispute that many employees will at least be displaced and will need to find other jobs (which may or may not crop up). But as long as AI can't research and program better AI, the job of AI researcher can't really be automated, and it will only get more valuable. And once that *is* possible? Well, like I said: that would change *everything*.

But having said that, [most predictions](https://aiimpacts.org/ai-timeline-surveys/) put the date for human-level AI a few more decades away.

So by all means: pursue a career in AI! There's a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 15:50:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> In my opinion both paths are good. However, I would personally prefer math. I wrote a bit about it [on /r/artificial's wiki](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_university) with a link to another discussion about math vs. computer science.

Basically, I think mathematics will *always* help you, *and* it apparently benefits a lot from taking it in college (compared to self-study). It will be relevant basically no matter which path towards AI you want to pursue. Neuroscience on the other hand seems like a more specialized choice. Some people think that brain emulation is the best path towards AGI, and some think we can learn a lot from the brain to improve neural networks. DeepMind's CEO Demis Hassabis studied cognitive and computational neuroscience and wrote [this article](https://www.cell.com/neuron/fulltext/S0896-6273(17\)30509-3). However, I do think that even for AGI and NN research, most people are not really following the neuroscience path. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 15:41:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki that might be helpful, but it seems like you already have a lot of good ideas, and have gotten some good advice.

I would echo some of it, and suggest you do less reading and more hands-on. If you read AI: A Modern Approach, that should really cover you for a while (and to be honest, you probably don't need to read it entirely). You may want to add in a book that's more specifically on machine learning though (e.g. Goodfellow's DL book, as was already suggested).

Personally I typically prefer courses / MOOCs to books though (although a course might be augmented by literature), mostly because they tend to be a bit more hands-on. I think it's a good idea to really code the systems/algorithms you learn about. Since you don't just want to do ML, I suggest starting with an Intro to AI course like the one on Udacity or maybe the one by ColumbiaX (although I haven't taken that one so I can't vouch for it). For ML, Ng's course is a great one to start with. After that, you should probably aim for something a bit more in-depth and practical / hands-on.

Once you got the basics covered, you can of course dig deeper with books, articles, and perhaps most importantly: your own projects. AI in gaming can be good for that (see my comment [here](https://www.reddit.com/r/artificial/comments/c1bv57/where_to_start_learning/) for some links).

> My University has agreed to helping me in this pursuit of AI through for example bringing me in contact with AI researchers at teachers. How can I make the best of this offer?

Take them up on their offer. Talk to all relevant teachers and ask them for advice. Not only are they likely more expert than most of the people here, but following *their* advice rather than ours (even if it's otherwise equally good) has the advantage that you can sort of keep working with them and that they can likely help you better if you follow their path. Ask if there are courses you can take at your university, and if you can work on projects with them (maybe ask first what you need to do/learn to become eligible for that). Sometimes you can replace an (elective) course with doing a project (or self-study) with/for a professor. Also ask them if there are any AI-related events/activities/societies that you could attend.

Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 15:25:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> Looking at projects pursued by the researchers in the [AGI Society](http://www.agi-society.org/) and at the annual [AGI Conference](http://agi-conf.org) I don't see any approaches that are primarily based on evolutionary computation. The closest is perhaps [MOSES](https://wiki.opencog.org/w/Meta-Optimizing_Semantic_Evolutionary_Search) which is a part of OpenCog. Having said that, I think some (simple) forms of evolution are used quite frequently without being called "evolution", e.g. in attention, learning and/or forgetting mechanisms. For instance, an "unfit" idea might be forgotten, while "fit" ideas are actually utilized by the system and allowed to generate offspring (e.g. generalizations, specializations or even random mutations). Or ideas that are used together (often) (i.e. "meet and are successful/attractive") might be combined.

I think a lot of evolutionary computation (EC) is happening in the Artificial Life (ALife) community though. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 15:03:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> OpenAI wrote an article with a similar call on how [AI Safety needs Social Scientists](https://openai.com/blog/ai-safety-needs-social-scientists/). Scanning through the reference lists of this article and the one you linked leads me to believe that there is not yet a journal specifically dedicated to "the social science side of AI". I do feel like I've seen it argued before that we should (also) study AI using something like "AI psychology", but outside of this [short paper](http://www.iiisci.org/journal/CV$/sci/pdfs/iZA532FA.pdf) I can't find much.

I would think that some of these concepts might be found in the study of "artificial emotion" and "human-robot interaction" (or human-computer / human-machine), and [computational social science](https://en.wikipedia.org/wiki/Computational_social_science) might have already formalized some of the concepts we might want to study (although it's aimed at studying humans). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 14:39:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know, but perhaps this link will help: https://paperswithcode.com/task/relational-reasoning </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 14:35:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> Ultimately most AI applications are "reusing" some other (typically more general) idea. AI research is actually mostly about creating methods that can be used in different contexts (e.g. neural networks). Perhaps you're not asking about general-purpose methodologies getting reused "4 good", but about concrete applications being adapted into others. I don't know any examples of the top of my head, although I'm sure many exist. So I think that if you google "AI4Good" you'll actually get many good results, like the [AI4Good foundation](https://ai4good.org/) (with many events where people presented their work) and the recent [global summit](https://aiforgood.itu.int/) in Geneva. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-24 14:08:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> Capsule Networks are a kind of Deep Neural Network. You can read more [here](https://towardsdatascience.com/capsule-networks-the-new-deep-learning-network-bd917e6818e8) (and in Hinton's papers of course). According to that article:

> While CapsNet has achieved state of the art performance on simple datasets such as MNIST, it struggles on more complex data that might be found on datasets such as CIFAR-10 or Imagenet. This is because of the excess amount of information that is found in images throw off the capsules.

> Capsule nets are still in a research and development phase and not reliable enough to be used in commercial tasks as there are very few proven results with them. However, the concept is sound and more progress in this area could lead to the standardization of Capsule Nets for deep learning image recognition. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-17 14:05:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. Gameplaying AI mostly involves Monte Carlo Tree Search (MCTS), (Deep) Neural Networks (NNs) and Reinforcement Learning (RL).

If you're starting from scratch, you're probably going to want to pick up some rudimentary programming skills first so that you can implement the algorithms you'll learn about subsequently (which is good practice). Something like Udacity's CS 101 is a decent start. For machine learning basics that may help with neural networks, you could start with Andrew Ng's Coursera course. And then you're probably going to want to follow that up with a more hands-on deep learning / neural networks course like Google's on Udacity or fast.ai or something. For the basics of AI and (game tree) search, Thrun & Norvig's course on Udacity is decent (and maybe the ColumbiaX/EdX one is as well, but I don't know). I also don't know if Mike Genesereth's Coursera course on General Game Playing is still available, but if it is, that's a good one too. For reinforcement learning there's a course on Udacity by Charles Isbell and Michael Littman that covers the basics. David Silver (leader of Deepmind's RL research group) also has a more advanced RL course that also covers function approximation (i.e. NNs) and ends with an application to game playing.

You may also be interested in checking out things like [Robocode](https://robocode.sourceforge.io/), [The AI Games](http://theaigames.com/), [General Game Playing](http://general-game-playing.de/), [General Video Game Playing](http://www.gvgai.net/), etc. that provide a framework you can implement with your own AI that plays some kind of game(s). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-16 16:48:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> > How AI helps reduce the gap between doctors and patients

That's probably a good example, because it's a more specific question about a more commonly considered (and broad) domain.

> Economy and transportation does kinda peak my interest. Do you maybe have any topic under them in your mind?

The other day somebody asked about Train Traffic Control using AI here. I think they've been using a lot of AI for many decades in the railroad industry, but it hasn't really received that much attention.

There's been a lot of talk about self-driving cars, but what about self-driving trucks, taxis, buses, trams, trains, ships, aircraft, etc.? There might be effects on how to change cities, or how you'd design new ones from the ground up, or how people view their commute (and make decisions about where to live relative to their jobs). For trains in particular you might think that could just be fully automated. Why hasn't that happened? What are the additional roles of a driver? For instance, a bus driver (and perhaps especially schoolbus driver) might provide additional oversight, ensure people buy tickets, call 911 if needed, etc. Instead of a parent driving their kids to school, can they just leave it to a self-driving car / taxi? What if they get into a fight, or if some nefarious person tries to get in? What about the intersection with cybersecurity: maybe self-driving cars drive better / safer, but what if they're hacked?

For the economy I don't know much that isn't very cliché, like technological unemployment, inequality, etc. High-speed trading and flash-crashes have probably also been done quite a bit. This is probably more finance than economics, but e.g. banks are doing (possibly discriminating/racist) risk assessments for loans using AI (and e.g. detecting fraudulent credit card transactions). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-16 13:58:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> > The lecturer also mentioned it shouldnt be an implementation, it should be a simple research on how AI helps or does something, for example "how AI can improve cybersecurity" but seeing the research for it has already being done i assume.

It sounds a little like you're supposed to do a report on AI in conjunction with some other research area or application (domain). Since you're not supposed to do any implementation or "science", the concept of "reproduction" is a little odd, so perhaps your teacher just means that she doesn't want you to basically copy/summarize/whatever a single other paper. Because I would normally think the existence of other material on your topic is not a dealbreaker for doing a report on it. Actually, that report would then presumably involve integrating the most important works on e.g. AI + CyberSec, which are all written from different perspectives, into something more coherent.

If you're really supposed to write about a domain that nobody else has written about, that seems really difficult, because then what do you have to write about? If there's even a single research paper, it probably has a section on related work and impacts.

However, it may help you to pick something that is a bit less covered than AI + cybersecurity, or the economy, or transportation, or healtcare, etc. Perhaps there's a more specific area that you know a lot about, that's small enough that it hasn't received much *specific* coverage yet. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-16 13:08:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> Are you in middle, high, undergraduate or graduate school? What year? What is your program, and what is this course (assuming "Computer Research Project" isn't the name)? What kind of programming experience do you have? How much time do you have to spend on it, over how long of a period? What are the instructions for this project/assignment? What aspect(s) / subfield(s) of AI are you most interested in?

You mention a lot of stuff has already been done, but unless this is a grad school (or possibly undergrad graduation) project, that probably shouldn't matter very much. What you can typically do is reproduce the results of a certain paper, and if that's not enough, tweak it every so slightly and measure how your tweak affected the results (it's okay if it made them worse). Most papers mention future work or current shortcomings that can give a hint about what you could try. If you take a relatively recent *or* less famous paper (maybe something by your professor?) the probability that this has already been done is smaller.

If you're looking for something that's (usually) simpler, there are a lot of challenges and competitions that provide a framework (less work for you) and ask you to write an AI for a certain task (or set of tasks). Winning will of course be difficult, but making something that works should be doable. (And then you can make a small variation on that, and analyze what this does to your results.) I'm thinking of things like [Robocode](https://robocode.sourceforge.io/), [The AI Games](http://theaigames.com/), [General Game Playing](http://general-game-playing.de/), etc. Or you could look at something like [AI Safety Gridworlds](https://arxiv.org/abs/1711.09883) and implement a new gridworld or learner (which typically shouldn't take too much computation).

Anyway, good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-14 00:54:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's already posted [here](https://www.reddit.com/r/artificial/comments/c06k1q/what_are_your_favorite_ai_articles_of_may_2019/). I think Sicara's round up is generally pretty good. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-13 00:28:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> In Russell & Norvig's taxonomy I've always been drawn towards the "rational" side, because the difference with the "human" just seemed to be that you'd have a bunch of mistakes and suboptimal decisions. I still mostly feel that way, but I am now more concerned with the value alignment problem. I don't necessarily think that a humanlike value system would definitely solve everything, but we definitely need something in addition to pure rationality due to the orthogonality thesis.

Having said that, I don't think these will all necessarily lead to different paths of research. For instance, it's my impression that Ben Goertzel is probably going for "acting rationally", but to achieve that he's building a system that looks, acts and thinks more-or-less humanly, because he thinks it's the best way to achieve that. Because even if you care about (super)human-*level* AGI and not human*like* AGI, a human*like* AGI would fit the bill *and* you could build off of an actual example (i.e. humans). Furthermore, the effective/realized intelligence of a system depends not just on its programming, but also on its "life" experience and what it learned, and we know that the human life experience coupled with human "seed intelligence" eventually results in adult human intelligence. If you build "seed intelligence" that's really great, but you don't give it the right experience, you still might not get good results, so by using humanoid robotics he increases the humanlikeness of the experience the AI is likely to encounter. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-12 16:22:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't really know a lot about this, but I'm pretty sure they've been using AI (especially planning / scheduling) in the railway sector for decades. Just googling "ai train traffic control" the top result is a paper from 1990 (by Komaya) and the third result is called "Recent Train Traffic Control Systems" from 1999 (by Katahira). I don't get a great high-quality feeling from [this EU report](http://www.europarl.europa.eu/RegData/etudes/BRIE/2019/635609/EPRS_BRI(2019\)635609_EN.pdf) but it's from March 2019, so it may be a decent starting point for you to look into what has already been done and what the remaining challenges are.

Do you have a thesis supervisor? If so, I would talk to them about this topic and whether they're aware of previous work in the sector. They may also help you get into contact with researchers working on this topic or railway employees who work with AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-12 15:49:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> There is some work using "random features", but I don't know it well enough to say if it's similar. You may want to consider asking on /r/MachineLearning or perhaps /r/MLQuestions. I suspect you'll get a better response if you phrase it as "What's this regularization technique for Gradient Boosting Trees called, where has it been used and what do you think of it?". Suggesting you may have invented something could be perceived as arrogant by some people, and if the answer to the previous question amounts to "interesting, but I haven't seen it before", you'll still have your answer. In any case, good job (re)inventing something that seems to work well for you! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-12 15:35:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are some resources on our [wiki](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-07 20:30:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> I haven't taken the course, but from the looks of it, it doesn't seem very applied to me. It seems to be approached more from a business and societal perspective. You might be having a similar problem that was asked about in [another thread](https://www.reddit.com/r/artificial/comments/bxgrgf/i_dont_think_im_learning_much_from_udacitys_intro/) so perhaps the suggestions there will also work for you.

A master's degree might be interesting for you, but it will probably mean 2 years without a salary in an academic program aimed at doing research. If that's what you want: great. If you just want to get a bit better with applied AI, then given your experience, you could probably learn it on your own with online resources, or maybe through a program that's more targeted on industry. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-07 20:11:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> Banned for out-of-nowhere over-the-line hostility against /u/victor_knight here and elsewhere. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-07 20:07:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for letting me know. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-07 14:27:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> No. There are more tools now, and they automate / implement / help with the stuff that was once cutting edge experimental research. That doesn't mean AI is *just* those tools now though: the cutting edge just moved, and now there are more advanced things we can do that are still in an experimental stage. And in the future, that will also become mundane, standardized and captured in handy tools that will enable yet more advanced work. And so on. That's what progress looks like. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-07 10:15:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your first paragraph seems related to the idea of the [philosophical zombie](https://plato.stanford.edu/entries/zombies/), so you might be interested in that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-07 09:35:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> I thought the course was pretty good as an introduction to AI and gave a nice overview of the field. However, that does mean a relatively shallow treatment of each of the 15 subfields they're covering quite briefly. I think there are more specialized and applied courses like Google's [Deep Learning course](https://eu.udacity.com/course/deep-learning--ud730) (also on Udacity) and maybe [fast.ai](https://www.fast.ai/) or something. And there are also videos on YouTube to teach you to "Do X in 5 minutes with Tensorflow" and things like that (e.g. by Siraj Raval). Maybe you would prefer something like that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-07 09:14:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> To elaborate on divenorth's answer:

Yes - to make intelligent decisions a system would need to have some way to distinguish the desirability of outcomes. You could call more desirable outcomes (more) pleasurable and less desirable ones (more) painful if you want. Reinforcement learning does this quite explicitly with rewards and punishments (i.e. negative rewards/utility).

No - if by pleasure and pain you are specifically referring to the subjective experience or qualia of pleasure/pain, then there seems to be no reason to think that's necessary to be intelligent. This sentience / phenomenal consciousness is often defined to be a non-functional part of consciousness. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-06 19:27:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'd say the clarification changes a lot, but I suppose it partially depends on who the target of your outrage was initially. If it was aimed at euthanasia law and the society that approved of it, this changes everything. If it was aimed at the parents and doctors involved, it probably changes less. Although in my mind "actively killed patient with lethal injection", "didn't provide treatment to prevent death" and "didn't force treatment on patient against her will" are all quite different morally as well.

I immediately thought there was something fishy about this story, but my primary concern if it was true was about euthanasia law not being as good as I thought. Turns out it's fine. We could still look at another law which apparently allows doctors to refrain from forcefully treating patients against their will, but I'm already much less shocked that this would be allowed. I don't really care about the judgement of random two people I don't know (the parents). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-06 16:52:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> PM'ed you because some of the questions are quite personal/identifying. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-06 08:42:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm an academic who does AI research (but not really development anymore). Are you interested in that, or do you prefer someone who works in industry on R&D? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-05 19:59:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> > it doesn't strike you as at all interesting that they did this at her home, rather than the hospital?

Not really. Why do you think they should be in a hospital? The whole point is that they agreed *not* to treat her. Being at home is a much nicer environment. As established, this wasn't euthanasia, but even if it was, doing so at home would be [standard procedure](https://www.levenseindekliniek.nl/en/).

As for the rest: let's just say I disagree with your intuitions. It seems to me that the people involved in this heartbreaking situation were eventually convinced that respecting Noa's wishes was the right thing to do. I have my doubts about that, based on the very limited information available, but I see no indications of any nefarious ulterior motives or shady behavior. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-05 19:05:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> According to this [new article](https://www.politico.eu/article/noa-pothoven-euthanasia-that-wasnt-suicide-mental-illness-anorexia/) on Politico I'm basically correct. (pinging /u/the_propaganda because they might be interested to know this wasn't really euthanasia)

> See, I may be operating under a giant misconception, but from what I understand not only did they have a choice, it's quite standard to have people who are attempting to take their life away committed until they get better. If anything the situation strikes me as the complete opposite of what you said - that they actively went along with her decision, and just did the bare minimum to not get persecuted.

A relevant line from the article:

> Her parents and doctors agreed not to force feed her or compel her into treatment against her will.

As the article explains, this isn't euthanasia, and it's basically as I thought, but I can imagine that to many people this reads as a technicality and that the article's attempt to address this is inadequate.

I don't know the Dutch law well enough to say if doctors and/or parents could have legally made a different choice. I think that for certain kinds of medical interventions you certainly cannot go against the patient's consent. I don't know if forced commitment, feeding and treatment were a legal option in this case.

If it was, then the doctors and parents did have a choice and they choose to respect Noa's wishes. I don't know if that necessarily contradicts what I said: that they didn't agree with her decision but tolerated it. From the little information we have, it certainly seems that they didn't agree. The (doctors at the) clinic said as much in the quote from my first post, and according to the [Daily Mail](https://www.dailymail.co.uk/news/article-7103019/Dutch-girl-17-legally-ends-life-euthanasia-clinic.html) the mother said "We, her parents, want her to choose the path of life. Noa really doesn't want to die at all. She only longs for peace". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-05 17:04:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm going to charitably assume that you don't mean to suggest that this girl *ended her life* for a *publicity stunt*, or because she felt so strongly that euthanasia should be available to girls like her, that despite not really wanting to die, she was willing to die to make that happen.

You seem to think that the specific method she chose for her suicide was staged for a broader audience in order to achieve secondary objectives like selling a book or making euthanasia legal in cases like this. I won't deny that this is possible, but I don't think it's likely.

I will mostly ignore the "selling her book" possibility, because it seems that any suicide method could have accomplished this equally well.

So why did she choose this method? We know that legal euthanasia was her preferred method to die, so it seems plausible that even in the absence of any secondary effects, she would choose a method that resembles it as closely as possible. Alternative or additional explanations seem unnecessary to explain this choice.

But let's examine the alternative explanation that this was done to make euthanasia legal in such cases anyway. First of all, we might have expected her to say a bit more about that. Her note doesn't even contain a "this would have been a lot easier with an injection" remark. We haven't seen any real advocacy for this position from her.

Secondly, while it's possible that other people who were involved will do this later, this seems unlikely. The reporting we have says her parents wanted her to live, and the end-of-life clinic denied her application. It seems nobody involved really agreed with her. I guess they did in the end tolerate her decision to some degree, although I'm not sure if they had a choice from a legal perspective.

Also, for a publicity stunt there appears to be an awful lack of publicity and *seeking* publicity. Like I said: she made one Instagram post where she doesn't even say anything about euthanasia. Virtually no Dutch newspapers have reported on this, and there's no national conversation on TV or anything. This somewhat surprises me, because I would think that at least the more sensationalist outlets would find this a salient story, and indeed a conversation starter. I'm quite sure that *if* anyone involved sought publicity, they would get it. But so far that hasn't happened.

The only ones who seem to have sought publicity are foreign media like the Sun and the Daily Mail, probably because *they* thought it would outrage their readership; especially in the way they're reporting it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-05 12:52:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm somewhat doubtful this is actually legal euthanasia. Hear me out.

> Good luck finding a mainstream article on this; the closest to the mainstream I can find was the Daily Mail and the Sun, despite the fact that it's being reported by reliable Dutch newspapers.

Is it? I don't see anything in the Volkskrant, Telegraaf, Trouw, NRC, Parool and NU.nl. [De Gelderlander](https://www.gelderlander.nl/arnhem/noa-is-17-jaar-geworden-ik-word-losgelaten-omdat-mijn-lijden-ondraaglijk-is~a7a2cc47/) and [AD](https://www.ad.nl/binnenland/noa-is-17-jaar-geworden-ik-word-losgelaten-omdat-mijn-lijden-ondraaglijk-is~a7a2cc47/) are reporting on it, but are not explicitly saying it was euthanasia. Can you find a reliable Dutch newspaper that does?

If you look at the girl's goodbye message on Instragram, she expresses uncertainty about when she'll die (max 10 days), that she's starving herself and that people are going to let her. (Active) euthanasia would have a doctor give her a lethal injection, so there should be no uncertainty about when that would happen and there'd be no reason to starve herself. Letting her die is obviously not nothing, but it likely doesn't fall under euthanasia law, and would probably be possible in a lot of other countries without legal euthanasia, as long as they allow patients to withhold consent about medical treatments (but IANAL).

The Gelderlander does mention she *enquired* about euthanasia at an [end-of-life clinic](https://www.levenseindekliniek.nl/en/), but not that they granted her request. In fact, if you go to the [earlier article](https://www.gelderlander.nl/home/noa-16-uit-arnhem-is-nu-al-klaar-met-haar-verwoeste-leven~a01a7bd1/) they published on that, it becomes clear that they didn't:

> ,,They think I'm to young to die. They think I should first complete my trauma treatment and that my brain should be fully developed. That takes until you're 21. I'm devastated, because I can't wait that long anymore.’’

That was in December, so I guess it's possible that things have changed, but I think there are a lot of indications that this wasn't a case of legal euthanasia.

This is probably the reason Dutch media has mostly not picked this up, and it is mostly reported by (sensationalist) foreign outlets from countries with a hot euthanasia debate, where it sounds plausible that this is indeed Dutch law without having to investigate it any further. To be fair to them, this story does look quite vague and a lot like euthanasia, so this may be a genuine misunderstanding combined with not super good journalism (e.g. some papers were reporting she could do this because she was 17, but the rules for 16-year-olds are actually exactly the same). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-04 10:29:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> * [Artificial General Intelligence: Concept, State of the Art and Future Prospects](http://www.degruyter.com/view/j/jagi.ahead-of-print/jagi-2014-0001/jagi-2014-0001.xml) by Ben Goertzel (2014)

* Pei Wang's [Intro to AGI](https://cis.temple.edu/~pwang/AGI-Intro.html) (not really a comprehensive survey, but a good intro with links to other approaches as well)

* [A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3070741) by Seth Baum (2017) (I think there are some accuracy issues, but it serves fine as a list of projects for which you can look up the professor(s) who could potentially serve as your PhD supervisor)

* [ Review of 40 Years of Cognitive Architecture Research: Core Cognitive Abilities and Practical Applications](https://arxiv.org/abs/1610.08602) by Kotseruba & Tsotsos (2018) (another huge list of projects)

There are a lot more, but I think this should do for now.

Edit: by the way: I think Pei Wang has less experience supervising PhD students, but he does finally seem to have some funding for a research group, so I think this may also be a pretty decent place to do a PhD. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-04 10:20:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> Excellent reply!

> I haven't seen anything better that has a program in place; maybe Goertzel, but you aren't going to get a PhD at Singularity.NET.

It's true that you can't get a PhD from a company (Hanson Robotics and the OpenCog Foundation would probably be more relevant than SingularityNET though). However, Goertzel has in the past worked together with (I think) Hong Kong Polytechnic Institute and possibly other universities for this purpose. If /u/bobmichal thinks he'd prefer to work with Goertzel, I recommend he reaches out to him and inquires about possible solutions.

> Alternately, Schmidhuber but they are the same approach.

Schmidhuber's Gödel machine is indeed somewhat similar in approach to Hutter's AIXI (Hutter was Schmidhuber's PhD student). However, Schmidhuber is also doing a lot with deep learning. And if you look at [his company's team](https://nnaisense.com/#team), his director of AGI and Eric Nivel are probably more inclined to work with a (cognitive) control architecture like AERA. One problem might be that I don't know if Schmidhuber still takes PhD students, because I think he moved most of his efforts from IDSIA to NNAISENSE. But I''m not sure about that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-04 10:06:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> I personally prefer the cognitive architecture approach to Hutter's algorithmic information theory approach, but I think you could do much worse than to do a PhD with Hutter. His idea is, I think, not completely unreasonable: to try to formally understand what it is that we want to achieve with respect to AGI and then approximate that. If you're very mathematically inclined, it's nice that every step is very rigorous and can be reasoned about. But despite the existence of some more practical approximations (like MC-AIXI), I don't think it offers much practical guidance on how to actually build AGI. What I do think it offers is a nice notion of an idealized intelligent agent that can be reasoned about, e.g. in the context of evaluation (AIQ) and/or AI safety. (There are other issues with AIXI, such as the inability to model itself, but that's another challenge to work on and *maybe* starting with an idealized version is a good idea.)

Aside from the question of whether Hutter's approach is the most promising for reaching AGI, there may be some nice things about doing a PhD with him. As Beelzebub points out: at least he's working on AGI. The same cannot be said for most AI researchers. Furthermore, among the university professors who are into AGI, he may be the most prominent and from an academic perspective he seems to have his shit together with a well-funded research group. He has graduated a decent number of (PhD) students, who now all seem to be working at DeepMind (co-founded by Hutter's student Shane Legg). He also seems to be okay with letting his students work on what they want: I'm told he doesn't really believe in Bostrom/Yudkowsky-style AI risk, but quite a lot of his students are working on it, and although I guess his mind isn't open enough to be completely convinced by them, he does co-author their papers and even let Tom Everitt write his thesis on "Towards Safe AGI". So basically, it seems like you wouldn't even have to work on AIXI, but the mathematical skills you'll acquire seem valuable in any case.

Of course, Marcus Hutter is not the only professor working on AGI. You can find other people working on it through /r/artificial's [wiki](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F). You may specifically want to look at survey articles and people serving on the program committees of AGI conferences or the editors of the journal. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-06-03 16:13:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> * 6x Liverpool
* **5x Ajax**
* 3x Tottenham
* 2x Barcelona
* 2x Man. City
* 1x Lyon
* 1x Juventus

\* I miscounted at first and thought Liverpool had as many as Ajax, but this is still pretty damn impressive. And well deserved! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-29 15:54:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I am currently at an internship for software safety and reliability.

Don't you have to take this into account? What you're suggesting now seems like it's more about sociology. I realize you're making a game for that, but the topic is not about the safety and reliability of your game software. Don't get me wrong: I think it's an interesting topic, but I wonder what your supervisors will think of it.

If you want something else, I recommend that you take a look at the Future of Life Institute's [Research Priorities for Robust and Benefcial Artifcial Intelligence](https://futureoflife.org/data/documents/research_priorities.pdf?x60419) and [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565) (and derived work e.g. about [gridworlds](https://arxiv.org/abs/1711.09883)). Rob Miles also has several videos on his [YouTube channel](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/featured) where he talks about the different aspects of that paper. These documents (and others) suggest some research topics that I think are more directly related to software safety and reliability. Another thing you might do is write a paper on which/how current software safety and reliability methods and practices could be relevant to AGI (or argue why it's a fundamentally different problem). For inspiration on topics you could also look at what people have been doing at the first two [AI Safety camps](https://aisafetycamp.com/previous-camps/). They work in groups of 3-5 people but they only have 1 week full-time + some preparation time, so this might be approximately the size of project you could do (although you should perhaps aim slightly lower because they often don't finish and typically have at least one person who is a bit more senior than you).

To make this slightly more concrete: you could perhaps implement some ideas on safe exploration or (adversarial) robustness. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-28 14:58:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't have a great view of what's used in industry, but evolutionary computation is a big and broad area of AI. You probably already found this, but Wikipedia has a [List of genetic algorithm applications](https://en.wikipedia.org/wiki/List_of_genetic_algorithm_applications#Industry,_Management_and_Engineering) and here's a book called [Industrial Applications of Evolutionary Algorithms](https://www.springer.com/gp/book/9783642274664), and you can find a lot more. And here's a cool [TEDx talk](https://www.ted.com/talks/maurice_conti_the_incredible_inventions_of_intuitive_ai?language=en) on generative AI.

Evolutionary methods actually seem so useful to me that I wouldn't even be super surprised if it turns out that they're just as often as neural networks, at least among people who are not just responding to the latest hype. Apparently it's not that easy to get deep learning working in a lot of cases, which is why things like XGBoost / random forests are also still used a lot in practice. When I was in undergrad (I think), I encountered a researcher who said genetic algorithms were basically the second-best solution to any problem, and that it's a great thing to try first to see if it's good enough for your use case because it was so easy to apply. This was 10-15 years ago, so before the (re)popularization of deep learning, but I suspect it still remains mostly true.

You could also combine evolutionary methods with neural networks in various ways if you want (see [Neuroevolution](https://en.wikipedia.org/wiki/Neuroevolution)).

So I don't really think you have to worry about not being able to apply evolutionary computation in the real world. At least not any more than with e.g. deep learning. I guess it's just less likely that you'll be hired purely based on hype, but that's actually not so bad. What you should worry about is whether you think this is interesting and want to spend 3-10 years on in your PhD. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-27 02:15:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-26 22:52:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> I have a lot to say about this, but not much time right now.

Some background on me: I did my PhD in a small university in Europe after I already had a master's degree and some research and industry experience. Things may be different if you're enrolling in a US PhD program right after your bachelor (or in other situations).

First, you have to figure out if a PhD is really what you want. What kind of job do you want, and does that require a PhD? A lot of the time it doesn't. PhDs are (mostly) for doing research. Do you want to spend the next 3-10 years of your life on that? PhDs often earn more than non-PhDs in e.g. programming jobs, but getting 3-10 years of programming (or whatever) experience will *also* increase your salary *and* you would have been earning (much more) money during that period.

If you're applying for a PhD position, there are a lot of things to consider. Aside from topic fit, I get the feeling a lot of people are led by prestige of the institution or the PhD supervisor. I'm not saying to ignore this entirely, but I would also look at whether it is the right environment and supervisor for you. Everybody works in a different way. Do you want a supervisor who tells you what (research) to do (good if you don't know), or who lets you do your own thing (good if you know what you want to research)? Hands-on or hands-off? It's probably good if one of you is very ambitious, while the other is good at tightening the scope of your research to make it actually doable.

I'm sure there can be advantages to having an inexperienced supervisor (they might have more time for you), but generally I'm inclined to recommend looking for an experienced one who has already successfully supervised many students. They have likely learned from the experience and gotten better at it, and the more students they supervised the more situations and personalities they likely had to deal with (so odds are they'll be able to deal with yours). If you can figure out the rate of students who dropped out under their supervision, that would also be great, and you can also try to look up where students who did graduate ended up (e.g. homeless or full professor at a prestigious uni).

I would also personally try to look for a large-ish research group. It may mean your supervisor has less time for just you, but this can be somewhat compensated by discussions with others in the group. It's great for networking and (co-)authoring papers as well, and there might be (more) infrastructure in place for you to do your work (saving you time). I was my supervisor's only PhD student (at that time) and it meant that for every (weird) idea that popped into his head, I was his go-to guy. In some sense this is great, because all those ideas go to me and not someone else so if they pan out, I'm the one who gets the nice research (together with my supervisor), but in practice it was mostly just very time consuming for me.

This will have to do for now... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-24 01:27:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> In that case the effects mentioned in my post do indeed seem irrelevant. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-24 00:53:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is just speculation, but it could be that the group of conscripts has changed. According to [Wikipedia](https://en.wikipedia.org/wiki/Norwegian_Armed_Forces#Conscription) only 1 out of 7 Norwegians liable for military service were actually conscripted, which were typically those who were motivated to serve. It seems very possible that as Norway moved from mandatory service for everyone to "basically-at-will", higher-IQ people opted out at a higher rate because better / more prestigious options were open to them. One (wildly speculative) reason for why the US hasn't seen the same effect as the mentioned countries might have to do with how the military is viewed.

Also, it seems that the gender ratio may have changed (I wouldn't necessarily expect this to have huge first-order effects, but it may have second-order effects on how military service is viewed and the composition of the group). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-21 19:07:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> "Mind control" is perhaps a bit strong, but AI certainly is capable of various forms of manipulation. Right now, I think it would be difficult to ascribe this to a conscious decision-making process on the part of our AI systems, but that doesn't mean it doesn't happen.

AI can be used to keep you more effectively in the Skinner box of online games and social media itself. Ads are obviously intended to manipulate your thoughts, desires and behaviors, but the same is true of many other articles. Again, AI can help manipulate you towards certain viewpoints (albeit not with 100% reliability). One thing I've heard, but cannot really confirm, is that social media is essentially training us to become more predictable which is done by making us more polarized.

For any kind of AI I would certainly say that if it *can* use "mind control" (or manipulation), then I would by default assume that it would also use it. I'd need a reason for why it *wouldn't* do so. After all, to an amoral AI it's just another tool in the belt to accomplish whatever its goals are.

I do actually also see issues like these discussed semi-regularly in places dedicated to the superintelligence control problem like the AI Alignment Forum or LessWrong (not so much on /r/ControlProblem if I recall, but that would be the main place on Reddit). Basically, if you want your safe ASI to optimally satisfy your human preferences/values, it might make sense for the AI to try to make your preferences simpler and easier to satisfy. Or something like that. I don't recall exactly, but hopefully you can find some discussion if you search those places. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-21 18:13:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> I only skimmed the first paper and read the abstract of the second. It looks to me like the first paper builds on the second one, as it's cited on multiple occasions. My impression is that the *interpretation stage* is largely taken from the second paper, but that the *observation stage* is new. Furthermore, this is more specifically aimed at AI. My guess is that this isn't some huge landmark paper that changes everything we knew about AI, but that it's like 99+% of other papers that incrementally builds on previous research.

By the way, this paper is by Virginia Dignum (and her husband)'s new group in Umea, but she worked for many years as a professor in Delft with Ibo van de Poel, so that might explain if the ideas are a bit similar. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-16 21:09:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> AFAIK it's not officially available for free, but my first result on Google for "pattern recognition and machine learning bishop" is a full-text PDF that someone at Lisbon University seems to have uploaded on their user page.

I know most books are "available for free" if you look for them on shady sites, but this simple availability when simply searching for the name may have confused some people into thinking it is indeed available for free... (I'm actually surprised this is somehow my number 1 (non-sponsored) result above the official Springer website, Amazon, etc.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-13 11:28:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I'm interested in neural networks, consciousness and specifically BCI's (brain computer interfaces).

As you already seem to have (sort of) realized, this suggests a path of computational neuroscience to me. They should definitely cover neural networks and neural correlates of consciousness (which is not really the full philosophy of consciousness, but I don't know what you prefer), and it's probably the best major for BCI (I don't know if it's always explicitly taught; but I don't know any program where it definitely would be). With most computer science degrees you'll probably (not even certainly) just cover neural networks. If you can get a PhD, that's great, but you could always start with a master's degree. Another thing you could perhaps look into, given your background, is neuroeconomics.

For your gap year, you could try to do things that are not adequately covered by your next degree, or that you can never be good enough at such as mathematics or maybe programming. You can also use it to build your resume and network. A research/dev job or internship would be cool. I don't know much about entrepreneurship, but my feeling is that it's better to do this if you already have some experience (to put it bluntly: commit your inevitable newbie mistakes on your employer's time), and that you shouldn't try to squeeze it into a single gap year. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-13 11:15:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> Mostly through testing. You typically keep some part of the data set apart (the test set), which is not used for training and only for evaluation. If it performs well on this new, unseen data, you assume that it's safe. You can go beyond this by selecting or constructing the data in a specific way (e.g. adversarially) to test for different kinds of robustness.

Furthermore, you could still design the system to have some safety features from the ground up (such as safeguards to prevent certain anticipated failures), and a big part of safety is the context in which the system is embedded. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-13 11:11:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> People have trained chatbots on the texts of deceased loved ones (see e.g. [Replika](https://www.theverge.com/a/luka-artificial-intelligence-memorial-roman-mazurenko-bot) and [Eterni.me](http://www.eterni.me/)), so I don't see why it couldn't be done with tweets. Twitter bots (like Microsoft's [Tay](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)) also exist. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-13 11:07:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> They released smaller versions of the trained model, which are still somewhat useful. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-12 14:17:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay...

I think the paths to AI for Law (AI4L) and Law for AI (L4AI) are potentially pretty different though. Basically, for AI4L you would primarily want to have an AI background, while for L4AI you'd primarily want a Law background (ideally specialized in technology somehow). For L4AI, you would probably not have to change that much in your career: just read a few books on AI, and then dive into the topic from a law perspective. For AI4L, you would probably want to take a full AI / machine learning / computer science education so that you could make AI systems for any application domain, which in this case just happens to be law (where you'd have an advantage). That is, unless you just want to be the (law) domain expert on a project like that (but then you wouldn't really be the AI researcher/developer). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-12 14:00:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think it's very good that ethics are getting more attention and researchers and especially research companies are taking / having to take more responsibility for the impact of their work. At the same time it's a difficult issue, and we shouldn't refrain from publishing anything that may possibly perhaps cause something negative in the future. The future is hard to foresee, and most technology can be used to somehow further the user's goals (which may not necessarily be good).

What you should do probably depends on the nature of your actual work. If it's just an incremental improvement on something else, I don't think much needs to be done. If you want, you could mention in the Future Work some ideas for making the technology more ethical / beneficial to society.

If you're going to discuss ethical concerns, it naturally raises the question of "what are you doing about it?". It sounds a bit odd to say "my work could have terrible consequences XYZ if misused, here's the source code y'all!". In that case you're probably better off not mentioning it at all, and hoping the readers don't think of the ethical implications themselves (I'd say the odds for this are unfortunately pretty good). Actually, since this is a master thesis and not (yet) a high-profile publication, it's quite likely that virtually nobody except for your supervisor(s) will read it, so I would probably just try to gauge their opinion on this.

If you think there are significant ethical concerns with your work, there are different ways to address it. One would be to not publish anything at all. I get that you need to write that thesis to graduate, but at least in the universities I've been at, theses could be sealed from the public, so only your supervisor(s) would know about it. If you do publish, the question becomes what do you publish? On one extreme end of the spectrum you could publish everything including source code, excellent documentation and all details needed for reproducing your work. That's kind of the ideal if there were no ethical concerns, but if there are, you could try to assess if the benefits of making reproduction less easy outweigh the costs. For instance, if you're thinking about publishing everything except the source code, you should think about who this is likely to stop and whether that's significant. If the main threat comes from widespread use by hordes of technologically unsophisticated trolls, then omitting the source code (or a running model) could have a significant effect, because most will be incapable or unwilling to expend the effort to reproduce your work even with detailed instructions. If you want even more control, you could omit even more detail and e.g. only make it available to individual researchers upon request.

If the main concern is unintentional misuse, then you could also detail how this might happen and explain how it could be avoided. For instance, if you're afraid your better learning algorithm could be used to discriminate against minorities, explain what can be done to avoid that.

Finally, even if there are ethical concerns, you could justify your work by explaining how the benefits (of the work *and* detailed publication) outweigh the risks. For instance, people have argued that publishing "deep fakes" is good and important as a warning to society to stop trusting images/videos/audio/text/etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-12 13:21:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. In addition to Andrew Ng's other course that Karl already recommended, he now also has a course called [AI for Everyone](https://www.coursera.org/learn/ai-for-everyone) which may be very interesting for people approaching AI from other professions.

It's not super clear to me what you want exactly. Why do you want to study AI? What work would you like to do? What aspect(s) of AI are you particularly interested in? Etc. You mention combining it with your law background. Are you more interested in AI for Law (e.g. automating part of a lawyer's work), or Law for AI (e.g. legislation to ensure AI benefits society)?

In any case, you're still easily young enough to start on this. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-08 01:10:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> Justifications in AI papers are very similar to what you describe in biology. If the paper is specifically application-oriented, like e.g. research into some disease, you'd see the exact same thing. For fundamental/basic research, it's a little bit different, and the justification is usually that it addresses some problem with previous techniques. In these cases real-world impacts are often not directly taken into account at all.

We are increasingly seeing calls to change this; for AI researchers to take responsibility for how their technology is or can be used. For instance, there are questions about military applications, the potential for discrimination, and the potential for misuse of e.g. OpenAI's language model (GPT-2). This is usually easiest to "police" this when the technology is close to application and/or when a researcher/company collaborates with certain partners, but it's almost impossible for basic research (e.g. should the backprop algorithm not have been published, given that it could be used to discriminate against minorities?). In addition to calls to *not* do certain kinds of R&D, there are also more initiatives to work on things like explainability, privacy, AI ethics and A(G)I safety. But while some researchers certainly make a career of that, many others stick to "merely" solving standing problems in their field. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-06 23:26:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> Clearly explain the difference between narrow and general AI and between intelligence and sentience. Given your title I would indeed focus on AGI (so not narrow or strong/sentient AI), and the efforts made in that direction. You mentioned you already talked to Ben Goertzel, which is great, although I wouldn't solely focus on his POV (there are already documentaries about him). He did write a good survey though, that gives a nice overview of difference approaches and perspectives. Also check out the slides from Alexey Potapov's tutorial at the 2017 AGI conference and Pei Wang's intro to AGI. I'm on mobile, so I'm not linking things, but there may be a bunch of useful links in the Getting Started article on /r/artificial's wiki. Aside from focusing on the AGI community/society, you could also look at companies like Deepmind, Facebook (different threads from Tomas Mikolov, Marco Baroni and Yann LeCun), OpenAI, GoodAI, etc. And maybe things like Gary Marcus's criticism of deep learning. I think this all fits well in Beelzebub's suggestion.

If you do end up focusing on potential impacts of AGI, please really explore the arguments of Stuart Russell, Nick Bostrom and Eliezer Yudkowsky very carefully. They are basically correct, and their critics typically aren't even aware they exist. There's more info on /r/ControlProblem. I think you could address this without being overly negative, but I understand if you want to focus on something else.

Above all, I think you should convey the immense uncertainty that exists in this area (but not in scary way). Nobody (including professionals) agrees on when we'll get AGI, how to get there, and what will happen when we do (although you can do Bostrom-esque analyses). There's work on all of that (including on predicting / forecasting such things), and you could make an interesting documentary on that, but one failure mode I see a lot in documentaries (in all fields) is that they pretend they know the answer.

Good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-05 21:40:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> ¿Porque no los dos?

I hope AI won't destroy the world, but if it does, humans will be ultimately responsible for it anyway. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-02 22:56:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I don't think when we get to a point of having AGI it will be some overnight transformation. It will be a very gradual expansion of the capabilities of AI as we have today.

I think this is a *major* assumption. I'm not saying it's 100% certainly false, but I certainly wouldn't want to bet the future of humanity on it. Neither would I want to bet it on the off chance that it really does happen a lot sooner than we think. [Estimates fluctuate greatly](https://aiimpacts.org/ai-timeline-surveys/) and [Yudkowsky makes a good point](https://intelligence.org/2017/10/13/fire-alarm/) about how you and I will probably not even know when AGI is close.

Furthermore, the history of AI and technology in general is that we're typically lagging behind with solving the issues. We are now working on things like ethical guidelines and technical solutions for things like privacy, fairness and transparency, but that hasn't prevented us from creating opaque, privacy violating, unfair AI systems. I don't want the same to happen when the stakes are massively higher.

> Yes I know. Call me biased, but I happen to think it's the best way XD

I think it's fine to think that (although I disagree), but I don't see a reason to be so certain about it to dismiss all other possibilities. I'm not necessarily saying you're doing that, but I just want to point out that you can simultaneously think "my favorite method will probably lead to AGI first" and "but we should probably prepare for the eventuality that I'm wrong".

> Thank you for this information.

No problem. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-02 17:15:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> > You're arguing that NNs are a small part of the scope of AGI

I'm specifically arguing that it tells you virtually nothing about AI safety / risks from AGI / the control problem that /u/2Punx2Furious pointed out.

> However, it will not be in the form of some super-human Hal that needs to be controlled from destroying humanity.

Many people say this, and they pretty much never address any of the many arguments that have been put forth by AI safety experts.

> As someone that studies the brain, we are not anywhere near reaching the level of understanding to create an AGI agent that would make the control problem relevant.

You know studying the brain is not the only—or even the most commonly taken—path towards trying to create AGI, right? Anyway, even if you agree with e.g. Bostrom that AGI isn't particularly close, when would you suggest we start working on the control problem? How long do you think that would take to solve?

> In any event, I am curious, what types of architectures are you studying that do not incorporate some type of NN?

NNs are very useful and *currently* among the best ML/optimization approaches we have. Still, they are not at the core to many approaches to AGI, including e.g. AIXI, NARS, Sigma, AERA, etc. In some cases (like OpenCog) they are used as one of the many approaches, and in others (like NARS) there is maybe one research paper that investigates if it makes sense to use NNs as a vision preprocessor or something. But even knowing and understanding everything about NNs wouldn't tell you much about how these systems actually work. See our wiki's [Getting Started article](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) for more info. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-02 08:32:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Lol I build neural nets as part of my research career.

This is a very informative statement. If you knew anything about this area at all, you'd know that NN experience is almost completely irrelevant. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-02 08:25:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> You should say a little bit more about what the survey is for, what kind of participants you're looking for, what their rights are, etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-01 17:51:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> FYI research from the AGI Society (linked on that wiki) often does look back to the old GOFAI stuff. It can't really hurt to look at old cognitive architectures like the ones you mentioned, but I also think it's good and proper to look at newer ones like OpenCog, NARS and Sigma. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-05-01 14:43:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's some information on /r/artificial's [wiki](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). It doesn't say anything about A(G)I Safety, but you could perhaps find resources on that on /r/ControlProblem.

There are many potential paths to AGI. As an undergrad you could try to study something relevant (like AI, ML, CS, mathematics, data science, cognitive science, neuroscience, philosophy). You can't really major in AI/ML in most schools, and CS (and data science) will probably provide the easiest path into a career in AI in the sense that a lot of fellow students and professors will be interested in it. You could seek out clubs, societies, internships, events, etc. that are relevant to AI.

Ultimately, to maximize your contribution you're probably going to want to get a PhD, so you should be preparing for that as well. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-30 01:18:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> I expected "burn me". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-29 11:37:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm a bit confused by the question, because I'd say that almost by definition computer scientists should be taught computer science in college. I guess the question is: what should that CS curriculum entail? And aside from the typical computer classes, I would expect there to be a lot of mathematics and logic, as well as some philosophy and ethics of technology. I guess I think almost any education could benefit from some philosophy and ethics, but that it can generally be specialized to the topic (e.g. computer scientists have somewhat different concerns than psychologists and classical philosophers). I would probably also include some "human factors" in the curriculum, including how people (can) think, feel, perceive and act/move. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-28 22:23:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't have anything specific for you, but /r/HealthAI/ might have some links. Also, if you can't access a paper, you can always try Sci-Hub, LibGen, or ask for it on /r/Scholar. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-28 10:26:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> Machine learning </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-28 00:52:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> What exactly is the added value of calling the algorithm (gradient descent) racist even with this context? You said it yourself: it's not accurate. And I agree that maybe very thoughtful AI/ML professionals would recognize what you were trying to get at, but those are probably the ones you wouldn't even need to tell this in the first place.

If the miscommunication happens here, we might say both parties are acting imperfectly, but in this case it seems that you have more knowledge of the situation than the person you're telling about this phenomenon, so I think it would then be a good idea to clarify what you mean if you notice that you're not being understood. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-27 14:11:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think it's good to be very precise about what is meant by words like "algorithm" and "racist". "Algorithm" in our current context seems to refer to things like linear regression, backprop, etc. "Racist" can range from "the worst thing a person can be", to "feelings of animosity towards people based on their race", to "prejudice+power", to "bad outcomes for non-white people", and unfortunately a lot of people strategically equivocate between those.

All algorithms have (inductive) bias, and I agree that on some data sets they might learn a model that has 4% error on a certain pattern rather than 5% error spread across all kinds of different patterns. If your data set has 75% of one class (white), 12% of another class (black) and 5% of yet another (Asian), then this could obviously happen, and you could debate whether the data set is biased because it's skewed or not because it's representative of the US population.

But it still feels weird to call "linear regression" or whatever *racist* specifically, because there's nothing in there about race at all. If the "race" variable is "sexual orientation", suddenly it would no longer be racist, but perhaps homophobic. If the bit was flipped, suddenly it'd be making more errors on whites, which according to the "prejudice+power" people isn't racism at all. If it's about favorite colors... So basically it's all about what the data actually means, but these algorithms do nothing with the meaning of the data.

Having said that, if you consider it a problem that some algos prefer 4% error on one cluster, vs. 5% error that's more distributed, then you could certainly argue that we should make algorithms that don't do that. Perhaps even because that would generally be better for minorities. But that actually still depends a lot on how the system ends up being used, because maybe making more errors on minorities ends up being in their favor in some settings. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-26 23:56:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think your posts have mostly been very good, and even this one wasn't confusing. The thing that's confusing is that a lot of people are saying things like "AI algorithms are racist/sexist/etc." and that is what is eliciting this response from AI/ML developers who (often) don't know exactly what's meant. I think you were correct about the fact that they're not really disagreeing with anything substantial and that this is making the whole debate with *actual* dissenters harder, but I think the best way to solve this is for the people who know most about this (i.e. people studying this stuff like you), who are often/sometimes making these statements, to use your superior knowledge to explain what's going on, rather than vilifying the people who misunderstand you (calling them immoral etc.).

Edit: by the way, I really appreciate you making all these posts and I think the work you do on bias/fairness in AI is important. I also hope that everyone in the field can get on board with that. (Although I also want to say that not everything that turns out bad for some sympathetic group should automatically be decried as racism/sexism/etc. I'm not saying you're doing that, but I do feel this happens too often.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-26 23:48:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> I somewhat disagree with this post, because I think you're being sloppy about what you call an algorithm and it is fuelling some of the problems you describe in your fourth point.

> I’m not going to say “neural networks are biased against black people, use logistic regressions instead.” It seems somewhat like that’s what you would want to count as a biased algorithm.

IME, this is what many AI/ML professionals are hearing when you say AI/ML algorithms are biased. They *are* hearing "gradient descent is racist" or "2x+7 is sexist" or whatever, and they rightfully think that's ridiculous. They might not disagree with you if you (and others) described more clearly what you meant.

> I believe the correct thing to evaluate is a deployed model.

This is in my opinion more or less correct, although I would say the "*system* as it is applied/deployed in a certain context/situation". The model is a part of that system, and the (NN/linear regression) algorithm / math together with the data created that model. But as you seem to agree its not that algorithm/math that is racist, but what's (potentially) racist is the way in which the resulting system is applied/deployed.

> Everything comes from somewhere. Math for the sake of math isn’t ever racist or etc, so you’ll always find something else you can ascribe it to if you’re insistent enough.

You make this sound like a bad thing, but in my opinion you *should* find that something else (AKA the actual thing that's causing the racism/sexism/bias).

> The algorithm is systematically harming people in an immoral fashion, despite your protestations that it’s “not really the algorithm’s fault”

I think you mean the *deployed model* (or system), not the NN/regression algorithm. This is the kind of sloppy use of language that causes confusion with AI/ML developers.

> When you do this, you’re not really substantively contributing to the conversation about fixing the problem, nor are you disagreeing with anyone. I don’t know a single AI researcher or developer who thinks that the equation 2x+7 is racist.

I agree, but I think this is in large part because "the conversation" is poorly communicated to these "participants". If you wanted to have a conversation about why most prime numbers are odd, and you begin it by asking "why are the primes odd", someone who correctly points out that they aren't is not contributing to the conversation you wanted to have, but it's not exactly their fault.

> When you do this, you’re peddling a narrative that is widely used to hurt people, whether you are trying to reinforce those points or not.

I somewhat agree, but again a lot of this is caused by not properly explaining what you mean.

> tldr: I care a lot more about talking about and fixing the problem of an algorithm making racist predictions than hurting the algorithm’s feelings by calling it racist.

Then explain what you mean in the language of the people you're explaining it to. Why do you insist on saying "AI algorithms can be racist", when you could just say "AI systems can be applied in a way that is racist / has disparate outcomes in certain situations, and if you're developing an AI system for those situations you need to do it in a way that this is not the case". Not as pithy, but it avoids all of this unnecessary confusion and animosity.

I'm not even saying that there's a single definition of "algorithm" and that you're using it wrong. But if the people you're talking to clearly don't know as much about the issue as you do and this is causing confusion, I think explaining it to them in their terms is a lot more helpful than doubling down. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-26 23:00:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> Cool! They usually alternate between North America, Europe and Asia/Australia, so it may come closer next time. Last year was in Prague, so I expect next year to be in America. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-26 00:19:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> In my experience the [AGI Society](http://www.agi-society.org/) is not well known among "mainstream" researchers, but they have a few hundred members (researchers) and organize an annual [conference](http://agi-conf.org). There are more links in our wiki's [Getting Started article](https://www.reddit.com/r/artificial/wiki/getting-started). That's a partial answer of "who is working on it". Of course, Deepmind probably has over a 1000+ researchers now, which would make them singlehandedly bigger than this entire community.

Your questions are quite complicated, and unfortunately I don't have much time now to think about them right now. But other people have! [This old thread](https://www.reddit.com/r/MachineLearning/comments/72dus0/discussion_serious_what_are_the_major_challenges/) is about what major challenges need to be solved for AGI where I linked some articles with challenges, roadmaps, milestones, etc. I also think [Ben Goertzel's post](http://multiverseaccordingtoben.blogspot.com/2011/06/why-is-evaluating-partial-progress.html) on why evaluating partial progress is hard is relevant to your question on intermediate goals. (There are more links on AGI evaluation [here](https://www.reddit.com/r/agi/comments/52tv08/benchmarks_besides_turing_test/).) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-26 00:04:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> If AI Now had a shred of honesty, they would call their "reports" "manifestos" or something like that. I've looked at a couple and while they are peppered with references, they're mostly not to academic papers but to shit online articles that share their world view or are simply misinformed.

I remember the most recent yearly report saying something along the lines of "it's time to look at the relation between the composition and the field and the bias exhibited by AI systems", and then they cited one paper from 1980 about how products often reflect the organizations that make them and 1 tweet from some random ML researcher. That's their "proof". Or another time they claimed that the representation issues in the field were due to conscious and unconscious bias and efforts to keep people out (or something like that) and they cited a Google study that just concluded that there are differences in the degrees to which school girls and black/hispanic kids are interested in AI (IIRC girls were less interested and black/hispanic kids were more interested but had less opportunities to pursue this interest early in life).

Anyway, they're liars. And it pains me to say that, because I actually do care about AI ethics. I do think AI systems are often biased in ways that hurt women and minorities, and I do wish that we could interest a more diverse group of people in careers in the field. I don't think spreading lies about how racist and sexist everybody is, is a good way to do that though. And I also don't think more racism and sexism in the opposite direction is the solution.

I'm not saying these people never do any good work on AI ethics. They are scientists as well as activists. The problem is when they're acting like activists while pretending to act like scientists. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-25 23:50:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> What do you mean when you talk about "AI of the future"? Do you consider current machine learning "supervised"? Even if we produce all labeled data ourselves, you might say we then "give" it to the AI system to learn from "unsupervised". I mean, it's not like we're checking every update, and we often cannot really explain how they arrive at their decisions. This is already considered a problem by many, so there is quite a bit of research into explainable/interpretable/etc. machine learning. However, it seems odd to characterize the way this works as "supervision". What are you concerned about exactly? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-24 22:14:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> Data point: at first I heard the same as vn4dw (i.e. niggas/neegas) after having been primed by the main post of course. After reading that it could also be "idiots" I listened again and I heard "idiots". And now it seems I can go back and forth, depending on what word I have in my head. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-24 17:35:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Is there a general rule of p = 1 / (2n +1), where n is the number of questions in the game?

Yes. You already know you either got all questions correct (event `A`), or all-but-one (event `B`). All answer configurations are equally likely, so `Pr(A) = #A / (#A + #B)`, where `#X` is the number of configurations that satisfy event `X`. There's only one way to get all answers correct, so `#A = 1`. For the wrong question, you have `2` options to select a wrong answer, and any of the `n` questions can be the wrong one, so `#B = 2n`.

Edit: this is for calculating Pr(all answers correct | all but one correct and last one unknown), ~~but I'm not sure it's the correct answer to the riddle.~~

Edit2: Pretty sure this is correct, because I verified it in another way (sorry I don't have time to explain it anymore)

Pr(Q2 | L2, rest) = Pr(L2, all)/Pr(L2, rest) ...... (L2 means Q2 is the last question)

Pr(L2, all) = Pr(L2|all)\*Pr(all) = 1/n \* 1/3^n

Pr(L2, rest) = Pr(L2|rest)\*Pr(rest) = Pr(L2 | rest) \* 1/3^(n-1)

Pr(L2 | rest) = Pr(L2 | rest, Q2)Pr(Q2) + Pr(L2 | rest, !Q2)Pr(!Q2) = 1/n \* 1/3 + 2/3 = (1+2n) / (3n) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-24 14:10:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I'd like to believe that we don't let our prejudices get in the way of equal treatment this often, but is it true?

We, meaning many humans and organizations, let our prejudices get in the way of equal treatment all the time. We're evolved to have in-group preferences, and our "biases" are often intimately tied to our experience, intuition and culture. Computers are often thought to be objective, and there's certainly a lot of potential there, but they are not completely free of bias either. Particularly, the data we use to train/create our AI systems is often biased in various ways.

There is not always a conflict between efficiency and equality/fairness: bias in a data set can easily harm both. Of course, if you already trained your AI system and it turns out to be unfair, collecting more data and retraining it will always have immediate costs, even if that also improves the accuracy.

Unfortunately this can be quite difficult in some cases. Data is often gathered in a certain way because it's easier or cheaper. For instance, if you want to do something with people in video, it may be easy to get lots of video for free on YouTube, but maybe most vloggers are white women (or whatever). Or if you *are* making your own dataset, but you live in Iceland, then it may be a lot harder to get black subjects. Or if you want to predict recidivism rates for criminals, you may want to use past arrest records to train on, but it's unclear to account for police bias in those arrests (or the data necessary for that may not be available). Or you want to predict good job candidates based on resume's, and your data is based on people who were hired in the past (which could be affected by human bias in hiring).

There are also cases where the data is unbiased in the sense that it reflects reality, but people don't like it or the outcomes of algorithms based on it. For instance, language models often find (roughly speaking) analogies like "man is to doctor as woman is to nurse" with strengths commensurate with the rate at which those professions are practiced by the different genders. Or an association between "black" and "bad" (as in "it was a black day") might also associate "African-American" with "bad" through the obvious connection of "black" with "African-American". (Or maybe it actually *is* the case that on average black people reoffend more or are less likely to pay back a loan.) In these cases it might be that making your system more "fair", you will have to sacrifice its accuracy.

And actually, you can just think of fairness as another thing for your AI system to optimize. There are over 20 definitions of fairness, and most (or all?) are not perfectly aligned with each other or the concept of accuracy, so optimizing one of them will typically mean that you have to sacrifice others. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-24 09:23:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> The most obviously practical things you could do in an AI project tend to involve somebody programming something. It doesn't necessarily have to be much. If you (or your school) is willing to buy LEGO MindStorms, little kids can program that. Or you could try to download some existing AI systems and tasks/environments and analyze the interaction between different algorithms / hyperparameter settings and outcomes in different tasks/environments (e.g. using OpenAI Gym).

Another thing that seems practical is to do something with depictions of AI. A lot of articles on AI have pictures of robots next to them (often a Terminator), even if the article is about AI software that has nothing to do with a robot. Or pictures of brains, even though the AI is nothing like a human brain. If you're so inclined, you could perhaps think of something better, or you could analyze some relationship between depictions and certain qualities of the articles they accompany (e.g. is the article positive/negative about AI, is the author well informed, etc.).

You could also look at (potential) impacts of AI, and how to steer them in the right direction.

I will also say that it's not impossible to learn some rudimentary programming skills in 4 months, and then program some kind of AI (maybe minimax for Tic-Tac-Toe or a simple neural network classifier using Tensorflow). There are lots of tutorials and courses online. And if you're really interested in AI and might like to pursue it after school, you're going to want to learn to program anyway. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-23 14:15:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> Jürgen Schmidhuber is basically the king of credit assignment in the field, so [his overview paper from 2014](http://people.idsia.ch/~juergen/deep-learning-overview.html) is great if you want to know who invented and published what and when. I would probably also take a look at LeCun, Bengio and Hinton's 2015 [Nature paper](https://creativecoding.soe.ucsc.edu/courses/cs523/slides/week3/DeepLearning_LeCun.pdf) and the Intro chapter of Goodfellow, Bengio & Courville's 2016 [DL book](https://www.deeplearningbook.org/). Of course, this will not get you all the way to 2019. Some important trends are probably Generative Adversarial Networks, Neural Style Transfer, Transformers, Metalearning, interactions with Reinforcement Learning and a growing demand for explainability/interpretability and reproducibility. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-23 10:31:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> No, this uses specialized eye tracking hardware (and software) on toddlers sitting still while watching known stimuli (pictures). Eye tracking is much harder with a regular camera / webcam, even if you're sitting still right in front of it. Movement of the subject and camera further complicate matters. Finally, you wouldn't know what the subjects are looking at and how that (and their movement + other events) affects their gaze.

It's much easier to detect age by just looking at the entire face, and child porn can be identified (with some degree of inaccuracy) using more straightforward image recognition techniques. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-20 17:45:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki that may be helpful. The article also talks a bit about narrow and general AI. It's probably a good idea to emphasize that 1) we don't have general AI yet and it will probably take *at least* a few decades before we do, and 2) advances in AI you see today are not one step away from Skynet. While I think it's good and proper for professionals to be concerned about existential risks posed by AGI in the future, and I think that we can and should already do useful work to mitigate those risks, there's no real reason to be afraid of any and all new advances in (narrow) AI that you may hear about. The media likes to overhype things. /r/ControlProblem is about those long-term AI safety issues, and has excellent resources on its sidebar and wiki. I especially recommend the WaitButWhy article. For dangers of current AI, you might want to take a look at the [ethics guidelines](https://ec.europa.eu/futurium/en/ai-alliance-consultation) recently published by the European Commission's High-Level Expert Group on AI.

For information about the current field of AI, there are some links on our wiki, including to free books and the "main" textbook (AI: A Modern Approach) of which I think the first chapter can be found for free. [Here](https://arxiv.org/abs/0706.3639) is also a collection of definitions of AI.

One thing I would personally stress is how much is vague, unknown and uncertain about AI. As you can see, there are over 70 different definitions. Nobody agrees on [when](https://aiimpacts.org/ai-timeline-surveys/) we'll have AGI. Nobody knows what will happen exactly. Closer in time: nobody agrees on what the effects of further automation will be on employment, we have over 20 [definitions of fairness](http://fairware.cs.umass.edu/papers/Verma.pdf) (see also [this video](https://www.youtube.com/watch?v=jIXIuYdnyyk)), and so on. I imagine you're not going to present any of that, but the message I want to get across, and something that you probably *can* convey, is that the moment someone says anything with great certainty about AI, you should probably take it with more than a few grains of salt (even if it's an expert). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-20 17:18:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> I agree. The other day I watched Hannah Fry's Royal Institution lecture titled [Should Computers Run the World?](https://www.youtube.com/watch?v=Rzhpf1Ai7Z4). I *think* it was posted here, because I remember feeling essentially the same way that you do about this new article. The title sounds like the content could be relevant, but in reality it's about issues with current-day narrow AI, and would probably be more at home in /r/AIethics. I don't see the video here anymore, so I guess that means I was mistaken or someone (a mod?) removed it.

On the other hand, and I don't mean this as a criticism of you or your post, I don't necessarily want to discourage people from posting content here, especially given the mods' [call for submissions](https://www.reddit.com/r/ControlProblem/comments/b3vjq0/meta_to_visitors_please_post_more/). While the article (and Fry's lecture) does not itself address the superintelligence control problem, I think it would be possible for /r/ControlProblem's community to view it from that perspective and attempt to connect the dots ourselves.

I could see a few different ways of doing that here. The idea of the article seems to be that even if we could fix what's broken about an AI system (i.e. bias / uncontrollable), that doesn't mean that AI should exist / won't have bad effects / won't be misused. So perhaps we could discuss whether we want AGI/ASI to exist, *even if the control problem is solved*. Or perhaps it's possible to draw a parallel between the value of fairness and value-alignment in general. Or maybe the "some AI" from the title that shouldn't exist could be interpreted as "unsafe AGI", and we could discuss what could/should be done to prevent its creation (perhaps in some way drawing parallels to how we might stop current-day AI that's unfair despite being unbiased). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-20 15:07:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> The very next sentence after the part you quoted is:

> Certainly, if there is no underlying crime, you shouldn't get in trouble for lying to the feds, even though it's technically illegal.

I do not at all agree that this is "certainly" the case. I could perhaps be persuaded that *in some cases* lying to the feds might be okay (or at least that you shouldn't get prosecuted for it), but certainly not as a general statement. Some of those cases may involve the feds deliberately goading an innocent suspect into lying about something so they can prosecute them for lying instead of or in addition to the crime they were actually suspected of.  But I don't think that's always the case, and as a rule of thumb I agree that you just shouldn't lie to the feds.

Furthermore, it seems that "lying" and "obstructing justice" are equivocated here. Certainly lies can obstruct justice, but this is not the only way. Threatening officials' jobs may be another, just to name one. I understand it can be frustrating to see resources being spent on an investigation you know didn't occur, but this happens on different levels too (e.g. a police officer being suspected of a crime) and the correct line of action is to withdraw from the process as much as possible (e.g. in the aforementioned case, neither that officer nor his department will be the ones investigating that suspected crime), while cooperating fully so the investigators can uncover the truth as soon as possible. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-20 14:17:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> Please don't post basically the same question twice. If you feel like these are multiple questions, just elaborate in the text part of your post (which you've left empty here).

/u/ubersapiens is correct: we don't fully understand these networks, but hierarchy appears to be very important. Tomaso Poggio actually seems to have done quite a bit of work on this [[1](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14849/14414), [2](https://arxiv.org/abs/1608.03287), [3](https://arxiv.org/abs/1603.00988)], but there's more to find on the theory of deep learning as it relates to e.g. generalization [[4](https://arxiv.org/abs/1710.05468), [5](https://arxiv.org/abs/1611.03530)]. I think it's also fairly well-known that while a large enough single hidden layer let's a neural network represent any function, it's possible to reduce the number of needed parameters with deeper architectures.

But in terms of impact, I think there are mainly two properties that make deep learning "special". The first is a fairly strong ability to do its own feature extraction / representation learning. Before deep learning, a large part of building ML systems consisted of feature engineering, which required some form of domain expertise. Since deep networks can learn from raw pixels, sounds, etc. this is no longer needed. While getting a deep net to actually work is usually nontrivial, the shift from needing ML *and* domain expertise to just needing (a bit more) ML expertise opens up many doors.

The second "special" property is that deep learning can make great use of huge amounts of data (and computation). There are pretty much always diminishing returns for an ML algorithm when adding more data (or training on that data for more iterations), but with deep learning this only tends to happen if you have *a lot* of data (and even then, you could probably make use of it by making the network larger).

So more data + more computation + ability to use those without (much) domain expertise have enabled great advances.

---

In terms of what the technical difference between deep neural networks and non-deep NNs is: it's depth. But additionally, there are a lot of differences between current (deep) neural networks and earlier neural networks. In the old days, we used step functions for activations, which then mostly moved to sigmoids, while now we tend to use ReLUs. There was a time before the backpropagation algorithm was invented, and before people saw the need for hidden layers. It seems that currently the feedforward network is most popular, with a certain kind of recurrent network following. But there are many more types, and especially in the old days they were used a lot too (e.g. interactive activation, self-organizing maps, adaptive resonance theory, reservoir computing, Boltzmann machines, spiking NNs, etc.). Somewhere along the line people invented convolutional layers, pooling layers, dropout, sparse coding, regularization, unsupervised pretraining, etc. So when looking at the differences between current (deep) NNs and earlier ones, there tends to be a lot more than depth. (Also, it's not like deep neural networks weren't tried much earlier. They just didn't work very well back then, because of a lack of data, compute, and problems like vanishing gradients.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-19 14:05:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> $150k is probably the average (***heavily*** dependent on location), and $500k are the extremely rare exceptions (others make less than $150k, so it averages out to that). BTW, I would be very surprised if you got anywhere close $150k unless you're working in Silicon Valley or New York or a similarly expensive place. (Although I do think AI/ML PhDs typically make way more money than fresh-out-of-school programmers.)

Most universities won't let you *literally* get a degree in AI/ML, so your diploma will likely say "Computer Science", or maybe mathematics, statistics, data science, cognitive science, neuroscience, etc. However, while the people you're talking about typically weren't enrolled in a program called "AI/ML", they did in fact do their PhD on AI/ML. So what you need to do if you want to follow in their footsteps, is find a PhD position where you can do that too. Since most AI labs are part of Computer Science departments, I think it's most likely that you'd enroll in a CS program, but the other programs I mentioned are possible too ***if*** professors in those programs are doing AI/ML research. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-19 13:02:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> The idea of hillclimbing is to have some starting point (usually a random configuration that *could* be an answer to the problem) and create variations in some way. In this case, a variation could be created by adding, removing or replacing a course, and you might get different results depending on which operations you allow (e.g. add+remove, replace, or add+remove+replace).

With "standard" hillclimbing, whenever a variation constitutes an improvement, you make it your new starting point. You're finished when every possible variation from the current solution is not an improvement, because that means you're in an optimum (either local or global). There are variations where you always try all variations from your current solution and pick the best one to continue with, rather than the first one that's an improvement. This is similar to best-first search, and can still lead to non-global optima. One strategy for coping with this is random restarts: basically you run this procedure a number of times from different random starting positions, increasing the likelihood that you'll find a global optimum.

It sounds to me like this is an issue of (nonconvex) [combinatorial optimization](https://en.wikipedia.org/wiki/Combinatorial_optimization), which is hard in general. You'd like to find a [greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm) or [metaheuristic](https://en.wikipedia.org/wiki/Metaheuristic) to avoid having to brute-force check all possible solutions, but this is not always possible and depends heavily on the details of the problem. I hope looking around those Wikipedia pages will give you some pointers about possible methods for global optima you could try. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-18 17:36:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> I like Spurs much better than City, but I have to say I'm scared. City feels similar to Real and Juve: a big money juggernaut who were favorites to win the whole thing. Ajax cherished that underdog role, played freely and in some sense with nothing to lose, against teams under tremendous pressure to win. But it's no longer just Ajax's fairy tale: we are now also in Tottenham's. I feel that now we *have to* win, especially because Tottenham isn't such a juggernaut and they are missing Kane and Son. If the players feel the same way, I'm anxious to see how they'll deal with it.

But we'll see. It will be nice to see all those ex-Ajax players again. And who knows, maybe we can entice some of them to come back. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-18 15:30:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> What you're talking about is called "reward hacking/hijacking/corruption" in the literature and is related to the "wireheading" problem. You can see a short overview of the issue in Section 4 of the [Concrete Problems in AI Safety](https://arxiv.org/abs/1606.06565) paper, Everitt & Hutter's [paper on wireheading](https://arxiv.org/abs/1605.03143) or Tom Everitt's entire [PhD thesis](https://www.tomeveritt.se/papers/2018-thesis.pdf).

Essentially I think your analysis is correct: reward hacking / wireheading would be rational if the agent is trying to optimize expected future reward based on the expected future *reward mechanism*, but it would not be rational if it uses the current reward mechanism. This is related to what Victor is saying: namely that you'd want to have the AI care about the actual goal, and not the function that measures goal completion. Unfortunately it's not clear how to program this distinction. And perhaps you would actually *want* the reward function to be able to change in certain ways: e.g. if the goal is to make humans happy, you might want the AI to use what it has learned to get a better idea of what "human" and "happy" mean.

Also, dumb reinforcement learning agents (i.e. not superintelligent) may do this "accidentally". If you look at many current reinforcement learning agents: they don't really have a good idea of what their reward is. They typically just learn a state->action mapping. So if you put an item in the world that e.g. doubles all reward calculations, they should probably learn to seek that out, even though no deliberations are happening like the ones you mentioned. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-18 11:30:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> > How would you suggest making the problem more specific?

Describe the input and goal in full detail. For instance:

**Input:** a list of N courses C, each of which has some value C.V, and a list of time slots C.T, each of which has a location C.T.L, a starting and a finishing time C.T.S and C.T.F. Also, a location for the office O and home H, and a distance function that calculates the distance between two locations D.

**Goal:** Choose a set of M courses so that the sum of their values minus X times the total travel time is maximized. M needs to be between 3 and 5, the time slots of two courses cannot overlap, and the travel time between two courses must not be larger than the available time in between them. Total travel time is calculated as follows: ...

I don't know if this is correct, but it's the level of detail that's needed to approach the problem.

> I also see a problem where we have I global optimum to look to since we need to.

In some cases it's possible to design a greedy (hill climbing) algorithm that gets to the global optimum. Whether that's possible depends on the details of the problem.

> This problem could be solved by generating all possible schedules, calculating the commute time (based on some predefined values) and just getting the minimum. Is there a way to use some sort of search algorithm for this same problem ?

Brute force calculating all possibilities *is* a search algorithm. If it's feasible in this case, maybe that's the way to go. However, it would probably not be feasible if the number of courses is very large. If that can never really happen, then maybe brute force is the best solution (it's certainly very cost-effective in terms of development time). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-18 09:31:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your problem description sounds very vague to me. Especially the input and the goal.

I think hill climbing could work to some degree, but there's probably a good chance of getting stuck in a local optimum. </TEXT>
</WRITING>
<WRITING>
<TITLE> Why the world’s leading AI charity decided to take billions from investors: A Q&A with the founders of the cutting-edge AI lab OpenAI by Kelsey Piper </TITLE>
<DATE> 2019-04-17 20:05:40 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Why the world’s leading AI charity decided to take billions from investors: A Q&A with the founders of the cutting-edge AI lab OpenAI </TITLE>
<DATE> 2019-04-17 20:04:35 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Why the world’s leading AI charity decided to take billions from investors: A Q&A with the founders of the cutting-edge AI lab OpenAI </TITLE>
<DATE> 2019-04-17 20:02:22 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-17 19:11:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> Ah okay. No hard feelings I hope! It just annoys me a little when commentators are always saying it's so unforgivable that players aren't always 5000% motivated and playing their very best each match. I guess I finally thought I saw it on Reddit now, so I had to say my piece. But hey, you just wanted to make a fun meme, so good on you! I should probably stop turning into a grumpy old man. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-17 18:30:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you're a Heracles fan who's happy about your team's results against Ajax this year: congratulations. But you could also make this meme with a Heracles player, Ajax and e.g. PEC Zwolle. People say it's unprofessional to be less motivated in some matches, but that's exactly what all professionals do. Before losing to Ajax, Juve lost to SPAL. Barcelona tied against Huesca. There are tons of examples.

For Ajax playing against Heracles, Heerenveen, etc. is just another match, but for them it's always the match of the year. They are motivated like Ajax was against Juve and Real. They, in turn, lose points to other clubs who are worse than them, like PEC. You can't win them all, but luckily Ajax can still win it all. Go triple! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-17 18:10:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> This sounds like good advice. I'm curious how you (and other experienced professors) would answer the question of what the student should have done to prevent this. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-17 14:52:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> You're right. Thanks! I just saw this on [nu.nl](https://www.reddit.com/r/artificial/comments/bdgwcw/google_has_opened_its_first_africa_artificial/):

> "Tien minuten voor tijd was ik op, al zei ik dat niet. Ik ben wel een paar keer naar de kant gelopen om een slok water te nemen en toen zagen de trainers het ook. De kramp schoot er een paar keer in, maar dat was het allemaal waard."

Translation:

> "Ten minutes before the end I was done, although I didn't say it. I did walk to the side a few times to take a sip of water, and then the trainers saw it too. I was cramping up a few times, but it was all worth it." </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-17 14:26:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> Congratulations on making this! It sounds great. However, could you say a bit more about it to make it a bit more interesting to the users of /r/artificial? Perhaps some more details / papers on how this is made / achieved and how it could be reproduced, or an analysis of what we're looking at and how it relates to the chosen approach. Thanks! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-17 14:23:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> Already posted [here](https://www.reddit.com/r/artificial/comments/bdgwcw/google_has_opened_its_first_africa_artificial/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-17 12:03:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I also cried tears of joy yesterday, but I was wondering about the logic of substituting Magallán for Sinkgraven. Can anyone explain the idea to me? I thought Magallán was generally regarded as a bit "unsure" (to put it nicely), and Sinkgraven and our center duo were playing well. Why would you risk messing with that, given that Juventus barely got any chances at that point in the match?

(I don't mean this as a criticism of Ten Hag or anything. Great respect for him and the whole club! I just like to analyze things.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-17 10:57:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I understand that you're curious to discuss this hypothesis with AI enthusiasts, but on the other hand you're basically attacking a user who has never posted here and sending other Redditors over to potentially pile on. If you want to discuss certain posts, I would suggest that you just copy-paste (perhaps with some context) them without mentioning the user.

FWIW I don't think /u/westom is a bot or a translation. The account is 5 years old. More likely it's just someone who vastly overestimates their expertise in certain areas and enjoys posting about that. It's actually not that rare. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-16 15:02:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> AI already is better than humans at writing in some aspects and contexts (e.g. it's faster and costs less). See [the Rise of the Robot Reporter](https://www.nytimes.com/2019/02/05/business/media/artificial-intelligence-journalism-robots.html). Aside from the lack of typos, I don't think the quality and depth is there yet though.
[OpenAI's GPT-2](https://openai.com/blog/better-language-models/) seems pretty good, but not as good as a competent human. In the future, AI will probably get better, and AI might eventually outperform human writers in every way. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-16 14:56:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> Assuming this AGI magically disappears after my one question, I would most like to ask how to create safe AGI in a way that I can understand before turning it on. I guess in some way, this is a variant of the "ask for more wishes" wish. This seems like probably the most useful kind of question that could be asked.

However, the mere promise of "AGI" doesn't mean the system would be able to answer this, or many other questions. AGI is often taken as synonymous with human-level intelligent, and I can talk to human-level intellects all the time. And even if it is millions of times smarter, it may still not know anything about the afterlife or the (outside) of the simulation we (and the AGI) would exist in. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-16 12:06:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> I thought this was quite interesting. I'm not sure people commonly fall into all of these traps, but I found it useful to keep them in mind when thinking about AI ethics. The seven traps are:

1. **The *reductionism* trap**: reducing "ethical" to a single value like "fair"
* **The *simplicity* trap**: oversimplifying the issue with checklists implies a one-off process for safeguarding ethics
* **The *relativism* trap**: everybody disagrees, nothing is objectively moral, so let's not bother
* **The *value alignment* trap**: there's one single morally right answer
* **The *dichotomy* trap**: we shouldn't draw simple dichotomies between being ethical or unethical; also ethics is better construed as something to *think* about or *do* and not something to *be* (or not be)
* **The *myopia* trap**: ethical trade-offs translate/generalize across contexts
* **The *rule of law* trap**: ethics and law are basically the same thing

---

I think I agree that most of these are pitfalls to avoid. Some of these could be worded better: I thought the "dichotomy trap" would be mostly about the binary nature of ethical vs. unethical, which should be more of a continuum, but it was actually more about the fact that we should not say an entity *is* (un)ethical, but that ethics is a process of thought/action. The "myopia trap" could probably better be called the "generalization trap" and maybe "value alignment" should be "objectivism".

The main thing I don't agree with is the criticism of checklists as part of the "simplicity trap", especially when appropriate caveats for its use are carefully pointed out. The authors claim that a checklist implies a one-off review process, and I don't see how that's true at all. You could apply the checklist continually at multiple points in time. Furthermore, while I think *over*simplification should indeed be avoided (naturally), the value of creating simple and practical guidelines that people/companies can actually follow should not be underestimated. Actually, this may be exactly what is needed if you want your lofty ethics to go from "nice theoretical discussion" to "actually applied in practice". </TEXT>
</WRITING>
<WRITING>
<TITLE> AI Ethics: Seven Traps </TITLE>
<DATE> 2019-04-16 11:41:13 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-15 13:36:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for the addition! There's some interesting information there, as well as the earlier paper.

The dataset is not really available *on* the website though. The "Dataset" link just lets you email datasets@ajlunited.org and presumably request it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-15 13:29:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm one of the new subscribers, although I've been a lifelong Ajax fan. As far as I can tell, this sub is still quite low traffic, with only a few new top-level posts per day. It's easy to avoid anything I'm not interested in, and I don't see a need to change anything at this point. Of course, I don't know how it was e.g. last year and if the regulars then liked it much better.

I really like the memes I've seen here, especially after the Real Madrid match. Maybe I haven't been around long enough to see the really bad ones and whatever "botergeil" refers to.

Pretty much my only problem with the recent "PSV TIED UPVOTE PARTY" thread is that it had "UPVOTE PARTY" in the title (although I don't *really* care). Generally speaking, I would like to see discussions about matches/clubs/players that might be relevant to Ajax from the perspective of Ajax fans. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-15 11:06:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. I think Bostrom's Superintelligence is great, but I don't think it's a good book for getting into the field. It's mainly about the impacts, risks and safety of artificial general intelligence (see /r/ControlProblem), but does not tell you much about how to actually make AI/ML. The standard text for that is Russell & Norvig's AI: A Modern Approach, but there are more options listed on the wiki. These are all books and courses though, and I don't really know any articles that give a great overview. The AI field is probably just too large for that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-15 11:01:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> I always hate it when news media discuss other (written) works without linking to them, so let me try to correct that a bit:

* [Raji & Buolamwini's study that criticized Amazon](http://www.aies-conference.com/wp-content/uploads/2019/01/AIES-19_paper_223.pdf)
* [Amazon's response](https://aws.amazon.com/blogs/machine-learning/thoughts-on-recent-research-paper-and-associated-article-on-amazon-rekognition/)
* [Group of "concerned researchers" supporting Buolamwini](https://medium.com/@bu64dcjrytwitb8/on-recent-research-auditing-commercial-facial-analysis-technology-19148bda1832) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-14 20:14:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://arxiv.org/abs/1707.08945

https://cvdazzle.com </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-14 13:55:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't have a lot of time right now, but the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) in our wiki might be a useful starting point for you. I would also google something like "machine learning task/todo extraction from text", because I wouldn't be super surprised if someone already did something like that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-14 13:52:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think there are some real world examples of people modifying traffic signs to fool self-driving cars in the real world, and make-up / outfits to fool face recognition (although in that case you'd make yourself stand out a lot to any human). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-11 01:13:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> He's a very important player for Ajax, but he grabbed a dumb yellow card and almost caused a penalty kick for Juve (and probably his second yellow in that case). Aside from those silly mistakes, I thought he played well, and he'll be sorely missed in the away match in Turin. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-10 09:52:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not sure how to concretely answer a "how hard" question like this, because I can hardly say "this hard" or "43.7". I'm inclined to tell you to just go for it. Maybe /r/cscareerquestions has better answers.

Your degrees seem relevant enough, and having honours degrees is of course nice. It would have been nice if they'd be from internationally recognized top notch universities, but it's not a strict requirement. It would also help to have peer reviewed publications in good conferences/journals, have solid academic letters of recommendation and to be female, black or Hispanic. That's not to say you *need* any of those, but they could be helpful.

BTW, depending on the university, your industry experience may not be viewed very positively. I got advice for a recent tenure track application to downplay my own industry experience, because it might signal that I really want to be in industry and this whole "research" thing is just temporary (I think that's silly, since getting a much better playing industry job would be much easier, but whatever). Despite the increasing prominence of private industry in AI research, I strongly suspect universities would prefer to accept PhD candidates who intend to stay in academia.

I definitely don't want to discourage you, but I recommend looking up the acceptance rates of the programs you intend to apply to. Most tend to be very low, so don't be surprised if you don't get in, and apply to many different programs. Also look how it's done in different countries. For instance, I applied for PhD positions in the US and Scotland (among others), and in the US you're really applying to some program while in Scotland I co-wrote a grant application with a professor I had contacted. That grant application unfortunately fell through, and I didn't get accepted at MIT, Stanford and CMU either. Looking back, I understand why (the grant application was too ambitious in a time where AI wasn't that popular, and the US apps were kind of sloppy, had the wrong motivation and I botched the GRE Subject Test).

So I'd say: try your hardest. Make your applications rock solid. Look up advice online (see also /r/gradadmissions) and ideally get a real-life academic advisor to look over your applications. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-10 04:04:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> Maybe some of Richard Loosemore's work, like [The Maverick Nanny with a Dopamine Drip](http://ieet.org/index.php/IEET/more/loosemore20140724) (2014) or [The Fallacy of Dumb Superintelligence](http://ieet.org/index.php/IEET/more/loosemore20121128) (2012). It seems to roughly make the argument you mention, and mentions brittleness and swarms. Rob Bensinger talks about it [here](https://nothingismere.com/2014/08/25/loosemore-on-ai-safety-and-attractors/) (and in some older articles he links there). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-10 02:07:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> Sounds like a silly argument. I don't know the article. Do you remember about the context in which you encountered the link, or perhaps the content / layout of the page? Was it being discussed here in the CW thread, on TheMotte, r/SSC or SSC? etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-10 01:31:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> I recommend checking out the side bar and wiki of /r/ControlProblem. They have a lot of information about this. The WaitButWhy article is probably a good starting point. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-09 13:42:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm sorry, but I don't know what kind of classifier a lexiogram is and I can't find it on Google. Do you have a link?

In any case, if you have a classifier (anything that maps input, in your case the bag of words, to a label), you should then use it to compute labels for your test data and compare those to the real labels. For instance, you can just calculate what percentage of them are correct.

Does that help? If not, I must say I'm a little confused about what you're getting stuck on. Is there a way that you could show what you've done and explain in more detail where you're getting stuck? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-09 11:04:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm afraid you may have made things quite difficult for yourself by talking about *predictions* of AI. This is necessarily speculative, and open to debate, and evidence is always going to be difficult to assess.

Researchers in the AI safety domain have spent quite a bit of time coming up with and investigating various plausible scenarios that involve out-of-control artificial general intelligence (AGI) or artificial superintelligence (ASI). You can find them through /r/ControlProblem's side bar and wiki, and I recommend starting with WaitButWhy's articles and then maybe Bostrom's book on Superintelligence. Tegmark's Life 3.0 also has a couple of different scenarios. The problem is of course that while strong arguments can be made for concepts like orthogonality and instrumental convergence, and plausible arguments for an intelligence explosion, *details* of the exact process of what might happen are necessarily speculative. This is true for any prediction of the future, but perhaps even more so with a transformative technology like A(G)I.

If possible, I might consider switching topics to describing a process that already exists, like how image classifiers are created (roughly speaking: gather data, annotate, divide into train-validation-test sets, enter loop of model selection / hyperparameter tuning -> training -> evaluation, until you're done). Or perhaps how various algorithms like neural networks / backpropagation, genetic algorithms or Monte Carlo tree search work. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-09 10:39:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> I haven't looked at it in depth, but Andrew Ng has a new course called [AI for Everyone](https://www.coursera.org/learn/ai-for-everyone), which sounds like it might be, well... for everyone. I'm not sure if you can get their materials, but you could perhaps look at [AI4ALL's education programs](http://ai-4-all.org/education/). There's also the [AI Family Challenge](https://www.curiositymachine.org/aichallenge/), and there might be some useful info for you here in [this article](https://www.gettingsmart.com/2019/01/why-every-high-school-should-require-an-ai-course/) on how every HS should require an AI course. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-09 10:22:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki. I'm not sure what the best online course is, but I do think Andrew Ng's Machine Learning course is pretty good.

If you really want to do groundbreaking work in the area though, I think you're going to have to get an education. And while it's possible to learn a lot by yourself, it's much more likely you'll succeed if you go to college for this. Otherwise you'll just have to be extremely proactive. If there's math you encounter but don't know: look it up. Go to Wikipedia and Khan Academy. Take online courses on math and mathematical thinking. Figure it out. This is very difficult, which is why I recommend going to college. You should be able to get a decent job afterwards, so the cost is probably worth it.

Asking here is fine too, but unfortunately I don't know all of the available online courses. Ng's ML course is very introductory, so I imagine you should take it before his course on deep learning. The same might be true for fast.ai's, but I'm not sure. Anyway, good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-09 10:10:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> > You could also opt to become a PhD academic and somehow rack up over 100,000+ citations in AI and then they might hire you as well.

This makes it sound like it's hard for an AI PhD to get hired by tech giants, but in reality that happens all the time. I'm not saying they'll hire literally anyone with a relevant degree, but you really don't have to be all that exceptional (you can have well under 100 citations).

In fact, if you want a *research* role at a tech giant, this seems like the much more straightforward route. If you're just good at programming, they will probably want to hire you as a software engineer (SE). At least that's what happened to me when I was headhunted for one of these tech giants. I told the recruiter I wanted to work in the research department, but he told me they didn't hire directly into that, and that while it wasn't impossible to get there after having been hired as a SE, he warned me that Research was full of PhDs. I went on to do a PhD instead. (I'll note that recruiters are somewhat external to the organization, so he might not know everything *or* he might not have told me the full truth [e.g. if he only has a mandate to hire SEs and not researchers]. Also, things may have changed in the last few years, and I do think Research employs more SEs now, but I still think this mostly applies if you want to do research yourself.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-08 17:41:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> Bag of words is just a way of representing your data. It's almost orthogonal to what classification algorithm you use. There are many different ways to do text classification. Here's a [tutorial](https://medium.freecodecamp.org/text-classification-and-prediction-using-bag-of-words-8aeb1396cded) on one form of text classification using bag of words.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-07 11:33:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Throughout the book, she stresses that a major problem with AI is that it is being developed by white men. That means that many of the unquestioned assumptions that white men hold are given uncritical acceptance and get encoded in the AI systems that white men create.

This is unfortunately a pretty common sentiment among non-"White Men". I guess seeing some AI systems discriminate against--or making judgments that disfavor--black people or women, and noticing that the majority of people working in the field are "white" men, does seem like it might be two sides of the same coin, especially if you already feel like white men are your enemy. Of course, many of these people will also be quick to point out that the biases shown by AI systems reflect those in society at large, but they don't follow that to the logical conclusion that the "white men" factoid is not really needed as an explanation, and as far as I know there's also no evidence of it. Since bias typically comes from the data, I would say that it's valuable to make sure that the people who label that data are "diverse" in ways that matters.

This may be related to the belief that the demographics themselves indicate bias or discrimination. As far as I can tell this is pretty much completely false. Companies are hiring women at a rate equal to or higher than the proportion that graduates college with relevant degrees like CS, and white people are actually underrepresented compared to society. This is mainly due to the fact that there are a lot of Asians in tech (which are counted as white for some reason). It's still true that black and Hispanic people are underrepresented, but as far as I can tell this is true in any highly educated profession. I don't want to exclude the possibility that something wonky is going on with the hiring--there's certainly a lot of "positive" discrimination happening, but some of this may be offset by not wanting to hire minorities/women or SJWs.

Of course, even if the white+Asian men in AI aren't causing the discriminatory behavior of some AI, it's possible that they aren't noticing it. I think it's very possible for these things to go unnoticed, but I would warn against considering this a task for "diverse" people. If you hire a black trans lesbian as a developer, her job should just be to a developer: the same as her (mostly) white and Asian male colleagues. You shouldn't be giving her extra tasks like "catch the bias in our systems", that increase her workload. And even if you remove other developer tasks to equalize the workload, you shouldn't just assume that she wants or is even good at this task. If you need someone to catch the bias in your systems, hire someone to do that. And then it shouldn't matter if their white, black or purple, as long as they're good at that job (although I *would* expect the demographics of the applicant pool for that job to be a bit different). The only thing to look out for is that you don't hire someone who will complain no matter what in order to increase the perceived importance of their own job.

Aside from outright discrimination, it is of course possible that a homogeneous workforce would make assumptions that go unquestioned and that this affects their products and services in some ways. I could see this having some effects, e.g. because some companies may be more inclined to make products that the workforce would use themselves, but for the most part some halfway decent market research should be able to remedy the excesses. For instance, I'm pretty sure the reason most voice assistants have female voices is because that's what the public at large wants, and not because some low-level male developer is getting off on it. I've also heard some people suggest that the people who worry about risks from AGI are just doing so because as men they are always thinking of fighting or something like that, and those sexist arguments were enough for them not to engage with any real arguments.

To make a long story short, I do think that skewed demographics in companies and the industry at large can have some negative effects, but they are mostly overblown or at least wrongly assessed. I'm sure it can be difficult being the only black woman in a department, but this isn't really anyone's fault, nor does it mean that department will likely discriminate against black people / women. The effect of racial and gender diversity should be assessed per job/role, and I don't think it will matter much for most (except e.g. data labeler). Viewpoint diversity is probably more important, but has become a dirty word. Most issues that *do* arise from the underrepresentation of some groups in AI need to be solved by those groups stepping up and actually getting jobs in AI: nothing is stopping them, except interest (and in the cases of poverty-stricken minorities: poverty). Generating that interest might be important, and the people who whine about these issues are typically doing the worst job of it.

---

Addressing the Damore thing:

I haven't read anything by Amy Webb, including this book. Your excerpt about Damore shows me she's either a blatant liar or entirely incompetent at figuring out the truth about a matter. I think this should make people extremely distrustful of anything she writes. One thing that could perhaps be done, is to verify some other factual statements she makes in the book to see if this is an incident. Focus on "facts" that directly align with her biases and the main point she's apparently trying to make (she probably won't lie about basic facts like 2+2=4, but she might lie about or fail to investigate other claims against white men).

Otherwise, just treat it as a propaganda piece. You can still learn some things from it, mainly about how propaganda is made, or how biased people reason. It might help you find interesting facts, you just need to double check them. Remember that among huge swaths of the population, it is extremely popular to shit on the white male elite who are controlling everybody's lives. Oversimplifying narratives that blame a hated outgroup for anything and absolve the reader of any responsibility are always going to be popular.

I think it's commendable that you don't just want to tear this book to shreds, although it doesn't seem that Webb is as charitable to the people she writes about. If she's really full of shit, a good refutation may be the most valuable thing you can produce though (especially if you can be honest and charitable about it). Like I said: I haven't read the book, so there may be some good things about it that didn't come up here, but based on what I did see I'd have a hard time trusting that they're true.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-05 14:44:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> If you're not in the know, this insult may also seem a bit worse than it is. Kakkerlakken/Cockroaches is specifically the derogatory term used for Feyenoord (Ajax's archenemy). So a semantic translation might read "Scherpen fucking Feyenoorder", and it's because he is (or was) a Feyenoord fan. He mentioned last year that his dream was to become their keeper, and 8 years ago he tweeted some derogatory things about their archenemy (Ajax) as supporters are wont to do. (Also, insults of Jews may sound worse than they really are. Ajax calls themselves joden/jews, so derogatory comments about Jews in this context are just anti-Ajax and not evidence of antisemitism.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-05 13:03:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> > AI can never achieve human intelligence

I don't really believe that. "Never" is a long time. I think human intelligence runs on physics that can be emulated. Essentially, we are "machines" too, so there is an existence proof. However, if it turns out that communication between neurons is not the "currency" through which our brains are intelligent, but e.g. quantum effects or something we haven't discovered yet are relevant, things may be a lot harder.

One argument in favor of this prediction is that I don't think dogs, chimps or dolphins will ever be able to build dog/chimp/dolphin-level AI. It's just far too complicated for their abilities. Could it be that humans are similarly just not intelligent enough to build AGI? It seems to me that we have the abstract reasoning and collaboration skills that are required, but who knows.

> AI will achieve human intelligence and do all our dirty work

Hopefully, although I guess it would do more than just our *dirty* work. If safe AGI is developed by and mostly in the hands of beneficent organizations and people, this could have massively positive effects on he world. I do think a massive increase in inequality is almost unavoidable, but that may not matter if it is accompanied by a massive increase in well-being for everyone.

> AI will take over the world and makes us obsolete

This is unfortunately probably the most likely scenario. Developing AGI that has (super)human-level intelligence is almost certainly easier than developing AGI with (super)human-level intelligence *that is also safe / under control / value-aligned*, and even if that happens, there is a question of what the AGI's owners might do with it. A lot of excellent work has gone into this (see /r/ControlProblem), but unfortunately it is absolutely dwarfed by the massive efforts spent on so-called capability research, and many still foolishly deny the potentially catastrophic dangers of AGI.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-05 11:29:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> You are pretty much correct. One of the [first paper](https://arxiv.org/abs/1112.6209)s I remember from the beginning of the Deep Learning renaissance was about unsupervised learning on YouTube videos and learning to identify faces and cats. Deep neural networks actually do appear to construct [feature hierarchies](https://www.google.com/search?q=neural+network+feature+hierarchy&tbm=isch) where lower-level layers might correspond to differently oriented lines, then higher level layers might be combinations of those, then you might get e.g. face/image parts, until at the higher layers you have neurons representing the classes you're looking for. (It's a bit more complicated than this, but this is the general gist.)

And yes, "(un)supervised pretraining" has been used in a variety of ways to either speed up training, improve generalization or otherwise help learning. However, I've also heard that it has kind of fallen out of favor. This might be that in a lot of research (e.g. with ImageNet or CIFAR) there actually *is* a lot of labeled data, so the advantages of adding unlabeled data are not as great. It might also be that unsupervised learning doesn't produce the optimal configurations for any particular supervised classification scheme, and that "fine-tuning" an already-trained network with the few supervised examples you have is not actually easier than training a randomly initialized network. This is a bit technical, but I imagine this may especially be the case when using sigmoid activation functions, because (pre)trained networks have the tendency to saturate neuron activations, which makes the derivative low and learning with regular backpropagation slow.

You may also be interested in the somewhat related setting of semisupervised learning (SSL) where ML systems train on a combination of labeled and unlabeled data. However, with SSL it tends to be the case that the labeled and unlabeled data both comes from the same distribution (e.g. for a facial expression classifier all inputs would be faces, but only some would be annotated with "happy" or "sad"). Pretraining typically doesn't require this (e.g. you could pretrain that facial expression classifier with any set of pictures), although it doesn't hurt to have the pretraining set be at least somewhat similar (e.g. use pictures taken in the real world rather than schematic figures).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-05 10:55:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> It looks like this is a graded assignment on an online course. I'm not going to help you (or others) cheat on homework. You can post this again after the due date with your own solutions to start off the discussion.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-05 00:56:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, good luck with your EPQ! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-05 00:49:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your link doesn't seem to work. I think it's an invitation to edit a form, but Microsoft also tells me I don't have the rights to do that (which is probably good). Please let me know when you fixed it, so I can approve your post.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-04 16:43:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> To a first approximation, my guess would be that I probably disagree with pretty much every one of Kay Coles James's standpoints, but I still think it is good and commendable for the very progressive Google to put a conservative person like her on their ethics board. After all, 50% of Americans is conservative. I'm a lot less sure about the inclusion of Dyan Gibbens. I think it's good to have a diversity of viewpoints on the ethics board, but I'm not sure what makes the viewpoint of a drone company specifically valuable. I'm not saying they should only include Campaigners Against Killer Robots, but it would make more sense to me to include someone from e.g. the army to represent the opposite viewpoint.

I don't really think it's a bad thing that this board doesn't have that much power, and it makes sense to me that its role is purely advisory. I think companies should make their own ethical decisions, and be held responsible for them. Would it really help if some external board they appointed themselves made their decisions for them? I don't accept the "ethical authority" of anyone on any board Google (or another authority might appoint). The most they can do is point to situations where ethical dilemmas might arise, and provide guidance on how potentially bad outcomes could be mitigated. That's also why I think it's good to have diversity: it means they'll probably be notified of more issues. This also means that if James tells them "your algorithm doesn't discriminate against transsexuals enough", Google can just respond with "Great!". (And even aside from this, I think it would be good to not disqualify people if they have a few characteristics you dislike: maybe James is a good advocate for free enterprise or whatever, and someone else can be a good advocate on LGBT+ rights.)

However, I do have to say that I'm not quite sure what the value of this board is in terms of actually making Google more ethical. I understand it has the potential to make them *look* more ethical and legitimate, but that doesn't seem like a good thing to me.  I guess having 8 successful, influential people come together and advice you 4x per year is not nothing, but I also wonder whether it wouldn't be more beneficial to employ some full-time people to pour over Bryson, Floridi et al's work, and keep an eye out for what public figures and organizations yell at Google. But Bryson seems to think "what [she] know[s] is more useful than [her] level of fame is validating", so I guess that's fine by me. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-04 14:40:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's typically hard to make predictions about this. The advice I've always learned is to just test it. It's going to depend on the task and the amount of data you have.

However, we could say that increasing the size of your network should increase its "variance" in the bias-variance tradeoff. This means it should make it less likely to underfit and usually more likely to overfit. In any case, I would expect a larger network to pretty much always have a lower MSE on the train set, but if it's too large, the MSE might be higher on the test or validation set. A rule of thumb I've heard is that the number of parameters/weights in your network should be 5 to 10 times smaller than the number of examples you have. However, this is old pre-deep learning advice. The prevailing wisdom today seems to be to make the network as large as possible and prevent overfitting with a technique called "regularization".

My guess would also be that a larger network learns faster, because you effectively have a higher learning rate. (Imagine creating your 10-neuron network by just duplicating your 5-neuron network twice. Then for each update made to the 5NN, it would make 2 to the 10NN.) But I'm not super sure about this, because the larger network is of course also searching in a larger search space of possible weight configurations.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-04 12:35:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki that may be useful to you.

I think it's great that you want to get into ML, but I do have some concerns.

> I'm not sure if Spring Board's 6 month AI program is legit

I don't know either. I suggest looking for reviews like [this one](https://www.reddit.com/r/datascience/comments/7hdlh6/review_of_springboard_from_recent_alumnus/). I don't have anything to compare it to, but it does sound like it's quite a lot of money. It may be fine though (I don't know).

> I want to make an important note that I don't want a job as a coder

Here is how Springboard describes the job: *"Today, a machine machine learning engineer is a software engineer who not only understands the latest machine learning and deep learning concepts but is able to deploy an AI system in production that is highly reliable, fast and scalable."*

That may not be the same as a "coder", but you're still a "software engineer" who will probably do a lot of coding. Again, that may be fine, because I don't know what you envisioned the difference would be between a coder and someone who works in ML. Just know that you will probably have to code/program quite a bit.

> My longer term goal of 5 years is to get into quantum computing.

This is probably what concerns me the most. Not necessarily that you want to get into QC, but that you're not coming at it directly. Why are you first transitioning to one thing and then to another? Why not just go for QC directly then?

ML and QC don't really have that much to do with each other. Especially the kind of practical/applied ML Springboard will train you for. (If you get a PhD in ML, you may be able to specialize in its relationship to QC, but getting to that point will require a lot more than a year.)

I don't really know much about QC, but it seems very different to me than "websites in the 90s". Websites, even then, were relatively easy to build, and the tools to build them were accessible to virtually everyone. QC on the other hand seems extremely complex, costly and not currently that useful. It seems like it's still mostly in the research stages, which would mean you need an extensive education to get into it. You can do that of course, but it probably requires going back to university to study something appropriate (like physics or maybe computer science), and then maybe even getting a PhD in it. You can of course do that, but then I probably wouldn't spend your next year on something entirely different.

> I know it's quite a transition from general labor to such a technical subject but nearly anything is possible as long as you put your mind to it!

Good attitude! I'm sure you can do it, but I'm just a little bit concerned about your plan for how to do it. Anyway, good luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-04 10:23:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on /r/artificial's wiki that may be helpful to you.

The most common academic path is probably through Computer Science. If you could find a program that's explicitly on AI or Machine Learning, that would be even better. Data Science can also be a decent choice. Mathematics could work, because it will teach you good skills, but it will probably take more effort to get in touch with the "AI world". Cognitive science, neuroscience, philosophy and linguistics are all possibilities as well, but I probably wouldn't recommend it unless you are very specifically interested in one of those. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-03 17:39:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is probably super medically irresponsible / inaccurate, but I've noticed I more often wake up to pee if I don't eat potato chips in the evening. So I'm guessing it has to do with eating more salt.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-03 13:47:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> Interesting post. My first question is: why couldn't your system be implemented as a utility function?

I see two main problems with this (aside from the difficulty of actually encoding this in a machine that virtually every approach suffers from). One is that it basically seems to say you can never do anything. It's impossible to foresee all of the consequences of your actions, but it's likely that due to the butterfly effect any action you take may eventually contribute to some right being violated somewhere. If inaction also counts as a kind of action, then I guess inaction isn't recommended by default, but the problem remains that you're underspecifying the moral system to actually select an (in)action. If (in)action A ultimately (likely) leads to property violations of a person's entire body (i.e. life), 10 cats and $1000, while B leads to violations of two people and $3, what action should be taken?

My second problem is that the moral system you outline here is still in conflict with the moral systems of lots of other people. This probably becomes clearer when you add the weightings I'm claiming you have to add in my last paragraph (people might obviously disagree there), but also seems true even if we keep it as a "pure" rights system (which I claim doesn't work). Like you said, your system doesn't require anyone to feed a starving person. Well, a lot of people would probably disagree with that (and with other cases like "it's okay to steal a loaf of bread to save your kid's life").

I get that your system is consistent and has no internal contradictions, but that would be true for any utility function I put into an AI, and it doesn't solve the conflict of interest problem between people who might advocate for a different moral system.

You may also want to cross-post this to /r/ControlProblem. They're dedicated to AGI Safety and always looking for new posts.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-03 11:39:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> My first thought upon reading the title was [Winograd Schemas](http://commonsensereasoning.org/winograd.html) since they supposedly require common sense to solve. I think the best AI systems can get something like 70%, but they're pretty easy for humans. However, this isn't really a problem people have that would make sense to crowdsource like actions in FoldIt were. (BTW, I wonder what the relationship to [AlphaFold](https://deepmind.com/blog/alphafold/) is. Are they trying to solve the same problem?)

This is a bit on the edge of what AI can do, but if your AI *currently* can't do a thing yet because it lacks data or guidance, you can crowdsource this task to humans. This is what reCAPTCHA does for instance. It's not that AI *can't* do OCR, handwriting recognition or image recognition, but it will be able to do it better with more labeled examples. There's also other AI research that benefits from human input, but I don't think that corresponds to solving (other) real-life problems.

A more nefarious idea that *does* solve real-life problems for its users is that spammers can show the CAPTCHAs their spambots encounter to people who use a service they provide (like porn, often illegal software, maybe a game, etc.).

I don't know open problems like protein folding lend themselves to being facilitated by human input in games, but I notice that the makers of FoldIt have made quite a few [other games](http://centerforgamescience.org/games/) as well that you could perhaps look at.

Intuitively I'd say that humans can outperform machines if the puzzle requires common sense or certain kinds of creativity or intuition. Basically, if there are a lot of options / a large branching factor but most options look "obviously" bad to humans, humans can have an advantage because machines usually still have to look at all of those bad options (however, I think with recent-ish advances in learning heuristics for e.g. Go/chess, this has become less true).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-02 18:33:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> What are you talking about? Strumia *is* a scientist who wrote a [paper](https://alessandrostrumiahome.files.wordpress.com/2019/03/artgender.pdf) on which he based a [presentation](https://alessandrostrumiahome.files.wordpress.com/2019/03/strumiagenderslidescern.pdf) at an academic workshop for which he was attacked. What are these letters you're talking about? I cannot find any. Are you confused with someone else?

I'd say the presentation slides certainly don't look politically neutral, and we could indeed discuss whether his presentation was appropriate (obviously I don't know what he actually said). But he *was* definitely at a scientific workshop in his discipline presenting (bibliographic) research he had done. (And for the record: he had done bibliographic research before, and many of the other presenters at this Workshop on High Energy Theory and Gender were also physicists presenting research that was more sociological than physics.)

The only other example in the article makes it clear that he moved to China because he was fed up with the politics in Australian academia. There is also no suggestion that this guy was attacked for the science that he did: he just felt like he couldn't get "a tenured job in astronomy if you don’t belong to a protected group (alas, I am a white hetero Christian male, bad luck!) and/or you don’t do enough visible activism (or at least enough virtue signaling) for a number of green-left issues".  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-01 20:02:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> FYI, your post was caught in Reddit's automatic spam filter. I have no control or specialized information about that, but in my experience it always happens when people use URL shorteners. It's better to just link things directly on Reddit.

https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-01 14:06:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> You can do AI research with those degrees, e.g. into cognitive architectures or (more realistic) neural networks. Arguably computational cognitive science and computational neuroscience are both "AI". </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-01 12:02:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> Could you make your point in a more coherent manner? Are we supposed to know who Jeffrey Epstein and APAX are, what they have to do with each other or what's wrong with them?

I see Epstein has donated untold millions to many scientific endeavors, and that he is also a registered sex offender. So? That seems like a fairly private crime. How do you figure it is related to OpenCog/Goertzel specifically, and how do you think AI projects looking for funding should treat the money of a sex criminal? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-01 10:34:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I want to find an area that will still be a problem 3-5 years from now.

Virtually all areas will still be a "problem" 3-5 years from now. Just look back in time for that long. Are there any areas that completely got solved? If you're worried about loss of popularity, just pick something that's up and coming, rather than something that has been fizzing out for a while.

You'll actually want to pick a *topic* that seems clearly solvable in 3-5 years, because that's what you'll have to do. It may also mean that others could beat you to it. To mitigate this, you should probably pick something where you have a big advantage. That might be a special talent/interest/experience, but more likely has to do with the lab where you're located. If you pick something similar to what your supervisor does, it will 1) be less likely that others pick the same topic, and 2) you'll be at an advantage as you have the world's foremost expert advizing you and you're working in a lab that's already set up for that kind of work. Of course, this means you won't be as independent as you might be if you tried to take the much harder road of doing everything alone, and it means you're more-or-less executing someone else's research agenda (as a PhD student you should always have some freedom here though), but it doesn't sound like you have a super clear idea of what you want to do yourself anyway.

In any case, you should expect your topic to change. Professors always told me that nobody actually ends up doing exactly what was in their PhD proposal. If the field moves in a certain direction, or someone beats you to a certain milestone, you should be able to adapt.

> The megalomaniac in me wants to work in an area that can (one day) become its own subfield within AI.

This is more-or-less what I did and I would recommend against it. Especially if the reason you're doing it is that there isn't really much already there to build on (if there is, you're not really starting that subfield). Making incremental improvements to existing things is much easier. Performance criteria have already been established and if you can sort of piggy-back on the perceived importance of the work you're improving.
 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-04-01 02:02:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> > My current understanding of AI is that 'AI' is a field with lots of different techniques within it, that are often used in isolation, and for very specific purposes (e.g. machine learning for classification problems).

Yes, that's pretty much correct for 99% of AI research and development. This is often referred to as narrow AI to contrast it with AGI.

> Is anyone actually trying to build a unified software system that can perform all intellectual tasks that a human can?

Yes, although this is relatively rare (although arguably less rare for the past 5 or so years). We have a section on [Getting Started with AGI](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on /r/artificial's wiki that points at some research that *is* doing that. One of the problems is that there are not only different definitions of AGI, but also different approaches with different apparent milestones. All of this makes it very hard to [evaluate progress](https://www.reddit.com/r/agi/comments/52tv08/benchmarks_besides_turing_test/) towards AGI as well.

This makes it difficult to say who or what is the closest system we currently have to AGI. I'm inclined to answer with cognitive architectures like OpenCog and NARS (check the wiki), because at least they're explicitly targeted at AGI and they are open to arbitrary problems in an important sense. However, getting actual AGI may still take very long, so it's very hard to say. Maybe e.g. OpenCog is on the right track, but it will take them 20 years to realize AGI, and maybe in 10 years a company like Deepmind will get on the same track and beat them to it because they have much more money, resources and talent.

> I'm no expert in this area but it seems to me that that would require many work in many different areas of AI, such as NLP, computer vision, etc. It seems like we have quite impressive systems in many of these seperate fields, but are people trying to build systems that bring all these types of things together?

Well, ideally AGI doesn't require messing around with all of that. Maybe there's a "master algorithm" that underlies most of cognition (e.g. whatever the fairly uniform neocortex is using). More unified approaches like NARS work like this.

On the other end of the spectrum is the idea that you could just build a million narrow AI programs and "just" glue them together with a Big Switch program that switches between the narrow AI programs when it sees fit. The problem with this is that virtually all of the complexity of building AGI is offloaded unto that Big Switch program.

Somewhere in between you have an approach like OpenCog that does have different components for processing language, perception, procedural knowledge, declarative knowledge, motivation and so on.

> On a related point, what's the closest thing we have to a replicant? Some kind of humanoid robot whose software is designed so that it mimics a human as much as possible.

I think the Japanese and Disney are quite good at creating humanlike animatronics (or "robots"), but they lack intelligence.
[Sophia](https://www.hansonrobotics.com/sophia/), despite not nearly as impressive as the talkshows might have you believe, may be the best somewhat humanlike robot platform, as it actually allows integration of many modalities and can be controlled with actual AI software based on OpenCog or SingularityNET. However, it doesn't have legs of its own (or it didn't use to have them; although it can be connected to a "leg platform"). Something like Boston Dynamics' Atlas probably has a wider range of "useful" motions, but clearly doesn't aim to pass for a human. "Petman" may be the best walker, and with the hazmat suit looks kind of of human (although that should probably be considered cheating).

Since a replicant should presumably also sound human, you may also want to look at Google's Duplex project (although one of their main insights was to make the AI narrower). You could also look at what chatbots are winning the annual Loebner prize.

If you put all of that together, you could perhaps imagine the best "replicant" that could currently be made. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-31 13:25:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> How do you square exposure to culture of AI with a light workload and how do you define busywork? A lot of courses give you assignments and project work because it's the best way to learn.

I don't know what courses your university offers, but some AI 101 courses might just tell you about the categories of AI without giving you a lot to do. But you mention you already know quite a bit about AI, so maybe that's too elementary for you. If you want to dive deeper, I would suggest picking a subfield of AI and taking a course on that. And if you really want a taste of the research culture, I would look for a course that has you do a group project. That way you have the biggest opportunity to see from start to finish what it's like. I would look in the Computer Science department, and as for course titles, it depends on your personal interests: Machine Learning, Deep Learning, Neural Networks, Natural Language Processing, Computer Vision, Pattern Recognition, Data Science, Search & Planning, Multi-Agent Systems, Expert / Knowledge-based Systems, etc.

> Would a philosophy or pyschology course in cognition or a computation biology course be what I am looking for.

Well, I don't *know* what you're looking for, but while these things can be interesting, I'd say they're not really used in the vast majority of AI careers. Furthermore, they have their own perspectives and culture, so you'd learn about that and not AI. I think that if you were going to study AI for your bachelor, master and PhD, these things should be part of the program, but if you're only taking a few AI courses, I would not focus my energy here. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-29 12:39:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Editing your posts post-ban to continue a conversation and/or flame people counts as putting stuff in the subreddit.

What about editing posts to do other things? You could say any edits literally put stuff in the subreddit, but this line seems to single out particular kinds of edits, perhaps leaving the suggestion that other kinds are allowed. I can imagine that if you do want to allow *some* edits, the line is hard to draw exactly.

But would it for instance be allowed to edit in a sincere-sounding apology? Or to remove/rephrase parts of the post (especially the offending parts)? Presumably the first-order effects of this would be positive. If part of a comment had the potential to derail conversations, this potential is removed and future damage is prevented to some degree. Value could be added by linking sources for previously unsubstantiated controversial claims. Anyone can see that the post is edited after the ban has been instated, so this shouldn't call into question the moderation.

Allowing this may not be worth it, but in any case, I think it may be desirable to clarify the kinds of edits that are still allowed (if any).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-29 12:16:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> I like that they did this, but I don't think all of the "better" charts are really that much better.

My textbook also says that truncating the scale is bad, but in this case I'd say it was pretty obvious. There's a tradeoff between the Original and the Better chart: the Better chart is indeed better for telling the story that Corbyn is far ahead of other left-wing competitors. If the story you wanted to tell includes more or less exact amounts of Facebook likes for these different groups/people, the Original chart is a lot more readable.

The discussion of the "butcher's dog" chart makes me think the author doesn't understand the concept of correlation. The Original chart chose its scales so that you could clearly see that correlation, but according to the author the correlation disappears when you change the scales. This is of course silly, as it's not how correlation works. The Better chart is just worse at showing the correlation is close to 1.

Then with the Brexit poll, the "Better" chart is supposedly better because it smooths the results and doesn't make "it appear[s] as if respondents had a rather erratic view of the referendum result". I prefer the Original chart, because it gives me some impression of the inaccuracy of their method. The scatterplot they drew in the Better chart is nice, but it's a lot less clear.

In the "free markets and free workers" chart the point is presumably again that the two measures are strongly correlated. The Original graph shows this somewhat (although not super clearly), while the Better graph fails to tell that story entirely.

I agree that the last chart has quite a lot of data, but it still seems pretty informative to me. Certainly more than omitting it.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-29 00:07:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> I would suggest reading some literature on the issue. /r/ControlProblem has some on its wiki and side bar. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-29 00:02:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> > but in the paper I didn't see an analog to evolution by selection of next generation from offspring of individuals who reach age of reproduction.

I don't see any mentions of this either. They say they used a modification of the NEAT algorithm, so I assume they used its standard selection methods. The fitness score is how long the individual survived.

> Another dumb question: some reward/penalty inputs are built-in to natural individuals: hunger and satisfaction, physical pain or pleasure. How is this modeled?

A binary reward or punishment on each time step.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-28 09:21:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> It would probably help if you linked to the YouTube videos in question.

Language like "generation" and (less commonly) "dying" tends to belong to evolutionary algorithms, not (deep) neural networks. In typical neural networks the "activation value" of each node/neuron is determined by a nonlinear function applied to the weighted sum of its inputs. This means that the connection weights ultimately determine the "behavior" of a neural network: i.e. how it transforms input into output.

Learning is done by adjusting these connection weights so that the behavior becomes "better" according to some objective function. The most common algorithm for this is called backpropagation, which looks at the average error the network made over a number (batch) of examples, and then changes all weights a little bit in the direction that would make this error smaller if it was computed again.

Another way to adjust the weights is through evolutionary computation. The idea here is that you have a population of neural networks (individuals), each with a different set of connection weights. Some of these will get higher performance/fitness than others. When moving the population to the next generation, you typically take the best few individuals, and generate variations on them. This can be done by mutation (i.e. some of the weights are randomly changed) or crossover (i.e. you take half the weights of one good network and the other half from another good network; a naive implementation of this doesn't work very well for neural networks though). Some of these variants will be better than the previous generation, and some will be worse. For the next generation, you continue with the better ones, create new better-and-worse variants again, and so on.

Learning algorithms tend to get their data from humans (with supervised and most unsupervised learning) or from the (typically simulated) environment (with reinforcement learning).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-27 18:08:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> I (male 30-39) scored 7: https://imgur.com/IA2NGBR

Pretty useless statistics by the way. They just give the minimum (0) and maximum (39999750), which I thought was 100 by the way. Why no mean/median and standard deviation? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-27 17:39:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is kind of an idea paper (there are no experimental results or anything), but the idea of creating AGI by "just" (re)running evolution is of course quite old, and IMO naive. We're the living proof that you can get GI through evolution, but that proof only applies if you can reproduce the evolutionary setting more or less exactly. Of course, simulating the entirety of nature on Earth for a couple billion years is not super feasible, and even if you get very close you can get stuck with e.g. dumb dinosaurs if you forget to drop one little meteor. But of course our actual evolutionary algorithms don't even come close to simulating evolution on Earth, just like our artificial neural networks (even spiking ones) aren't particularly close to natural ones.

This paper wants to use Neuroevolution to get AGI (NAGI), but doesn't really make a great case for why this should be the way to go, and how they ensure that they're going to get to AGI in any appreciable amount of time. They massively simplify the setting by fixing the "bodies" of the agents, if you can even call them that (there's no real body moving around an environment or anything, just fixed I/O). At the beginning of each generation the weights of the agent's spiking neural net are randomized so it has to learn online, during its life, based on inputs and a reward/punishment signal (which comes from the environment, and therefore is *ex*trinsic, authors). The authors don't describe very clearly how this reward signal is used in the learning (although it's "important"), but still claim this isn't reinforcement (or supervised) learning, but "self-supervised".

The environments are very simple. In the first, the agent gets one bit of input (black or white) and produces one bit of output (eat or avoid), and in the second the input is a 2-dimensional real vector. What is somewhat interesting is that the environments are "mutable", in the sense that at some discrete timesteps they will change what the "correct" output is (e.g. in env1 you should eat white and avoid black for the first 8 timesteps and vice versa for the next 8, and so on). Furthermore, the agent's (unobserved) health is affected by the correctness of its outputs somehow, and "lifetime" is used as the fitness function. The authors make a point about how agents can only learn (in their way) in *reactive* environments (i.e. that affect and are affected by the agent), unlike e.g. image classification tasks/environments, but aside from the "lifetime" thing that the agent cannot perceive the example environments sound *exactly* like mini image classification environments to me.

Some of the ideas here sound like they could be okay, but they are not motivated very well, and the description is not concrete enough for me to actually implement it. I think the manuscript needs significant editing before I would submit it for publication, and even then I would aim at a small low-impact conference or workshop or something.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-27 14:43:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> AI (and its subfield ML) is a very broad field. ML has many different subfields, classified among different axes, and many methods for achieving it. These include deep neural networks, support vector machines, linear/logistic regression, random forests (bagging), (gradient) boosting, nearest neighbors, and yes, evolutionary/genetic algorithms. As you've noticed, the concepts of AI and ML are quite expansive, and among other things include anything a mathematician might call "optimization" (as long as you're studying it from an AI perspective and not a pure math perspective).

You ask us to evaluate the veracity of your supervisors' claims, but of course we don't know the details of whether you/they're *actually* doing cutting-edge research that's used in the real world and whether they *actually* have close industry collaborations for you. But for each of the methods I listed above, it *could* be true in some sense: they could all be considered AI/ML, I think they all still have their niches (so they're used in the real world by potentially some industrial research partners) and for each you could do cutting-edge research in the sense that you could improve that method (but not always in the sense that you will outperform all other methods).

If you thought "all (cutting-edge) AI = deep learning" you were wrong. However, I can't say if your supervisors tried to capitalize on that common misconception. If I'm working on e.g. case-based reasoning, and I point to AlphaGo / OpenAI Five and say "See that? That's AI. I'm also doing AI. Want a PhD?", I'm not *technically* lying, but that could be a bit misleading. Of course, I have no idea if something like that happened. I would find it slightly weird if they just promised you a PhD in AI/ML with no further details. In my experience, when talking to people who have any knowledge about the field, professionals would tend to be more specific about what they're working on in what subsubsubfield.

Obviously deep learning has been the most popular method this decade and it has encroached on the territory of some others (e.g. I don't know off the top of my head what SVMs are still better at), so I understand if you want to be the millionth person who wants to jump on that bandwagon. But evolutionary algorithms are actually also still relatively popular, as you noted yourself. Neural networks still need to do some inner optimization, which is traditionally done with the backpropagation algorithm, but which can also be done with genetic algorithms (and this is often done too). There's a whole field of neuroevolution, and these methods are also often used in metalearning and hyperparameter tuning.

I'd say it's still an interesting area of research within ML, but of course that doesn't mean you need to be personally interested in it. Getting a PhD is a huge investment. Consider carefully if *this* PhD is worth it to you, if it can get you what you want out of life, and otherwise consider switching or stopping.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-27 02:28:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> > people who are smart enough to succeed in any other profession will generally go into that profession

I get that it's kind of a chicken-and-egg thing, but this is what she's trying to change, right? Maybe if teachers make more, more talented people will consider it as a career.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-26 10:53:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> My bachelor and master programs were specifically on AI. I've had a number of related side jobs and internships (e.g. teaching assistant, part-time programmer for a professor's startup and brain-computer interfacing researcher) before I worked full-time in a computer vision company for some years. I then went to a research institute to research (mostly) reinforcement learning and shortly thereafter I started my PhD on (roughly speaking) AGI. And now I have a postdoc position on AI and ethics, which is currently not super related to AGI unfortunately.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-26 10:45:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know why you think an uploaded human would be more dangerous or harder to stop than an artificial general intelligence (AGI) / artificial superintelligence (ASI). There may be a slightly larger ethical conundrum, but if a (normal) human poses a large threat we typically don't mind eliminating that threat. Police and the military do this all the time, so I don't see it as a huge issue. At least a human upload would have some semblance of human values. I don't really think that'd be very safe, but it seems a lot better than a completely unaligned ASI.

You may be interested in /r/ControlProblem. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-26 10:40:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> You seem to have a couple of misunderstandings in your OP and your reply to /u/Geminii27's excellent comment.

**If humans do X, then thinking an AI would do X is engaging in the anthropomorphic fallacy.** This is not the case. It's only anthropomorphism if you think AI would do X *because* humans do X. If you arrive at X through a reasoning process that has nothing to do with humans, that's entirely valid. Thinking "that's what a human would do, therefore it's not what an AI would do" is faulty logic.

**Recycling paperclips would satisfy the paperclip maximizer, and this would be safe (aside from glitches etc.).** The goal / utility function of the paperclip maximizer is not to be constantly in the process of making a paperclip. It's to maximize the number of paperclips. You think turning humans into paperclips and some other actions are illogical, because you're working with a different utility function. But if you used the one I mentioned, I hope you can see that converting other things into paperclips is clearly the way to go (since recycling also destroys paperclips).

**Your paperclip recycler would be safe  (aside from glitches etc.).**
Even in your recycling scenario humans would probably still get killed by an intelligent enough AI. If all it cares about is recycling paperclips, then it should make (sub)goals to ensure it will be able to do that as much (and as fast?) as possible. If it's shut off, it won't be recycling paperclips, so that's bad, so it should ensure that doesn't happen by eliminating threats and protecting itself from that scenario.

**If it was more intelligent/competent, it could achieve the same things *without* killing humans.** (Not sure if you said/believe this, but it seems like a common and related misconception.) This is probably true -- more intelligence helps in dealing with "handicaps" like this -- but why would it make things harder for itself? Paperclipper AI doesn't care about human lives. That's the whole point. The cost of the "distraction" of eliminating humans will be measured only in numbers of missed paperclips -- not things like "effort" or "human lives" or whatever -- so it's just a matter of weighing the finite short-term losses against the long-term gains of being able to continue the process unthreatened.

**The paperclipper might not be able to eradicate humans.** (I'm getting this mostly from the first paragraph of your reply to Gemini.) This isn't strictly false, but the point of the thought experiment is to imagine what an artificial superintelligence (ASI) with a relatively simple and innocuous sounding goal would do if it could. As long as the AI isn't very intelligent / powerful, its optimal strategies would probably be very different.

---

Perhaps it helps to envision that two paperclipper ASIs are created, one that does as everybody says (i.e. pursue [convergent instrumental goals](https://en.wikipedia.org/wiki/Instrumental_convergence) and kill threats / competitors / resources), and one that does what you're saying. Which one will succeed at the goal better? I'd say obviously "mine" would eliminate "yours" rather quickly if it's just sitting around recycling paperclips, either to eliminate threats or because it has useful paperclip recycling capabilities that can help "mine" recycle more paperclips faster. If you want, you can also imagine them being in different universes, and compare who would succeed more at their goal in the long run, taking into account speed (which can be gained through more resources) and duration of activity (who will get shut off if humans get tired of the AI, or some natural disaster strikes Earth or whatever)?

The point here is to show that the genocidal AI is *objectively* better at achieving the paperclip-related goal than your peaceful recycling AI. It has nothing to do with human characteristics: it's just more intelligent/rational/optimal.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-25 12:46:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm on mobile right now, so it's hard to link, but they're in the Getting Started article on /r/artificial's wiki. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-25 11:21:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think Hutter's approach and others based on algorithmic information theory are quite interesting from a theoretical perspective. I'd say it's mostly useful for analyses of (super)intelligence and maybe for evaluation (as in AIQ), but I don't see it providing a ton of guidance about how to actually make AGI. However, I think people like Alexey Potapov are running with it and combining it with probabilistic programming and deep learning, so perhaps it's more useful than I think.

I really like Wang's NARS. It's just so elegant as a unified approach to AGI. I think as a reasoning engine it's quite amazing, but the control part lacks the rigorous foundations and is (I think) more ad hoc. I'm afraid this is a big shortcoming, although Wang now (finally) has a team who can help him work on it, so who knows. At the last AGI conference they also had a poster on incorporating deep learning, which may help with some of the efficiency issues.

Goertzel's OpenCog is far less elegant, and I think it's a bit lacking in a unified vision. Goertzel has a tendency to want to incorporate everything he sees and finds promising. I used to be more skeptical of this, but I have kind of come around to his idea of cognitive synergy (in my own words: maybe the universe is messy and no algorithm is good at everything, so you need a bunch of them to fill in the blanks for each other). This is a little reminiscent of the "big switch" approach (build a million AI programs and then "just" glue them together with a Big Switch that switches between these AIs when appropriate), where the problem is that most of the complexity of AGI would actually be in the glue / Big Switch itself. OpenCog is kind of trying to address this, and I would like to see more efforts on that core issue, rather than on things like Sophia and SingularityNET (even though I understand they're part of the plan).

Out of these three I personally prefer working with NARS when doing more practical things, and I will sometimes look at Hutter-style theory when I'm doing more theoretical things. However, maybe OpenCog with it's larger team and "messier", more practical approach is actually the most likely to get AGI first (but I'm very uncertain). I also like AERA, which is unified like NARS but more focused on control and less on logic, but the project seems to be kind of dead. And I'd like to look more at other cognitive architectures like Sigma. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-25 10:33:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> Jürgen Schmidhuber, Paul Rosenbloom, Joscha Bach, Kristinn Thórisson, Claes Strannegård, Alexey Potapov. You can check who is publishing most at the annual AGI conferences and journal. And of course there are many people working on deep learning who hope it'll lead to AGI, including Geoff Hinton, Yoshua Bengio and Yann LeCun. Companies specifically working on it include GoodAI, Deepmind, Vicarious, Numenta, NNAISENSE and OpenAI, and you can probably also include most big AI companies (Google, Facebook, Microsoft, Amazon, Apple, Baidu, IBM, etc.).
 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-25 10:10:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> He's still working on OpenCog and so is his team. SingularityNET and Sophia / Hanson Robotics are part of his AGI vision though.

OpenCog always had a couple of different modules (MindAgents) to deal with different kinds of situations and hopefully achieve some kind of "cognitive synergy". SingularityNET links together even more AI systems.

Goertzel's approach is also to build fairly humanlike AGI for a variety of reasons, and having a humanlike robot platform fits into those plans. Plus he thinks (social) experience is a large part of the equation and looking like a human helps people treat the system like a human.

So basically, it's all part of the plan. (Although I'm personally most interested in the "pure OpenCog" part of that plan, so I wish even more of the focus was on that.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-22 20:02:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm a postdoc in my mid-thirties with a few years of industry experience, and I've studied and worked in AI for my entire adult life. I don't know if I qualify as "young" to you: mid-thirties doesn't sound very young to me, but I'm also not an old professor and postdoc is a fairly junior position. I think that a non-negligible chance of existential risk from "default AGI" would make the control problem probably the most important issue to work on, but I actually consider that chance large. I think this even though there's also uncertainty about when we'll get AGI, because there's also uncertainty about how long it will take to solve the control problem and it could be the case that the AGI system will need to be constructed with safety in mind from the bottom up. I also think useful work can probably be done on it now, and at the very least we should foster a professional culture of safe AGI research.


 That's for your "survey", although I don't know how you can conduct a useful survey this way. This sub is fairly low traffic, so you'll probably get a handful of more experienced experts to respond, while you've kind of encouraged the young hobbyists to shut up because they probably don't want to feed your skepticism. I'm curious what you'll conclude from that, but I think this is probably a fine way to get somewhat older professionals to give you their opinion, which can also be interesting of course.

> I have a bit of a special position in that I think AI safety is interesting, but have an (as of now) very skeptical position to the discourse around A*G*I risk (i.e. the control problem) in AI safety crowds.

Just to be clear: when you say "AI safety" do you mean the same thing as "working on the AGI control problem" (like people working on the AGI control problem often do), or something else related to the safety of (narrow) AI? If "AI safety" refers to AGI for you, I'm curious why you think it's interesting despite not liking the discourse in the field. If "AI safety" refers to narrow AI for you, I don't think being skeptical of the control problem puts you in a very special position, but in that case I would agree that the discourse in that field about the AGI control problem tends to be bad (because most deny it's a problem). I think seeing your blog posts will help understand where you're coming from.

> "Fan of technology"

Why did you include this criterion? The others seem like they could elicit some skepticism, but I'd think that virtually everybody working on AI can also be classified as a fan of it. And even for amateurs this should make it more likely that they're informed and perhaps also more likely to deny the dangers because it contradicts their fandom. I also think this undermines your observation, because there are a lot of old people who hate and fear technology/AI.

I think that if we convert your observation to one that says older, more experienced AI/ML professionals are likely to be skeptical of the control problem, then I agree with you. Among professionals, I definitely think the group of believers skews young (and therefore naturally less experienced). I think some surveys also bore this out, but I can't find them (on mobile); this is also my anecdotal experience though.

As a skeptic, I understand you feel strengthened by this. I certainly wish older professors would see the light. My guess is that they're more set in their ways, less open to new ideas, "inoculated" by bad arguments about Terminators 35+ years ago with no time/interest to read the newer better arguments, and more materially and cognitively/emotionally invested because they've dedicated their careers to something they always considered good. I guess this is why they say science progresses one funeral at a time.

I also want to say that I think it's a mistake to treat people with "applied or research experience in AI/ML" as experts on the risks/safety of AGI. For most of them, their work has absolutely nothing to do with this. You can see this when they try to say something about the control problem: when they're not making appeals to their own authority, they virtually never relate it to their actual work. Having experience in AI/ML is useful for me in AI Safety only because it gets more people to take me seriously, but it has not actually taught me much about it: virtually all my knowledge in this area comes from experts who have dedicated their careers to it, because *that's how you become an expert in something*. (Naturally this is also true for other areas: what I know about NNs comes from NN experts, etc.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-21 18:22:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I can see it more with the swimsuit photo, but then I think you also have to ban the 18th episode of the pokemon anime on similar grounds.

Yes, I also voted "yes" on removing the Pokémon episode, because I agree with you Misty is a sexualized minor. As I mentioned, that's because I want to minimize subjectivity in rule enforcement. But I also think that if a rule bans a Pokémon episode for reasons other than maybe copyright, that rule should probably be re-examined.

> The konosuba valentine photo is named that because the original post was made to be a valentines day card

I'm not surprised.

> The Goblin Slayer scene was definitely meant to be more "goblins are evil" than sexy, but... Kuroinu episode 1 scene 1 is nearly identical in many ways and obody would argue that Kuroinu isn't porn. So it is counted as "being sexualized".

I don't know what Koroinu is, so I don't have a lot to say, except maybe that I think context can matter. If there's a rape scene in a porn show, it's more likely to be "sexualized" than if a virtually identical scene is in an action/adventure/horror/whatever show where it's meant to be interpreted as something horrible. Context of posting matters too: if you then cut out that particular scene, stripping it from its context, and post it on /r/sexyrapeclips or whatever, it's also more likely to be sexualized.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-21 14:26:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> I voted "yes" on the "Should they?" question a few times because I want admins to follow Reddit's rules, even if I disagree with those rules. The alternative is a much more arbitrary system that gives the admins even more room to govern subjectively, and presumably not just for this one rule. I think anime/art presents an interesting case that makes it clear to me that the reasonable sounding rule of "don't sexualize minors" is too simplistic. If it were up to me, I'd prefer a rule that allows all of these clips, even though I'm not really interested in anime.

I will say that a lot of these things seem obviously sexualized to me, in the sense that I imagine the artist was clearly trying to draw a sexually tantalizing picture. It's often not even the fault of the (fan) artist, because the original shows often seem to already make underage characters have insane looks and bodily proportions clearly meant to be sexy.

The swimsuit picture is very different from a typical father posting a picture of a swimming competition. The father would be posting a picture of their daughter at a sporting competition (or day out) where she just happens to not be wearing a lot of cloths. I would be a lot more suspicious if I asked for a picture of his daughter (why am I doing this again?) and he dug up some old bikini picture of his daughter pouting from a low angle with hearts drawn around it. I would wonder: out of all the pictures you could have shown, why did you choose that one? Of course, a real-life parent is still limited to photos they actually took, where the kid looks nice, that they have available, ordered by recency. But a (fan) artist could draw literally anything. And the choices for this outfit, that pose, the hearts, etc. signal what they were probably going for.

A somewhat similar argument could be made this "Konosuba valentine" picture. These characters seem like they're sexualized to begin with (at least the blue and blonde haired ones), and the picture seems intended to capitalize on that. I imagine that why "valentine" is in the name of the picture? I think the intended reaction for this picture was "Wow! They're so sexy!". I think that's *fine*, but I can't say it's not sexualized.

I actually thought the Goblin Slayer scene was only borderline sexualized. I think there are some shots in it that are probably intended to be somewhat sexy (and the looks/proportions of this character are again very amenable to that), but the actual (rape) sex itself seems to be intended as horrible rather than sexy, and it's mostly off-camera and underscored with sounds of agony. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-21 11:02:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> The Turing test is not a chatbot, but a test for chatbots. There is no "Turing bot". There's actually also no official Turing test, but there is the [Loebner prize](http://www.aisb.org.uk/events/loebner-prize) which tries to be somewhat similar. You can look on the website who competed, and google them. Some are available online for you to chat with.

You can make your own chatbots with AIML or services like wit.ai. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 22:56:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> Both branches/nodes can (and in this case should) ask the same questions. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 15:48:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm on board with the idea of developing safe AGI for the benefit of all. However, it's not very clear to me what your organization is or why I should want to join it.

The main characterization of your organization is in constrasting it with, and criticizing, two others. Placing yourself beside those two makes me think that you want to be on the same level, or surpass them. However, the communication here and on the website does not fill me with confidence that you will manage to do that.

First of all, you need to be able to sell yourself. The writing here and (more importantly) on the website is quite slapdash and informal. Even the title of this post is the vaguest it could possibly be, as you don't even mention your organization's name. And by the way, where does that name come from? What does it mean or stand for?

I find the description of your organization quite vague. You spend most of your text antagonizing other organizations, but you don't say much about yourself. Who are you? Who is this CMU Team you mention here, but not on the website (where you use a singular first-person voice)? Why should strangers have faith in you? Here you say you're a "democratic AGI ethics and research organization", and on your website you say you want to create a "decentralized organization, a structure similar to an ethics board, to help direct the development of AGI in a democratic way". Leaving aside the fact that ethics boards are a way to centralize ethical decision making in an organization/community, this is not a lot of information. I'd like to see some concrete goals, plans, a charter, *anything*.

On the website itself you basically admit that you don't (yet) have a vision: "I don't have all the answers, but I'm certain that the path we're going down right now is not the best case scenario." I appreciate your candor, but this doesn't sound very professional, or like a great foundation to start an organization on. Even slight rephrasings may look a lot better. I'm not super great at this either, but e.g. "We are starting this organization to help ensure the democratic beneficial use and development of AI for all. We are concerned with AI development that's currently siloed off in self-interested corporations and nation-states, and one of our spear points will be to investigate how to give the people a voice in how AI will affect their lives."

At the very least, I think this has the advantage of not being directly antagonistic to anyone in particular. Aside from being a weak characterization of your own organization, you start off by saying DeepMind has sold their soul and OpenAI is now showing their true colors. Those are fighting words. If your organization has the ambition to engage in global governance, you're going to have to be a lot more diplomatic. If I were you, I wouldn't talk this way about my *enemies*, and I definitely wouldn't talk this way about the organizations that are most likely to be your *allies*. Whatever you may personally think of what these companies are currently doing, they are some of the few--and definitely some of the largest--companies who say they are explicitly concerned with the safety and benificence of AGI.

If you're just a CMU student looking to start a discussion club on democratic AI (or whatever) that may eventually grow into more, I think 1) that's great, and 2) the issues I've mentioned are not that big a deal. But if your ambitions are higher, I hope that addressing these issues can make you more successful in your goals. In any case, I wish you the best of luck! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 15:13:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> Disclaimer: I've met and talked to him quite a bit at AGI conferences in the past decade, but I'm not working with him directly.

Ben Goertzel strikes me as incredibly idealistic, intelligent and productive, but he's also certainly unorthodox and a bit all-over-the-place. He took the initiative to start the [AGI Society](http://www.agi-society.org/) and has been heading this community of people who are actively, directly and explicitly pursuing AGI for over 15 years, when nobody really believed in it anymore. I give him massive credit for that.

You seem to think that him being a salesman is something bad, but I would argue that it's a characteristic present in many successful people and researchers. Part of doing important research is convincing others of its importance and the impact of your work, so I think it's likely that any researcher you've probably heard of has this ability to some degree. I agree that Goertzel seems exceptionally good at "selling" his ideas, because he managed to start multiple groups and companies with people he got "on board" so to speak.

However, I have a tough time seeing him as a con artist, like some people say. If he just wanted money for himself, he could make way more money working for some kind of corporation as an AI/ML scientist or developer. Hell, he could change his focus and make more money with one of his own companies. (SingularityNET is even a foundation, and they discourage trading of the coin.) I'm not saying he never asks for funding/investments, but it's all in service to his lifelong pursuit of AGI and transhumanism.

I personally don't think OpenCog is the best approach to AGI, but it certainly represents an interesting line of research. One thing you see there is Goertzel's tendency to get enthusiastic about a lot of different things: OpenCog incorporates many different systems and algorithms (sometimes even entire AGI approaches) as MindAgents. Understanding this, and Goertzel's related idea of cognitive synergy, also makes it easier to understand his philosophy with SingularityNET. I do suspect he got excited about blockchain as a part of the hype, but the way he applies it is not totally crazy: the core idea of having a more-or-less automated marketplace for AI systems to provide their services makes some sense, and the idea that he can utilize this for making AGI is in line with his philosophy of cognitive synergy and OpenCog.

I just hate the name (also "AGI token"). And I actually think this shows an instance of Goertzel *not* being great at marketing. At least not to mainstream narrow AI professionals. I think the same problem occurs with Sophia. I think Hanson Robotics and Goertzel are totally overselling its abilities and consciousness, which really irks professionals and causes them to totally ignore the fact that Sophia is actually quite an impressive robotics platform. And again there's a reason for Goertzel to work with it: he wants to create humanlike AGI and thinks a big part of that will be the system "living" and being treated in a way similar to humans. (He also says he has a reason for overselling Sophia's capabilities that I don't really agree with: he thinks people are currently vastly overestimating the time it takes to get AGI, so if Sophia adjusts those expectations downwards, it will only make them more accurate in Goertzel's eyes.)

In conclusion, I think you should take him serious as a researcher and generally view him positively. I think you should be skeptical of any ideas, including Goertzel's, and maybe include in that assessment the fact that he can get very enthusiastic about things and that he works in a rather speculative area where it's much harder to test ideas than in narrow AI/ML. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 14:16:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Approximately, how long do you think a single simple session of AI training could take?

What do you mean by this? Are you asking about the time it would take to do the machine learning to create the AI, or how long it would take a human to learn something from an AI tutoring system?

In any case, I think the question cannot be answered in general, because it depends on the AI, the task, the available resources, and potentially the human.

The whole survey also seems a bit behind the times, because AI is already everywhere, in daily use and in people's homes, and it doesn't seem to be specifically related to medicine (I assume the motion sickness questions are related to VR).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 14:10:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> Well, a lot of computer vision systems can't do without. I suppose the main question is how important those systems are to the present world. They enable things like automated video surveillance, sorting of products in factories, robots (including self-driving cars), etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 14:00:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I would recommend following AI/ML news a bit and look out for cool stuff you may like. You can do that on Reddit (here and e.g. /r/MachineLearning), but also on other outlets. On Medium I like SyncedReview, Towards Data Science and Sicara has a nice monthly review of top papers. I also like Two Minute Papers on YouTube. You can also follow companies or researchers directly.

arXiv is not great for discovering papers. [arXiv-sanity](http://www.arxiv-sanity.com/) is better for machine learning stuff. If you have a vague idea of what you like, you can search there or just use Google / Google Scholar (or Semantic Scholar).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 13:50:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> I recommend checking out the Wikipedia page on Facial Recognition, especially the [Anti section](https://en.wikipedia.org/wiki/Facial_recognition_system#Anti_facial_recognition_systems) and maybe the [Controversies section](https://en.wikipedia.org/wiki/Facial_recognition_system#Controversies) to find allies for lobbying against the use of this technology if you oppose it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 13:40:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> Why do you keep posting the same question? You got your answers. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 13:39:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> For my bachelor degree (roughly 1 decade ago) I researched catastrophic interference/forgetting in neural networks. It's still not a solved problem, so you could still do that.

Generally speaking a good strategy is to pick a recent-ish paper you find cool, and try to replicate (and then maybe slightly expand on) the results. To do this, you should take care to pick something that doesn't require data or computation power you can't get access to, although in some cases it can also be enough to see how something works on your own data set and possibly with less computational power.

Some cool things might be to generate fake images with GANs, or to get a neural network to play a game or games. Look for open source implementations before programming everything yourself. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 13:34:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> Are you suggesting there will be some kind of Data Security AI that makes sure the data floating around the Healthcare domain is secure, or that usage of AI in the Healthcare domain (e.g. for diagnostics) raises questions about data security? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-20 11:53:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> Maybe if they all did it, but I bet silent cartoons would be handily outcompeted by ones with voice actors.

I'll note that in countries like France and Germany voice actors are hired to dub virtually anything. Clearly they consider this better than just having subtitles *and* (emotive) English / foreign-language voices (and I bet the imbalance gets even worse if you don't even have foreign-language voices who can emote; expressive music is no substitute IMO).

I note that countries like the Netherlands are more likely to subtitle than dub. I wonder if that's because the market is too small to offset the cost of voice actors, or just because everybody speaks English anyway (and many speak German/French) and dubbing just doesn't look/sound as good. However, kids' shows (which a lot of cartoons are) *are* dubbed, presumably because kids cannot or don't want to read subtitles.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-18 18:22:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> > The bar for leading to AGI is so low in pretty much every field of AI/ML right now.

I'd say the bar for *maybe* leading to AGI is low, but the bar for *definitely* or even *probably* leading to AGI is quite high, mostly because we don't know what will and won't help. I would consider this an especially large problem with work that isn't even explicitly aimed at AGI, but rather at making incremental improvements to existing algorithms in the direction of least resistance.

My impression is that NLP might be a little better at this, because at least they're *trying* to tackle (NL)Understanding and common sense, even if the results at the moment aren't super impressive. On the other hand, maybe CV people are trying to do something similar with explainability and (adversarial) robustness, with similarly subhuman results.

So I guess I'm inclined to agree with your final statement that it may be a distinction without a difference.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-18 15:19:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> The decisions are made in order, going from the root (top) to the leaves (bottom). So if an example comes in like {Sunny, Normal Humidity, Strong Wind}, you start by asking the first question, **Outlook?**, where the answer *Sunny* leads you to the left branch and the next question, **Humidity?**, which then leads to the final answer ***Yes***. In this case you are absolutely correct that **Windiness** plays no role in the decision.

Alternatively, if it's *Overcast*, neither **Windiness** nor **Humidity** matter, and if there's *Rain* then **Windiness** matters but **Humidity** doesn't. If this strikes you as weird, my guess is that it's because this particular decision tree doesn't reflect your own preferences. But that's fine: it just reflects someone else's, and we could make a decision tree for your preferences as well. And maybe *that* decision tree would always take all variables into account.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-18 15:05:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> I haven't really had my mind blown in a while, but I generally like papers/books that introduce (and later expand on) cognitive architectures or general approaches aimed at AGI.

I don't really have a specific recommendation, but if you didn't know about it yet, you may be interested in the [Journal of AGI](https://content.sciendo.com/view/journals/jagi/jagi-overview.xml), the annual [AGI conferences](http://agi-conf.org/) (proceedings freely available up to 2017) or our wiki's [Getting Started with AGI section](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-18 14:59:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> > But it's so superficial from a linguistic and comprehension standpoint

Right, but do you think most vision models actually understand/comprehend anything? There are many papers showing they can easily be fooled and completely lack common sense (just like language models). It also seems to me that most NLP and CV nowadays use pretty similar deep learning methods, although NLP has a slightly higher chance of using something recurrent, which seems like a step in the right direction.

Since you mention you're a linguist/NLP'er, I'm wondering if it's just easier for you to see through the charade in this area, but computer vision is further from your expertise so it's not as clear what the limitations are.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-18 14:52:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> The Future of Humanity Institute's Center for the Governance of AI recently published a survey of [American attitudes and trends](https://www.fhi.ox.ac.uk/aipublic2019/). They also refer to this [Eurobarometer report](https://perma.cc/9FRT-ADST) from 2017 about EU citizens.

My personal impression is that most laymen tend to overestimate AI in some way, although there are some skeptics who have heard someone say that it's all "just statistics". I do think the situation has improved a little bit over the past decade, because before that laymen wouldn't encounter the term "AI" much outside of science fiction, where it basically means AGI. Now that "AI" is again a popular buzzword, this has changed, but coverage still tends to be about "this amazing new tech", "AI taking your job", "racist AI massively impacting you", or the end of the world.

People seem to internalize these messages mostly based on personality type and general attitudes towards technology, but I sense more positivity than negativity (but I'm an AI researcher, so this affects which laymen I'm most likely to interact with). I'd say most people now know a little bit about AI, but they're still pretty clueless, as is of course typical of laymen (I'm super clueless about other topics as well).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-18 10:54:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> Well, I guess some people are getting a kick out of seeing these fake faces and I assume OP wants something similar. If it creates joy, I'd say that's not a bad "point of this tech".

Other than that, I guess (deep) fakes are mainly going to be used to mislead: to sell something fake as (more or less) real. In some cases this is okay I think, such as in fictional entertainment. Maybe as portraits of fictional characters in a book or video game or something. Or perhaps they could be used for research in psychology / social science. Unfortunately, they could also be used by less scrupulous people, because a headshot can make some things look more credible and fake faces are not as traceable as real ones.

Aside from direct applications, I guess it could also be used as data augmentation and as an advance in AI research on generative functions in general. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-17 10:05:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> To be clear: I didn't mean to say that there's some algorithm written down like a recipe, and that when they want to create new music, musicians take that piece of paper and dutifully execute it line-by-line. What I meant is that their brain consists of algorithms trained on existing music that get executed in the process of making "new" music. This is not a particularly meaningful statement in my opinion, but it's used to counter the idea that using algorithms based on existing music is somehow an inferior or less original approach to what humans are doing. Humans are probably using somewhat different algorithms, but my guess is that they're still extremely heavily influenced by their analysis of and experience with other music.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-17 09:56:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> Mostly I'd say it entails setting your arrows directly on AGI, as opposed to taking currently successful methods and improving them incrementally in a direction that affords it. Philosophy can be a part of that if it informs more practical considerations of how to design, build, train or evaluate (safe) AGI.

You can see what the research looks like in the [Journal of AGI](https://content.sciendo.com/view/journals/jagi/jagi-overview.xml) or proceedings from the annual [AGI conferences](http://www.agi-conf.org/) (I think all papers up until 2017 are freely available). There are a few more links on [getting started with AGI](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on /r/artificial's wiki. I'll note that in this context it refers mostly to research done in the [AGI Society](http://www.agi-society.org/) / research community; if others are explicitly and directly working towards AGI using different approaches, I'd say that's "AGI research" as well, but I don't have any more information about that.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-17 09:35:03 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know the details, but open source doesn't have to be more "hackable". It does seem like Wit has a sharing philosophy, but you can also make a "private app" which only shares data with the developers that you want. I don't know anything more than what's on their website though. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-17 00:52:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I’m a military robot and genetics expert

Interesting combination.

You are right that concerns about existential risk from ASI are mostly "born of theoretical thinking" rather than "years of experience". There is no other way, since ASI doesn't exist yet, and when you say you have experience with "such machines or software" I assume you mean with the kind of AI and robotics that have basically nothing to do with AGI/ASI.

Existential risk concerns come from fairly simple extrapolations from definitions of intelligence / cognitive capabilities, paired with different assumptions to explore different scenarios. Most of the problems won't occur or be as clear with narrow AI, although we can already see problems with impedance mismatches, misspecified objective functions and  other issues articulated in Concrete Problems on AI Safety (Amodei et al). I think AI/robotics experience could help by tackling these problems, but it obviously requires acknowledging that they exist first. The same probably goed for experience with other existential risks.

 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-16 20:44:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Sorry to say, but human dictators using AI even near that level, would be much more dangerous than the AI itself.

I 100% prefer your average dictator with value-aligned ASI to a paperclip maximizer. Mostly because they'd still have some kind of human values, and they would value the happiness of at least some humans.

> A superior AI might see us as we see flies. I for one am not in the habit of spending my days hunting flies.

Or it might see us like we saw other hominids, which we all drove to extinction (presumably because they were our competitors). Or like the southern/western black rhino which (partially) consisted of material we found a "better" use for. Or like any of the animals whose territory or food source we took away. Or animals we're driving to extinction by fucking with the climate.

And even if we're not like any of the many species driven to extinction by humanity, we might be like one of the species whose life and welfare is pretty much at the mercy of humans (a.k.a. all species).

> This idea from “I, robot” is kind of naive because it assumes that no human would have taken power long before that.

Ideas of existential risks stemming from AGI are (also) coming from expert professionals in the fields of AI and existential risk, among others, and they make no such assumption. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-16 20:28:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> I must say that I have a lot of trouble following your trains of thought, but many responsible experts are indeed concerned about the existential risks that artificial general intelligence (AGI) might pose, especially if it manages to quickly evolve into artificial superintelligence (ASI). /r/ControlProblem is dedicated to this issue, and has some interesting reading on its side bar and wiki.

The term "strong AI" is in my experience typically used to refer to AI that's sentient, and sometimes as a synonym for AGI, but your use here seems a bit non-standard. The idea that adding "rules" to it would immediately make it WAI makes me think that your notion of SAI is not self-consistent, because every AI is built on many (programming) rules. However, you're right that bringing extremely high levels of intelligence to bear on achieving a misguided (human-provided) goal is indeed quite dangerous.

If someone figures out how to make safe and/or value-aligned AGI, but they lack the computational resources to actually run it, how can they get access to the necessary hardware without compromise, theft, and corruption? Is that a good paraphrase of your question? First, I'd say that I'm somewhat skeptical that someone without the ability to run, evaluate and improve their idea will figure this out. Second, I'd say that this is indeed a dilemma, and I suppose I would think about asking for help with people or groups that I found trustworthy and responsible.

I would absolutely not disclose it to the general public. This would mean that whoever happens to have the most hardware and the most ambition will benefit the most, and I don't think that criterion is optimized for ethics and safety. A more "democratic" scenario might be that everybody gets AGIs of (very) roughly the same intelligence, but the outcome of such a multipolar scenario is also highly unpredictable. I would at the very least study it extensively before deciding to let the cat out of the bag. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-16 20:06:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> Maybe https://wit.ai/ </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-15 10:16:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> I do research into artificial general intelligence (AGI), so of course the idea that we should focus on more general methods resonates with me. I definitely wish (even) more people would work on (safe) AGI.

However, one thing to acknowledge is that this is not what everybody wants to do. The short term matters. I think it's interesting (and not wrong) that Sutton mentions Deep Blue as a more general method which relies on search rather than human knowledge, because it's also filled to the brim with domain-specific heuristics, unlike e.g. AlphaZero. Researchers in 1997 could have eschewed using these domain-specific methods, and *maybe* slightly sped up development of the more general AlphaZero, but they would still have to wait 2 decades for Moore's law to actually make it feasible and testable. Now, I think the utility of having a superhuman-level chessbot that's not even available to the public is rather limited, but in cases of more useful applications, using less-general methods to get them 2 decades earlier is nothing to sneeze at.

Don't get me wrong, when you're aiming for AGI, I'm all for using the most general method you can get to work. But another thing to consider is that this is very difficult to do if the compute (and in many cases data) is not actually available yet. Designing AlphaGo at a high level is not super hard: it's just neural networks + MCTS. Getting it to actually work is much harder, especially if you don't have the necessary compute to run and test it. And then what? Are other groups supposed to build on your unproven design? Actually, this sounds a lot like how AGI is practiced in my research community, so I don't really mind it, but I also note that not a lot of currently useful applications are coming out of it. What we do have is a lot of AGI designs that honestly sound pretty reasonable to me -- it's hard to see where the flaws are -- but that remain unproven.

Finally, I will note that using less-general methods inside of more general ones may not be a bad way to scaffold future progress. Both Deep Blue and AlphaGo are (roughly speaking) tree search + heuristics. AlphaGo could not have worked in 1997, but the idea of tree search was proven in part because of the possibility to fill a gap with human knowledge. Then AlphaGo came along and figured "maybe we can generalize this and use these heuristics from data" (and use a different kind of tree search), which proved the feasibility of its approach. And then AlphaGo Zero came, and did away with the need to learn from examples of human games. Using the less-general methods first here does not really seem like a waste of time, but was probably very useful to create something that could actually be run, tested, debugged, improved, and then built on to create an even more general version.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-15 08:42:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> I agree AI isn't up to the level of Freddie Mercury and Steven Tyler. Neither are most people. Hell, neither are most *musicians*. Also, most songs by these gentlemen are not "Bohemian Rhapsody" and "Dream On". I think the time to pay attention to, and be impressed by, AI comes well before they match our finest musicians' finest moments.

> all [computers] do is analyze existing music and then use algorithms (and some randomness) to generate "new" music based on known musical rules and styles

I'm pretty sure that's what humans do too. Humans are (currently) just better at it. *Maybe* on very rare occasions they "invent" new musical rules and styles. I don't think this is *beyond* AI; it's just that this rarely sounds good to many people, whether (experimental) musicians or AIs are doing it.

But I suspect composing good, original music is just a very hard task. AI has already shown that it can write poetry, make paintings, create video games (levels), and do many other things. It can even outperform humans in designing e.g. car parts with certain desirable properties.

So can machines create? Yes. Are they good at creating original, pleasant music? Not yet. Are they also created (and often controlled) themselves, making us attribute any "creativity" to the human creators/controllers? Also yes, I think. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-15 01:08:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> OpenAI recently tried to start a conversation with the AI research community on responsible disclosure, so I've been thinking about this, but I don't have an answer. On the one hand, I feel like scientists' primary duty is reporting the truth as they find it, but I do also think we have a responsibility to consider and mitigate potential negative impacts. I think this kind of thing has been considered a lot in some other fields, like those dealing with dangerous viruses or nuclear/chemical/biological weapons (or materials that could be used for that).

I would personally say that you should never outright lie as a scientist, and you need to follow proper scientific procedures. In your scenario I lean towards B, because you're still honestly reporting your results. It's also strictly superior to A, because you add information, which you should arguably do anyway, and prevent some of the harms by providing guidance on how to interpret your one, single study. Actually, I kind of feel like you should maybe do this anyway, even if you like your result more. C & D sound like cherry-picking and/or p-hacking, if I understand correctly, so I'd say that's definitely a breach of your mandate as a scientist. I guess E is as well, although I would think there is less damage.

However, I'm not sure how generalizable my answer is to other situations. In this case, the knowledge you publish doesn't directly harm anyone. That still requires 1) you being wrong, 2) a bad actor to cherry-pick the research literature, and 3) the establishment to fail to do their due diligence and fall for this transparent trick. Basically, I would lay most of the responsibility on the bad actor, and more importantly the establishment/courts who should do their best to understand the entire literature on the subject.

I think it might be different if e.g. the outcomes of your research cause bad outcomes more directly. For instance, if you publish how to make a bomb or a plastic gun or whatever, then bad actors can use this directly. The bad outcome doesn't require that you're wrong (in fact, it requires that you're correct), and the bad actor doesn't have to go through another responsible entity to get approval for this: they can just do it directly and kill people.

I'm not saying knowledge in the previous paragraph should never be published, but I do think it's more dangerous. I would consider if it could be published with a different focus, and perhaps in a way that's only easily accessible by other researchers. Maybe it can be published partially, so that the research community gets as much as possible the benefits of the knowledge, while mitigating the likelihood of bad outcomes (or at least your responsibility in them).

I also think science may need stronger norms on this. Not just on responsible disclosure, but also on violating scientific protocol. As you mention, D would probably be best for your career, which I consider to be a perverse incentive. I think that now most psychologists will acknowledge the replication crisis in their field (and scientists in other fields as well), but there is also a tendency among some to vilify the people who "expose" bad studies, and I think that's quite bad. I understand that our understanding of how to do good science always evolves, and I wouldn't want to punish scientists too hard if they didn't follow protocols in the past that we now consider mandatory, but if it comes out somehow that you intentionally chose option D to further your career, I think we need to think of some way to disincentivize that.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-14 17:04:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> The Turing Tapes </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-14 14:20:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> Oh, sorry, I guess I'm the one who's thoughts are still on GPT-2 and OpenAI...

As far as I can tell they didn't do anything new, and I think you're completely right. I can't find an academic publication on this, and I think this whole thing was just done to get the media's attention (ostensibly to teach them something about AI).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-14 13:43:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> What part of my post are you replying to? It sounds like you're talking about GPT-2 and OpenAI, but I didn't mention either in my post. Maybe you intended your reply to go elsewhere? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-14 02:51:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> This may be nitpicking your example, but this is [Norman's official website](http://norman-ai.mit.edu/). Before we blame *journalists* for anything, I would look at what the *researchers* have done here. They are the ones who named their AI "Norman", which isn't even a "clever" acronym for something more obviously machine-y, but an actual human character. The picture the BBC used is also straight from the website. Some more choice quotes: "World's first psychopath AI", "Artificial Intelligence is Born", "Shelley: the world's first collaborative AI Horror Writer", "Norman is born ...",  "Norman suffered from extended exposure to the darkest corners of Reddit", "What does AI see", and so on.

I think this example is especially egregious, but e.g. giving human names to AI systems is fairly common, and so is giving them human(oid) avatars/faces and voices. I think researchers have a responsibility here, but I also think anthropomorphism can be appropriate and useful. Just look at all of the anthropomorhic language we use as professionals: agents, observations, perceptions, actions, behaviors, belief-desire-intention frameworks, artificial emotions, attention, apprenticeship learning, etc., etc.

I think these things are *fine* and they just represent a natural way of talking about AI. I also think we should allow journalists a *little* leeway in making analogies to humans, because it can result in more understandable language for their audiences. The main problem seems to occur when anthropomorphism is taken too far, but I wonder if this problem is primary or a result of "AGI-ification".

> I remember having several discussions in the wake of the Facebook experiment where researchers had AIs communicate, and saw they developed a communication standard unreadable by humans. Based on the articles that circulated afterwards, a significant number of people concluded "they had to turn it off because they were on the verge of SKYNET".

As an example, I think your first sentence is quite anthropomorphic (somewhat obscured by saying "communicate" instead of "talk" and "communication standard" instead of "language"), and it's fine. The problem is with the Skynet conclusion, which is arguably not that anthropomorphic (i.e. it is clearly not human but a network of computers).

Like I said, the main problem seems to be with "AGI-ification", which I think is due to a mismatch into how laymen and professionals use the term "AI". Very roughly speaking, professionals tend to think it means "narrow AI", while laymen tend to think it means "AGI". Or at the very least, they don't appreciate the gap between those two.

I think increasing awareness might be a first step, on both sides. Educating the public may be difficult, because it's full of "casuals" who won't be reached. Perhaps changing things on the professional side would work better: acknowledge that when you say "AI", lots of people hear "AGI" (or "proto-AGI" or whatever), so either refrain from using that term or explicitly address and point out the difference. If the goal is to avoid misinformation, be explicit in interviews about the limitations of your model and how it (probably) has absolutely nothing to do with AGI. Also maybe point out explicitly that it is not at all like a human, and that journalists should not try to extrapolate further human (or AGI-like) properties from the anthropomorphic language you may have used: yes, those "AIs" may have "invented" their own "language" in some sense, but this does not mean they are basically Tolkien.

Unfortunately, I'm not sure the goal is always to avoid misinformation. Incentives for both researchers and journalists may be different, and this is something we may need to look at. Should researchers/journalists be sanctioned in some way for spreading misinformation? That also seems quite hard (and harsh), especially since we may not agree on what is misinformation (e.g. when we'll have AGI, what dangers that may pose, whether it makes sense to work on that yet, etc.).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-14 01:38:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> I would think that one of the defining features of a chatbot is that you can input arbitrary text. If I understand correctly what you made, that sounds like a [dialogue tree](https://en.wikipedia.org/wiki/Dialogue_tree) (or conversation tree).

I would also like to ask you what is your definition of AI? I find that there are so many different definitions that it's kind of pointless to "guard" the term from misuse. I recommend just accepting how people use it and roll with it. Or just don't mention it. You can just talk about your chatbot / dialog tree without ever bringing up AI if you don't want to. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-13 11:44:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> > It seems like OpenAI isn't even trying to do AI safety research

I should have responded to this yesterday, but I don't think this is entirely true. I don't really blame you for thinking this thought because 1) it also seems to me like they're putting more effort into capability research, 2) it's inherently flashier and easier to publish, and 3) as presumably a ML researcher it's more likely that work on ML/AI capability crosses your radar than work on A(G)I safety.

The most direct work on AI safety is probably AI safety via Debate, deep RL from human preferences, the report on malicious work, and the paper on concrete problems in AI safety. That last paper targets problems that we already have, which will also need to be solved for safe AGI. And then they have a bunch of papers that aim to tackle such concrete problems.

All in all, this is not that much, but I think you can also see their safety focus in their choice of *what* capability research to work on. They have a lot of work on cooperation (e.g. they say it's why they chose DOTA), learning from humans/imitation, generalization and exploration. Now, maybe you're thinking: a lot of other people are researching those topics too, does that mean they're also doing AI safety research? To which I'd say *maybe*, but they might also just be doing so inadvertently.

They also have more "community outreach" efforts. One example might be Gym, which can of course help researchers around the world make more capable RL algorithms, but it also works to standardize that research to some degree. It makes it easier to see what is already there, (encourage people to) evaluate algorithms on environments created for AI safety, and it potentially makes it easier to develop safety-enhancing tools (if everyone already has the same API). A different example is OpenAI's recent effort to try to get the AI/ML research community to think about responsible disclosure.

They also employ Paul Christiano (and apparently an entire Safety team), who writes a lot of blogs/sequences about AI safety. He was already doing that before he went to OpenAI, and I don't know if he's doing it in his free time, but if OpenAI is paying him for it, they should probably get some of the credit.

I agree that the focus still seems to be more on capability research (i.e. what you call "proto-AGI" projects) and I would like to see more safety research. But I just wanted to set the record straight on what they *have* been doing. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-13 00:09:39 </DATE>
<INFO> reddit post </INFO>
<TEXT> Yesterday someone asked about an [AI helping us see back in time](https://www.reddit.com/r/artificial/comments/aztljq/could_ai_help_see_back_in_time/), and I think this is just a specific case of that, so the answers might help.

The first question is probably whether the data is sufficient, even in theory (with unlimited intelligence). The second is whether (or when) it's possible to create a sufficiently powerful AI.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 21:00:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> Maybe on a history of non-profit and open source. They have built an image that many didn't buy, but some did (as JhanicManifold mentioned). Among the believers, recent moves may have made some more skeptical, but others will likely remain convinced of OpenAI's good intentions and lofty mission. And some might even approve of the recent moves in light of that mission. I also think that the future will hold many opportunities for them to show that they have stayed committed to their mission.

I also think there aren't that many teams that are working on (safe) AGI on the scale of OpenAI (and especially OpenAI LP if they get all of their money). I mean, I know a lot of AGI teams, but they're all tiny. Only DeepMind seems somewhat similar (and a lot bigger). Behemoths like Facebook and Google Brain don't seem as focused on AGI (yet?). At least not openly. What other teams were you thinking of? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 19:15:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> Not that I know of, but the announcement says this:

> We’ll need to invest billions of dollars in upcoming years into large-scale cloud compute, attracting and retaining talented people, and building AI supercomputers.

Their initial endowment was a pledge of **one** billion (which is perhaps [not as good](https://www.reddit.com/r/ControlProblem/comments/62qtsu/open_philanthropy_project_allocates_30mm/dfp2k48/) as an actual billion), and I don't know if they have added much to that, so they probably never had the billion**s** (**plural**) that they say they need here.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 18:11:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> Right, so the disconnect is that you cannot simultaneously 1) have (a lot) less kids, and 2) have the GND which requires growing amounts of kids. There's no synergy, because these plans don't even go together. The way you say it, there might be synergy between the GND and the *opposite* plan to have more kids: more kids make GND possible, and GND makes life better for those kids.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 17:55:50 </DATE>
<INFO> reddit post </INFO>
<TEXT> Apparently the move to for-profit has been in the works for 2 years, so I don't think it's a response to them not being 100% open about something a few weeks ago. It could still be the case that this is a general feeling they've had, of course, but that would be very unintuitive to me.

For one thing, I would think that moving towards a for-profit model is more likely to piss off donors. I mean, one reason to donate is probably explicitly that OpenAI was a non-profit that was not beholden to stakeholders, allowing them to focus on the greater good, and another reason might be that as a result there are not as much alternatives for income.

Furthermore, I would think that donors were properly informed on OpenAI's mission to develop safe AGI for everybody, and seeing the company start a conversation on responsible disclosure by holding back a perhaps potentially dangerous technology seems fully in line with that mission. If some donors thought they were instead donating to a "regular" (not safety-first) AI company that would simply open-source everything, I think they are misinformed. I also think that as long as OpenAI feels like they're doing the right thing, they have no real reason to feel guilty or like they're betraying their donors.

Finally, even if they felt guilty or somehow beholden to donors, that's a soft power. It feels weird to trade this (apparently problematic) soft power for the hard power of stakeholders in the for-profit. I also suspect that while keeping things a secret is par-for-the-course for most for-profit companies, doing so in the future will reflect even worse on OpenAI. While previously people might have *some* belief that they did it for the greater good, *now* everybody will just accuse them of sacrificing this supposed greater good for profit chasing.

> So in one sense this would let them be more autonomous, and that *could* be a good thing.

If that's the case, I agree it would certainly be a good thing. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 17:20:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> > The most charitable way that I can interpret their recent actions is by considering that they might not be transmitting their message to that many targets.

It's possible, but that does sound a little weird to me. Their message about responsible disclosure surrounding GPT-2 was aimed at the whole research community, and their name was chosen on the idea that they want to democratize A(G)I. One of their original claims to fame was OpenAI Gym (and later the short-lived Universe), which allowed everyone to collaborate better on a shared API and set of environments/agents.

What you say is not impossible. My own charitable take was that they simply felt like they need more money for their mission, that this is the best way to get it, and that it's worth any (temporary?) backlash from the wider AI/ML community. This is not that dissimilar to yours, because in mine they also consider the community less important than going for-profit.

But I really hope that it's not because they feel like they're not "transmitting their message to that many targets" anyway, but that they felt this move was *so* important that it outweighed the *also* extremely important concern of engaging the community and that consider this a temporary setback/sacrifice that they will work hard to fix. Because I think the strategy you outline will pretty much only work if OpenAI is indeed the first to develop AGI, and even with the increased funds, OpenAI is dwarfed by the larger worldwide AI/ML research community as well as a number of competitors that are even larger. Convincing others of the dangers of A(G)I and the importance of it's Safety is crucial in the very likely case that someone else will develop AGI first. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 16:03:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> Are you sure you're not being unfair to [Whitespace](https://en.wikipedia.org/wiki/Whitespace_(programming_language\))? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 16:01:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> The cap is 100 times investment.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 16:00:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> Apparently it has been in the works for 2 years. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 14:13:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> > they paid around 10 million for him

They paid around 4 million for him. At the time, I could imagine that a clause of 25 million seemed reasonable.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 13:12:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> Greg Brockman said "if we are successful at the mission [i.e. creating safe AGI], we'll create orders of magnitude more value than any company has to date". Maybe that lures in some investors? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 13:09:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> I want to preface this by saying that I think Bostrom and Yudkowsky are actually real experts of A(G)I Safety, because expertise is acquired by actually working on a problem. In fact, they are basically the (sub)field's founders and we owe them a great debt of gratitude. However, while I feel like this is slowly changing, their message is extremely controversial among people who have dedicated their career to AI, and I think many are (or were) looking for reasons to dismiss them.

A common criticism is (or at least used to be) that they are not really AI experts. Bostrom is primarily a philosopher, and Yudkowsky is self-taught. Neither has ever contributed anything to the field of AI (outside of their controversial work on safety, which is what's called into question here). Basically it's credentialism, arguments from (questioning of) authority, and (technically) ad hominem, which plays really well with people who already agree with you.

This can be expanded by saying that Bostrom is a professional fear mongerer (because most of his work is on existential risks) and a religious nut (I think he's an atheist, but he posited the [simulation argument](https://www.simulation-argument.com/) which people confuse with the simulation hypothesis, which some think is a crazy/religious idea), and I've seen the idea of anthropic bias also be ridiculed, although it's not commonly brought up. Yudkowsky is sometimes seen as a self-appointed genius blow-hard with a tendency to make up his own jargon, ignoring existing work, arrogantly declaring that he solved parts of philosophy (e.g. quantum mechanics), while his main claim to fame is some pretentious Harry Potter fan-fiction. Both may also just be in it for the money, because Bostrom wrote a bestselling book on Superintelligence and Yudkowsky asks for donations on behalf of MIRI, which is a non-profit (is that the same as a charity?).

Sometimes (often?) people go a little bit further with saying something like "if they'd spend some time actually researching AI/ML/<my field>, they would realize *XYZ*", where *XYZ* is typically something that's false, irrelevant or doesn't really contradict anything Bostrom/Yudkowsky has said. Common examples are that this is not how AI works (implied: *currently*), or that getting an AI system to do anything is super hard so self-improvement cannot be quick or AGI is still very far away.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 11:24:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> > /r/machinelearning's opinion on the change is pretty clear, but they also seem pretty biased against anyone affiliated with AI safety in the AGI sense

I agree with that. I think /r/machinelearning is good for technical discussions of recent papers, but whenever I see them veer even slightly outside of that, it's not a good look.

> has anyone outside of OpenAI more in line with the idea that AGI is a pressing concern issued opinions on this?

I'm a nobody, but I posted [my opinion](https://www.reddit.com/r/ControlProblem/comments/b05ag9/openai_creates_new_forprofit_company_openai_lp_to/) on /r/ControlProblem. I fear most for the reputational damage to OpenAI and AI safety advocacy in general, and I'm afraid people like the ones on /r/ML feel vindicated in their skepticism of everything OpenAI (and other safety advocates) have ever said about responsible development of A(G)I. It's really too bad, because I think there was a segment of AI/ML researchers to whom OpenAI was the only reputable group who was advocating this stuff. I think a lot of people are basically looking for excuses to dismiss all of these ideas: the excuse that MIRI/FHI etc. are not "real" AI/ML researchers still won't work on OpenAI, but now the excuse will just be that everything is a "marketing ploy" or "fear mongering as a business strategy" (even more so than before).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 11:11:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> Definitely! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 11:11:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Nico got a minimum clausule in his contract around 25 milion.

Nice to know, although that still seems on the low side.

> They better give him a new contract asap tho.

Agreed!
 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 11:11:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> >  Ajax would not sell at 8million.

I would sure hope that's the case, but the literal quote from the article is "Ajax are willing to sell the 26-year-old for around £8million in the summer".  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 10:48:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> I also saw it repeated in a number of other outlets, and just linked this one because it seems to be the source. But you're probably right that it's not reputable, and the money quote ("Ajax are willing to sell the 26-year-old for around £8million") does not appear to be sourced. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 10:46:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> The article says "Ajax are willing to sell the 26-year-old for around £8million in the summer", so that would be the asking price. Given that the asking price is usually higher than the (later agreed upon) value, that implies his value/worth would be even lower than £8million, which seems insane.  </TEXT>
</WRITING>
<WRITING>
<TITLE> Tagliafico only worth £8 million? </TITLE>
<DATE> 2019-03-12 10:13:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> I saw this [DailyMail article](https://www.dailymail.co.uk/sport/football/article-6789633/Arsenal-hunt-Ajaxs-Tagliafico-despite-competition-Atletico.html) on how Arsenal and Atletico Madrid are in competition for attracting Tagliafico. I'm not surprised by that, but I am surprised by the quote that "Ajax are willing to sell the 26-year-old for around £8million in the summer". Maybe I'm spoiled with all the transfer amounts thrown around lately, but this seems just *insanely* low to me.

*Maybe* this is a normal price for other clubs to consider paying for him (although I doubt it), but I feel it would be crazy for Ajax to let him go for that amount. They're already going to make a ton of money from this CL season and selling other players, and Tagliafico seems quite important to this team. More important I would say than e.g. David Neres, who Ajax didn't want to sell for €43 million (although I realize he's younger and perhaps more promising because of that).

What do you think? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 09:42:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> I do still believe in the good intentions of the people in charge of OpenAI, but I'm quite concerned about this. Both because I'm afraid the for-profit nature may change incentives for the company, and because of the reputational damage this will do.

I think OpenAI got a lot of goodwill from being open and non-profit, but in the eyes of many they have gotten rid of both characteristics in the last month. People were already accusing OpenAI's actions of being "marketing ploys" and "fear mongering as a business strategy", but I feel that to some degree their non-profit nature and singular focus contradicted that. Even for AI risk denialists this could strengthen the hypothesis that OpenAI were at least true believers, and given their capability research output they could not be dismissed as non-experts (like they do with e.g. Bostrom and Yudkowsky).

Furthermore, the fact that this has been in the works for 2 years kind of taints everything OpenAI did in that period, and perhaps even brings up the question whether this was the plan from day one. Every benefit-of-the-doubt that their non-profit nature afforded them goes away with this, and to many is now just evidence of their dishonesty.

I still (naively?) have some faith in OpenAI that they will indeed stick by their mission. I hope it will be minimally diverted by a drive for profits, and that the extra money to buy compute and talent outweighs damaging their ability (and the entire AI safety community's ability I fear) to convince people that AGI safety is indeed an important problem that deserves our attention.  </TEXT>
</WRITING>
<WRITING>
<TITLE> OpenAI creates new for-profit company "OpenAI LP" (to be referred to as "OpenAI") and moves most employees there to rapidly increase investments in compute and talent </TITLE>
<DATE> 2019-03-12 09:21:40 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-12 09:14:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-11 17:46:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> AI is good at pattern recognition and "prediction". "Prediction" in common parlance tends to refer to the future, but this doesn't have to be the case with AI. It would be totally valid 1) to "predict" the winner of the 2016 US election based on data from the years before, or 2) to "predict" that it rained this morning because the grass is wet (unless there's another explanation). #1 is still predicting forward in time, but starting from another time in the past, and #2 is predicting backwards. You could even combine them.

So AI could (try to) predict the past, and it makes total sense to try to do so. Of course, there are limitations. For one thing, it depends on the quality and quantity of your data and your algorithms. (Agent-based) simulations don't always need a lot of data, but you still need a starting point and to be able to validate them.

Now, forward predictions are typically not perfect because you don't know everything: you might miss some assumptions, context, rules, or details, or you may be flat-out wrong about some. With backward predictions you have those same problems, but you couldn't even make perfect "predictions" of the past in a deterministic universe with full knowledge of the rules and the current state. The reason for that is that multiple hypothetical previous states might have resulted in this one: e.g. the grass might be wet because it rained *or* because the sprinklers turned on (or some other reason). Basically, you can use deduction to reason into the future, but have to use abductive reasoning for the past. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-11 17:22:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> I prefer a 1000 words explaining exactly what the author expects, and what the justifications for that are.

I mean, the picture is nicely drawn, but it doesn't tell me much. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-11 17:15:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Here are the few career options

Where? You seem to be missing a link. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-10 09:23:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are quite a few (unproven) "theories" out there. See [here](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) for a starting point.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-10 08:38:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> There's some general advice on this in the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) of /r/artificial's wiki.

Basically, you can try looking for degrees in AI/ML. Perhaps "1 step away" are Data Science and Computer Science. Mathematics is also worth considering, because it seems that you can never know enough. CogSci is very interesting, but not really useful unless you want to get into artificial general intelligence, and even then it's not necessarily *more* useful than the others. It's not very helpful if you just want to do machine learning as it's currently done, or develop intelligent applications.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-09 11:24:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think that what it comes down to is people's beliefs about philosophy of mind, and if you want to know more I suggest reading up on that. Theories like physicalism (everything is or arises from the physical), functionalism (mental states are constituted by their functional role / causal relations with other mental states, inputs and outputs) and computationalism (the mind is an information processing system) are quite popular, and can lead to the concept of multiple realizability which is the idea that the same mental state could arise from different physical states. This is an oversimplification, but as I understand it, these are all somewhat intertwined, and can lead to the idea that "strong AI" is possible.

If you believe in some kind of dualism (e.g. the soul), that might mean machines can't have phenomenal consciousness, and if you think the mind is doing hypercomputations or not computations at all, then human-level intelligence might not be computable either. There are some arguments for all of these, but it's basically all conjecture and well... philosophy. It's my strong impression that most scientists believe the stuff in the first paragraph more than the second though.

In the theory of computation you also have things like the Church-Turing thesis and the Church-Turing-Deutsch principle which conjecture that anything physical can be simulated by a universal computation device. I think it's actually proven for Newtonian physics (which is of course wrong), but it also seems to be fairly widely believed for physics in general. If this is true, you could just simulate all the physics of a human brain and get intelligence + consciousness if you believe those things arise from the physics of the brain.

Also, for intelligence in particular, roughly defined as problem solving ability, we already have machines that can outperform human intelligence in many ways. Plus, if you apply some of the ideas from above, it seems that any problem can be solved with computation, and this can at the very least be much faster than humans (neurons communicate really slowly). Finally, there is the idea that it's quite unlikely that humans are somehow at the theoretical limit of intelligence, so it seems that other entities (like machines) should be able to go beyond us. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-09 10:12:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> I looked around their website for three seconds and I listened to some of the demos. I think that the voices themselves sound very human, and the Trump and Obama voices sound quite close to their real voices. *But* it's all very monotonous.

I can't quite find what the inputs to the software should be. If it's just text, I imagine it becomes *very* hard to do better. Intuitively I might say that you need to *understand* the text to be able to pronounce it like a human would. Such intuitions have been proven wrong time and again, but I do think getting this right is somewhat of a different task, so I could imagine that making incremental improvements to the algorithm they have won't help too much, which could explain the lack of progress.

Now, they do say that you can do something with intonations. Apparently you can mark them in the text or something. I get the feeling that they didn't do this with most demos. The "different intonations" versions of Max and Jane do have that, and I think especially Jane already sounds a bit better. But I'm wondering if they marked the whole segment with the same intonation or something like that. Maybe real intonations (and tempo and volume and ???) vary continuously throughout a sentence, and annotating that would be incredibly difficult (and likely impossible with their current software).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-08 16:56:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think these are good points. I still think professors should be able to assign their own textbooks, but I think it may genuinely be a relatively unique situation because it is so extremely probable that they would do this regardless of the money.

I do also *kind of* think I should be able to hire my brother if he's genuinely the best man for the job, but in that case I also have to acknowledge the very real possibility that my judgment is clouded. In such a case people might also ask "out of all the N candidates, what are the odds that *your brother* just happens to be the best one"? And unless we have other information, those odds are pretty much just 1/N. But if my brother is some kind of super rockstar in the field, I think this skepticism would already be a lot less, and people would lean more towards thanking me for "getting him" for the company. In the case of the professor's book, it's like he has literally designed the perfect candidate for the job. It would be weird if that person didn't get hired.

> In the case of a teacher that wrote a manual probably there should be a department recommendation for them to follow.

I think the department recommendation would pretty much always be "Yes, of course use the book that is literally the perfect fit for your class, why are you even bothering us with this? Do you think we have time to go out and find the best book for your course?".

> There's still some tenuous idea that they've somehow influenced their whole department (the same way there's a tenuous idea of corruption if the sister's company is eventually chosen anyway) but anyone who knows anything about intra-departmental politics would probably have a hearthy laugh at the idea.

I guess my experience is very different. There may be cases where the department wants different courses to use the same book or has maximum prizes or something like that, but outside of that the idea that a department would tell a professor they can't use their book for their class seems almost ludicrous to me (unless they're using their Intro to Physics book for Econ 101 or something). I know there are power struggles, but that would be such an overtly hostile vote of no confidence / act of war that I don't see it happening unless the entire department already hates and wants to get rid of that professor.

I don't know, maybe my (not super extensive) experience isn't representative, but in the universities where I've worked the professor is in charge of the courses they teach. I recently started my first postdoc position where I was (fully) responsible for teaching a class, and while I did get recommendations to utilize last year's materials, I was also encouraged by the rest of the department to make the course my own as the previous professor had done. Nobody cares what materials I use or how I structure my classes, as long as the learning objectives are met and the evaluations are okay. I've also helped set up entirely new courses at two different universities, and we had even more freedom there.

From that perspective the idea of not being able to choose what course materials to use seems very weird. However, my aunt works at a university of applied sciences, and she tells me things are very different there, with a lot more involvement and oversight from the department/university. I think that at least for some courses the course -- including the method, materials, tests -- seems to be primary, and they might even rotate the teachers who teach it. Obviously in that case you can't just change the book that's used (not to your own book, but also not to others). But it seems to me that's not really the situation we're discussing here.

> Anyway, I do think there's something weird about insisting on students buying this one book. My professors usually would have sort of "class companions" distributed by the university's press and it was your loss if you didn't want to buy it. I think that approach also avoids the issue here.

I think it depends a bit on the sense in which the book was mandatory. If it's "if you don't read the book / use the software, you will probably fail because you won't know the material", that's probably okay (unless there are exam questions like "what was the name of the dog in the example on page 456 of your statistics book?"). If it's "show me the receipt or fail the class, no matter how well you do on the tests and assignments", that's obviously bad. I would say it's bad even if the professor didn't write the book, but I do think it would increase the appearance of corruption if they did, because it would then be a lot less obvious that they're just making the best choice for teaching the material to the students.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-08 13:05:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your first two paragraphs seem fully in line with what I've been saying. There's the question of whether the professor's decision was actually corrupt (i.e. made because of the money), which we can't know. Although we can't know, we can still attempt to calculate the probability of honesty (or inversely, corruption), which corresponds to "how it looks", "appearance", "suspicious", etc.

I don't know that I've actually said much about the standard of proof here. I think it depends on the situation whether you'd use something like "beyond a reasonable doubt" or "preponderance of evidence" or "more likely than not" or whatever. But I think that the probability that a professor considers their own book the best resource for teaching their class, regardless of money, is extremely high, so it would clear almost any threshold you might choose. (I do think there are clues that might lower this probability, such as not allowing second-hand books if there's no good reason for that.)

I get the feeling that you (and others) are instead picking an extremely low standard of proof that basically amounts to "it's not technically impossible that it was corrupt". Now maybe there are cases where this is appropriate (perhaps a judge presiding over a case), but I don't think that the stakes here warrant that.

> there's definitely an impact elsewhere (I've heard of students who felt they should parrot their teacher because that's what he wanted to hear).

Definitely? I'd say *possibly*. I've also heard of your example, but I think this happens regardless of whether that teacher's book is also used for the class. There are teachers who want you to parrot them, regardless of what book they assigned. And there are teachers who want you to parrot the book, regardless of whether it's theirs. Maybe it's more likely with teachers who've written and then assigned their own book, but I don't know.

I will also say that if I knew my professor had written a book about the class we're taking, but we have to buy someone else's book, I would find that very suspicious as well.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-08 03:03:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> As I mentioned, there's a difference between something *being* corrupt and *looking* corrupt, or as you say *suspicious*. Since we can't really know for sure, we're stuck estimating a probability that decisions were corrupted. If a congressman accepts money from someone, and then suddenly changes their behavior to vote in their favor all the time, I agree this would probably increase your confidence that their decisions are corrupted. But with a professor, the probability that they were going to think their own textbook is the best is already pretty close to 1.

>  a politician could decide to vote a certain way (without telling anyone) and then accept ~~bribes~~ gifts from people sympathetic to that outcome and it wouldn't be corrupt.

If this is indeed what happens, the main problem is the dishonesty towards those sympathetic people, and potentially aiding and abetting the crime of bribery. If he tells them "if you want me to vote your way, you need to bribe me" that's bad. If he tells them, "this is what I stand for, if you agree, please donate to my campaign" that's normal politics.

Part of the reason the analogy doesn't really work is that for the professor there is no briber, there's no dishonesty, there's no crime, etc. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-08 02:52:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> I agree. I don't know if this was already mentioned in the OP, because it was already deleted when I replied to /u/VelveteenAmbush, so I really just replied to their comment. Using your own textbook for a class is not unethical, but requiring that they purchase a new version certainly seems a lot shadier, especially if there was no reason for this.

In this case there does seem to be a reason: they need a license for the associated online software. I think it could still be the case that the professor genuinely feels these resources are the best for teaching the course. However, any professor should also take into account costs of the required materials when selecting them. The article is mainly about how this imposes too large a costs on the students, which is true regardless of who is making money from it.

I do think it calls into question a little bit more whether the professor's judgement was compromised by the money he could make. I argued in other comments that the probability that a professor thinks his own textbook is the best for the class is roughly 1, but if you take into account that it costs a lot more (and apparently the software is glitchy and doesn't add much), it does probably lower that probability a bit.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-08 02:47:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> > But you agree that it wouldn't be corrupt for the congressman to accept the bribe as long as he is confident that he would have voted the same way regardless?

As I mentioned, I think it *is* corrupt, but it's very different from the professor case. I googled "corrupt definition", and these are the results:

> having or showing a willingness to act dishonestly in return for money or personal gain.

> "unscrupulous logging companies assisted by corrupt officials"

> synonyms:	dishonest, dishonourable, unscrupulous, unprincipled, amoral, untrustworthy, underhand, deceitful, double-dealing, disreputable, discreditable, shameful, scandalous;

As I mentioned in my previous reply, a lot of these things still apply to accepting the bribe even if the decisions don't change: it's dishonest, untrustworthy, deceitful, etc. A professor using his own textbook is none of these things. To be clear: for the professor's decision to be corrupt, he would have to make his decision based on the money, but a congressman taking bribes is also dishonest, etc. etc. for other reasons. It's a broken analogy.

> I think your slipperiness on this should give you pause that you really believe your own argument vis a vis the professors.

I'm not sure why you're calling me slippery: with every post you're trying to put words in my mouth. You changed the topic from corruption of professors to corruption and then legality and then corruption again of congressmen taking bribes. I have answered all your questions honestly, which involves stating my uncertainty (e.g. regarding pragmatics of law and complex ethical judgments) which I'm guessing is what you find "slippery". I have also repeatedly tried to get back to the topic at hand, away from this broken analogy, but for some reason you don't seem to want to argue the actual issue. That seems a lot more slippery to me, and I don't appreciate you insinuating I don't believe my own arguments. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-08 02:15:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> Whether it should be *legal* is a complicated question that involves not only ethics, but also pragmatics of law, law enforcement and politics. I do not feel very qualified to answer this, but my feeling is that the shortcuts the actual law uses are likely justified.

I'm also not sure if I would consider it *ethical* for the congressman to accept the bribe even if he knows it won't affect his decisions. He'd be deceiving and/or taking advantage of the briber. The briber is doing something unethical, and he's aiding and abetting in that (criminal) interaction, perhaps normalizing it (on the other hand, maybe it's good to punish the briber...). He's receiving money for doing nothing... And it's breaking the law.

I will note that none of these issues apply to the professor's book, which is what we were discussing and what I wanted to discuss. The bribery was just supposed to be a metaphor, and I think I've showed here that it breaks down completely once you get into these details. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-08 01:23:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you should acknowledge that you don't *know* for sure what their vote would have been and if they're telling the truth. You can try to make some estimation of the probability that they would have voted this way without the bribe, and make your judgement based on that. I will note that this probability for the professor and their own textbook is pretty much 1.

Ethics can be difficult to implement, so the law sometimes takes shortcuts. This might mean that it is designed to be easier to enforce, and e.g. say "taking money as a politician to make a certain decision is illegal regardless of whether this actually changes their decisions (because that would be too hard to establish)". Luckily a professor assigning their own book for a class is nowhere near illegal, and the probability is much easier to estimate, so we don't need to bother with such shortcuts. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-08 01:03:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> The definition of a bribe seems to be something like "if I give you $$$ then, because of that, you will vote like I tell you". If so, that is by definition corrupt. It is also not at all comparable to the professor+book situation.

I changed the conversation to campaign donations, because some people equate them with bribes and consider them similarly immoral. However, the quid pro quo nature is less clear, and they are also totally legal (within limits) and normal.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-08 00:50:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> > It is absolutely still corrupt.

The way I described it, it absolutely isn't. A corrupt decision is one that's made because of non-meritocratic considerations, like getting money for it. There's a difference between something *being* corrupt and *looking* corrupt to people with uncharitable interpretations of your actions. Sometimes it can be really hard to tell the difference, but in this case it is extremely likely that the teacher would choose this book for his class regardless of monetary considerations. The optics may be bad to people who don't understand this, but why should the professor suffer for that?

> "But I would have voted the same way anyway" would never fly for a Congressman taking bribes from a constituent.

I was under the impression that politicians get campaign donations all the time. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-07 22:53:16 </DATE>
<INFO> reddit post </INFO>
<TEXT> Even if they didn't get any money for it, it would probably be their genuine professional opinion that it is the best textbook for the class. In that case using it would not be corrupt. Not making any money would perhaps help people see that, but I don't think professors have a moral obligation to pay off other people's uncharitable interpretations of their actions.

ETA: Writing a textbook (that is typically the perfect resource for your class) is basically already charity work for the majority of professors who do it. I don't have hard data, but apparently most get 5-15% in royalties and sell extremely poorly. (Of course if you're one of the top N books that get used a lot, you'll be raking it in, but that N is pretty small.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-07 22:36:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> I have no idea about American politics, but could it be that they're barring Democratic candidates from participating in debates held by Fox? Or are they already just allowed to go to official DNC debates? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-07 20:43:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> > To make a group coordinated action, you need a consensus. Part of the reason this community exists is because we can debate and discuss calmly *without* a consensus.

I would think there is a consensus on a lot of things. For instance, on the value of (or desire for) calm debate on a lot of issues. People here generally seem to value charitability, analysis and knowledge. There are probably other things.

I agree there is no consensus on whether Democrats or Republicans are preferable, and a whole host of other things. We shouldn't organize this forum to "squash the SJWs/patriarchy" or "fight for/against abortion rights" or anything. But that doesn't appear to be the kind of action /u/Gloster80256 is suggesting. The main suggestion is "general Sequences for individual domains of knowledge", sooo... knowledge sharing. Would you agree we can probably get an 80% consensus on the question of whether people here value knowledge (and that this doesn't mean the community has failed)?

I agree that the individual Sequences might be more controversial, e.g. if people start writing them about culture ware topics. Maybe we can come up with ways to make this optimally valuable (I don't know, this already sounds a bit like LessWrong...). Or given that people here can apparently pull off a coordinated move, perhaps an alternative platform could be designed that can facilitate calm debate of CW topics even better than a subreddit.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-07 15:16:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> Or Ramos... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-07 11:10:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> Is ML research really accelerating, and if so, what is causing it? My guess would be mainly that it is due to more people working on it, with more funding, compute and data. It seems to me that most ML research has very little to do with AGI, but to the degree that it does and is accelerating, we would (tautologically) get AGI faster than if that weren't the case.

If someone invents AGI (i.e. its contemporary feasibility is demonstrated), it's fairly imaginable that even more people and resources would flock to this area of research. It's been a while since I read Superintelligence, but I think Bostrom talks about this in the chapter about intelligence explosion kinetics. However, there's a limit to this: we can't use more than 100% of people and resources.

Furthermore, the amount of research progress you get from these things is very likely sublinear. Doubling the amount of worldwide scientists working on X will typically not get us X twice as fast. And pretty much every learning curve shows that for a given algorithm there are diminishing gains from adding more data and/or compute.

This doesn't mean exponential/accelerating returns are impossible. However, I would think they'd have to come from recursive self-improvement, which does not really seem to be a driver of current ML advances. (Of course, this is a big question mark too.)

Another way that an AGI might quickly improve its intelligence is by making use of hardware/knowledge overhang (also described in Superintelligence). Basically, by making use of more of the hardware and/or knowledge that already exists in the world. However, I would argue that the current trend actually works a bit against this, because it's using a larger and larger proportion of the world's compute (and data, I think).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-06 03:19:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> > While the opinions mentioned in the article are arguably "expert opinions" they aren't actually researchers in AI. Maybe their views of the bigger picture have some value, or maybe they don't. Any thoughts?

Just that I think we should indeed be looking beyond AI researchers when asking questions like these. There is no doubt that AI researchers have valuable opinions, but they don't necessarily have any special insight into economics, business, geopolitics, history of science/technology, etc. With all talking heads, but especially ones with political and business interests, I think it's important to be at least a little bit skeptical of why they say what they're saying. One of the first things I did after reading your quote was check that Prof. Ferguson isn't affiliated with any major companies who might want more lenient legislation on privacy (I couldn't find such links).

On the main topic I don't really feel very qualified to weigh in. I know of arguments for and against the idea that China will soon overtake the US as the AI leader (whatever that means). I would think that China's population advantage will seal the deal eventually, but more complicated things are going on with flow of talent, culture, etc. I think legislation (e.g. regarding privacy) can make it harder to deploy some kinds of AI, but I think that for research it mainly just poses a different set of challenges, and figuring out how to learn more from less data is important for achieving AGI anyway. However, if such legislation has a chilling effect on AI usage, research and investment in industry, this can of course influence a country's performance in the "AI race". So, it's complicated... (although I lean towards the belief that China will probably outperform the US in, say, 2030).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 21:11:22 </DATE>
<INFO> reddit post </INFO>
<TEXT> > How do you think a cop would feel if they shot to wound and the guy with gun gets a shot off in a random direction which goes through a wall and kills a kid in his bedroom.

I would think that if the guy has a gun, then "no longer a significant threat" and "dead" are the same thing.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 21:08:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> I know pretty much all of this, and tried to construct my scenario around this. This is why I mentioned pulling out the gun *before* they moved closer, and I imagined they were advancing slowly and threateningly (after having stated their intent to kill me). I agree this is obviously an example I made up so that I could (apparently not very competently) convey it to you through language. It was meant to illustrate that it's not the case that it's *never* preferable to shoot to wound.

I would have thought that more situations might occur where wounding > killing > not shooting, and that are threatening, not yet high speed, and at a decent distance. However, I am coming to realize that there are so many conjunctions that perhaps these situations are so rare that it might be said that they occur *basically never* (a.k.a. "never" if you're not overly literal-minded like me).

Anyway, thanks for the discussion and your patience! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 20:46:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> > In both cases the shot individual is capable of threatening others, this is why police shoot people an excessive number of times - if someone is a threat you shoot them until they are dead so they don't further hurt the police or bystanders.

I guess I would prefer the policy to be to shoot until you can tell they no longer pose a significant threat. This may be equivalent to death in most(?) situations (so those situations might change), but seems like it's better for the rare(?) situations where there's a difference. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 20:41:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> > There's a fair chance that someone five meters away with a baseball bat can already hit you in the head inside of your reaction time to pull the trigger/hit them -- you definitely don't want them any closer than that.

Okay, then increase the distance a little. Note that in my scenario they're not running or anything.

> If that person is unarmed it's probably not a great idea to shoot them at all unless there's some extenuating circumstance.

I am assuming in this scenario that the guy is physically overpowering enough to likely succeed in his stated threat to kill me. Even if I go to jail for shooting him, that seems like a preferable outcome. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 20:14:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thinking Biden has/had the best shot against Trump and thinking Biden should be the Democrat candidate/champion are somewhat separate things.

If another candidate is strongly preferred to Biden and still has an acceptably strong shot at beating Trump, Democrats might prefer him/her. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 20:10:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> > "never point a gun at something you ~~do not intend~~ **are not prepared** to destroy is one of the corecommandments of firearms handling

This seems like a subtle but important difference, and what e.g. enables police to draw their guns if they merely *intend* to arrest someone. Saying you should be *prepared* to destroy someone seems like the same thing as accepting the responsibility that this might be the outcome. You could aim to wound and still be prepared for the possibility that your victim dies anyway.

> Every gunshot victim you hear from didn't die. This doesn't mean guns are acceptable for non-lethal uses, it means there's a survivorship bias when talking to literal survivors.

I get that. My point was mainly that the human body can be surprisingly resilient, just like it can sometimes be surprisingly fragile. What do you make of the statistic that apparently 67% of gunshot victims survive? I think there are lots of confounders (as I mentioned), but it seems to me that that it's certainly not *necessarily* lethal as you mention in another comment. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 19:57:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks for the clarifications! This (and the other reply) definitely helps me understand the philosophy better.

I would still think that there might be situations where shooting to (partially) incapacitate can be preferable to shooting to kill. You're not always 20m away or, a gun might be the only weapon you have (i.e. no nightstick, mace or taser), and your victim might not have a gun either.

Hypothetical situation: Someone 5m away threatens to kill me with their baseball bat / bare hands. I pull out my gun and tell them not to come closer. They call my bluff and move closer anyway. It seems to me that I could aim a little lower than if I intended to kill them, and lower the probability of that. Having a bullet in their leg (or wherever) seems like it may change both their willingness and ability to go through with their plan to kill me. If it doesn't, I have more bullets.

Obviously this is just one made-up situation. I think there might be others where my preference order might be something like "wound them > kill them > don't shoot". I understand that by shooting I accept the responsibility / liability if they die. I also understand that some risks are involved in aiming away from the center of mass, especially if you're far away and/or don't have time to aim. I don't know how common these situations are compared to situations where you should shoot to kill. But is it your position that it's *always* irresponsible to aim for another outcome or just *usually*?

> If youre in a situation that warrants you shooting a gun, there's no time for a Bayesian or Utilitarian calculation.

There's no time to get your calculator out, but it seems to be presumed here that you have time to contemplate whether the situation warrants killing someone, which you cannot really do without considering alternatives.
 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 19:24:57 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, I guess I can sort of see that. Like I said, if you shoot (in certain conditions) you should accept the responsibility for the probability that you'll kill them. Still, it seems to me that if your preference order is something like "shoot leg > kill > don't shoot", then you should still probably aim for the leg.

> I'm pretty sure the idea is that when you shoot someone, the probability that you can shoot them accurately enough to injure but not kill them is so low that you shouldn't count on it as an option.

Is this actually true though? I know these things can be unpredictable: you can die from a shot to the leg, but there have been people who survive gunshots to the head or chest. The impression I got is that shooting at someone from a distance where you can't very realistically distinguish between aiming at e.g. the legs and the torso/head actually results in a not very high probability of death (mostly because odds of missing are quite high, and you're much more likely than not to survive a gunshot to a random body part, especially with proper care).

I don't really know the statistics here. [This article](https://www.pennmedicine.org/news/news-releases/2014/january/survival-rates-similar-for-gun) says 33% of gunshot victims who arrived at a trauma center died. However, presumably this does not include people who died on the spot, and it might also not include people who were less severely injured. Furthermore, it seems quite likely that most of these people were indeed shot mostly with the intention to kill (as per the apparently accepted "wisdom").  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 18:30:19 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Firearms trainers teach you to never "shoot to wound"

Really!? That seems bad... [Edit after a number of replies: Thanks for the discussion! I understand this policy a lot better now. I'm still tripping a bit over the "never", and I have similar nitpicks downthread, but I can sort of see that this may indeed be a good thing to teach.]

> as that means you are accepting responsibility for the survival of someone youre firing a lethal weapon at.

What does this mean? That I'm responsible for all the bad things this person will do for the rest of their not-ended-by-me life? That doesn't make any sense to me... Or does it just mean that when I'm firing a gun I accept the responsibility that they *might* die? In which case I'd say it's still better to accept responsibility for a 30% chance of death (or whatever) than for a 95% chance.

> Since you cannot guarantee their survival(because it's a fucking gun), "shooting em in the leg" is a necessarily irresponsible act.

But shooting them in the face is more responsible? I really don't understand the notion of responsibility used here.

Furthermore, I can't guarantee your survival no matter what I do. I can't guarantee your survival if I punch you in the face, if I shove you, or if I pat you on the back. But all of those things are (obviously) less likely to kill you than a bullet to the face, and I would say they're much much better things to do (unless death is a desirable outcome).

I agree that if you shoot at somebody, you are accepting responsibility (and liability) for the dramatically increased probability that you will kill them, even if you aim for their leg. That doesn't mean this isn't preferable to an even higher probability if you aim to kill.

To be clear: I'm not following along with any analogy (which I don't understand), so I'm not making any statements about trigger warnings here. This is just a person with zero firearms experience expressing extreme surprise at what is apparently accepted firearm philosophy.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-05 02:41:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> Reading this I get the feeling of the perfect being the enemy of the good. As I mentioned, watermarking fakes isn't a perfect solution, but I think it might be better than nothing. I don't see the fact that any countermeasure will be circumvented by someone, someday, somehow as a great argument against having such a countermeasure.

What do you think of bicycle locks? Futile, they don't prevent every instance of abuse? Because sufficiently dedicated adversaries with the right tools can still steal your bike? Or because it creates a market for bolt cutters? Or are locks pretty useful, because they make stealing your bike significantly harder so that most people won't bother with it?

> So rather then simulating a security that isn't there and never will be, we should start educating the public to be carefull with everything they see or read.

These things don't seem mutually exclusive. However, I would be very reluctant to rely solely on educating people. We've had (other) scams forever, and so far we have not been able to educate people enough to make scamming an unprofitable business.

> we should be adding a clear line of propagation to files ...

Interesting idea. Someone in another thread suggested adding something in the image metadata and I would be concerned that it'd be too easy to remove/change. But you seem to suggest that instead of marking the AI-generated images as such, we mark *all* images everywhere with this, and then we mark images as "suspicious" if this metadata is missing or something seems wrong with it.

One problem with this seems to be that if I understand correctly, you'd have to do this to all images in the world, which sounds a lot harder than just doing it to the images you control. I also wonder how hard such a thing would be to falsify, but I do think I should take my own advice and not dismiss an idea outright because it's not totally foolproof.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-04 17:20:33 </DATE>
<INFO> reddit post </INFO>
<TEXT> He seems to be quite well informed. Do you have any arguments that actually address what he says? </TEXT>
</WRITING>
<WRITING>
<TITLE> [D] Should researchers add watermarks to visual "fakes" generation models and/or data sets? </TITLE>
<DATE> 2019-03-04 17:03:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> With OpenAI's recent attempt to discuss responsible disclosure, I was wondering what researchers could do in this regard. I'm not sure about text generation like with GPT-2, but there are certainly [possible abuses for face generation models](https://venturebeat.com/2019/03/03/why-thispersondoesnotexist-and-its-copycats-need-to-be-restricted/) as well as other deep fakes (e.g. creating impersonation video).

Assuming the research should be published at all (which I think many people would desire very much), what could (easily) be done to mitigate abuses by bad actors? One thing that doesn't seem that hard is to add watermarks to the training images. Presumably a system that can learn to generate complex visual objects like faces should have no problem also learning to add a simple watermark. These watermarks do not necessarily have to be visible to humans; we could build tools (e.g. into browsers) to detect them automatically.

The watermarks could be added to the publicly available data set, so that even if people train the model from scratch they'd end up with the watermark in the output, or as a pre-processing step before an image is actually used (which would presumably be easier to remove from the code). In either case, the released model should have been trained with the watermark.

None of this is foolproof, but the main idea is to limit the pool of actors who'd abuse the system. Many wannabe scammers are only able to use sites like ThisPersonDoesNotExist.com or the pre-trained model, so they would end up with the watermarks. People training their own models would have to have the skill to remove the watermarking code, or to remove the watermarks from the entire data set. Removing simple watermarks from an image isn't hard, but many people may still be unable to do it or simply forget. There are also [ways to make it harder](https://www.theverge.com/2017/8/18/16162108/google-research-algorithm-watermark-removal-photo-protection) and more research can advance this further. It won't be perfect, but it's probably (a lot?) better than nothing.

So what do you think? Is this an easy way to make disclosing visual fakes research (a little) less prone to abuse? Or are there major problems? How would you go about doing this? E.g. put the watermark in the data set you release to the public or keep the data clean and add it as a pre-processing step? Do you have better ideas? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-04 16:09:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> There are different ways to use NVIDIA's research. If NVIDIA was hosting an operational model that everyone can use, they could certainly add each generated face to a repository. I don't think they're doing that, but potentially outlets like ThisPersonDoesNotExist.com could do this if they wanted. It doesn't even have to be a repository: they just have to keep hosting the picture and letting search engines like TinEye index it so that they can be found with reverse image search (although it would be more efficient if they just had to remember the input values to the NN).

If NVIDIA is in control of the used facegen system, then they could of course also add metadata to the images. They could also include this in the code they open sourced, so that by default all clones would have it too. However, I think metadata is always pretty easy to strip off or replace without corrupting the image (I don't think this can be changed).

A better option in my opinion would be to put watermarks on the images. They can even be (mostly) invisible to humans, in which case you could still rely on automated detection. These can of course also be removed, but that would require photoshop skill and manual effort unless someone comes out with a tool that could do it automatically.

Going even further, I think NVIDIA could have watermarked their data set. This would ensure that even people using their open source code or programming their own version would end up with models that apply this watermarking unless they first remove the watermark from the entire data set.

None of these methods are fool proof, but they do increase the amount of effort / skills / resources a scammer would need to benefit from this work. I hope researchers will incorporate some of these measures into future releases. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-04 12:58:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> At the end you thank Hugh Zhang (among other people) who seems to be your colleague at Stanford(?) and is the author of the [Gradient article](https://thegradient.pub/openai-please-open-source-your-language-model/) you mention at the beginning which argues the exact opposite from you. Could you say a bit more about your relation and interactions on this topic? Were you able to change his mind? If not, what were the major sticking points? Thanks! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-04 12:48:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> > But isn't the option between one homie having a megaphone and everyone having a megaphone.

If everyone has a megaphone, you still can't hear each other. I would much rather have 1 person have a megaphone *if that person isn't a jackass*.

This applies to lots of things where a single individual can do a lot of damage. I think this can be true even if some of the "elites" who do have the technology are bad. As long as that number is small, we are probably still looking at a smaller amount of damage than if some percentage of *all* people (i.e. the jackasses), which is a huge absolute number, are abusing it.

I will say that there is certainly also an opportunity cost to consider to *not* releasing technology that can do good (in the hands of the general public). But I think a strategy of thinking about that tradeoff before you decide whether to make your work publicly available is better than not thinking about it and just doing it. It really depends on what the technology is.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-04 04:31:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> I agree, which is why I said "maybe" and put "stupid" in scare quotes. I was kind of responding to some of the other comments, and perhaps trying to meet them half-way (some scams really are stupid). I probably could have worded that a lot better.

I think the rest of my comment is pretty clear about how I do consider aiding scammers to be a problem, and one that we need to take responsibility for as AI professionals.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-04 01:15:38 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think these are good points that highlight some of the downsides of this technology. Yes, maybe the people who fall for scams are "stupid", but that doesn't mean they deserve to get scammed or that we need to make life easier for the criminals who scam them.

The question is what should be done about this. The upsides have to be weighed against the downsides, and we don't want to impede technological progress just because it can be misused: almost any technology can be used for nefarious purposes by bad actors. The ideas posed here seem like a good start: watermarking and indexing public images, and using browser plugins to detect fakes. I think researchers can also take this further: e.g. embed an (invisible) watermark in all training images. If your model is at all decent, it will easily reproduce this trivial aspect of all images and make fakes easier to (automatically) detect. These measures will mostly stop low-effort scammers, but of course it's harder to stop people with the technical skills and resources to train their own model based on the disclosed information (or independent research). Ultimately the world will probably have to adjust to the situation where AI-generated fakes (not just faces) cannot always reliably be distinguished from real, and some would argue this could happen faster if everyone has access to this technology.

Another possible measure against this is legislation requiring labels that something is created by/with AI, but there are also [issues](https://www.eff.org/deeplinks/2018/05/should-ai-always-identify-itself-its-more-complicated-you-might-think) with this (mainly related to free speech).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-03 15:46:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think /u/Wereitas's idea is that the equality question is linked to what everyone *can* do.

Robinson explains a problem with the "equality of opportunity" vs. "equality of outcome" dichotomy from the perspective of people who care about equality of opportunity. Actually, he argues, if you care about equality of opportunity, you should (also) be pushing for equality of outcome, because outcomes affect opportunities. But this is clearly not what the equality-of-opportunity camp is doing.

I think /u/Wereitas's idea is that this seeming contradiction can be resolved by framing the opposition in terms of different conceptions of "can". The equality question is linked to what everyone *can* do, and the two camps are something like "institutional barriers" vs. "personal ability". The institutional barriers camp roughly corresponds to the equality of opportunity camp, but without Robinson's contradiction: they just think there shouldn't be any institutional barriers to submitting a Linux patch, getting some job X, or becoming a billionaire. (Probably a straw man of) the equality of outcome camp corresponds roughly to the idea that it would be fair if everybody can equally submit a Linux patch, get job X and become a billionaire regardless of personal ability.

I think this is kind of shaky, especially the link between equality of outcome and personal ability, but do note that Robinson's contradiction seems to be resolved by replacing "equality of opportunity" with "no institutional barriers".  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-03 15:18:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> While I also find such laws immoral, I don't really think they were created in order to mislead people about what is bad. I don't think lawmakers back then thought "while we know that women and black people are our equals in every way, we want the people to think otherwise, so let's make a law that says only white male land owners can vote". If you look back in history, those people had all kinds of backwards ideas about the cognitive abilities (among other things) of women and black people or about how God intended things (or whatever), so it seems a lot more believable that these laws were were, as I said, created in accordance with the values of the people creating them. Our beliefs and values are different, which is why we find them immoral.

---

But given that the article was about forgery, would you perhaps like to argue why anti-forgery laws are immoral and/or "created to mislead people about what is bad"? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-03 13:50:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your previous reply seemed to be implying that it's misleading to "writ[e] laws on what is legal or not" (or that the people doing that are misleading).

I think there are probably Kantian and rule utilitarian arguments for why breaking the law is immoral in and of itself, but I don't think I subscribe to that. Although I do think the question of "How can we prevent <illegal thing X>?" makes some sense, no matter what X is.

I wouldn't say the law *defines* what is bad. Actually, it's kind of the other way around: laws are supposed to encode the (moral) values of the people making them, or nudge people/society to optimize those values. Of course we don't always succeed, people have different values, and those values might change over time.

But if you want a more direct reason for why we might want to prevent forgery and misleading people: it is because it has a great potential for harm and injustice.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-03 13:32:14 </DATE>
<INFO> reddit post </INFO>
<TEXT> I agree that we have a moral responsibility to sentient entities. The fact that we don't know how to detect sentience outside of ourselves is a bit of a problem, but it seems quite natural to say: "I know I'm sentient, and other humans claim they're sentient too in a similar way and their brains/bodies seem pretty similar as well, so I should probably assume that they are also sentient". Similar arguments can be made about animals (i.e. they're not that different from us), but it's much harder animals (including AI).

However, my main point here was not that we don't need to treat AI ethically if it's sentient, but rather that not giving AI "laws" is not an option (sorry for the 4 nots in that sentence).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-03 13:12:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> Writing laws on what is legal is misleading? Laws *define* what is legal... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-03 11:02:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> > My thinking is that limiting AI to any set of "laws", i.e. Asimov is, in itself, immoral.

Why do you think that's immoral? Maybe you think this is an infringement on the AI's freedom, but why does that matter? Does the AI care about freedom? If that's the case, it must be the consequence of the laws/rules/lines of code used to program it. There is no way to get around this: AIs are programmed. We program their motivational system, we program how they respond to data, and we program how they act in accordance with their motivation. Every line of code is an unbreakable law, more akin to a law of nature than to a law of government.



The eventual behavior does not depend solely on the code: it depends on the data and experience of the AI as well. You can make an AI that learns things. You could perhaps even make an AI that learns some values and then follows them. But you will still have defined the "law" of what is considered a good value, and the "law" of how to make new value hypotheses, etc. This doesn't really seem any more or less [compatible](https://en.wikipedia.org/wiki/Compatibilism) with free will than, say, Asimov's laws.

Finally, I would question our moral obligation to endow AI with free will (if it's possible). Why should we have to do that? Why do we have this obligations to our computers / AI, but not to e. g. a tennis racket? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-03 10:36:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Better question, why does it need to be prevented?

Because it's illegal? Because misleading people is bad?

> Even better question, why should open fear mongering that is no better written than Open-AI's publicity stunt be taken seriously?

No technology is 100% universally positive. Everything can be used in bad ways. The more powerful the technology, the more true this tends to be, and the larger the problem that it poses. We are now (finally) seeing some people and organizations realize this, act like adults and accept that they have some responsibility for the consequences of their work, research, products and services. This naturally results in questions about how the downsides can be mitigated, and it's a conversation we desperately need to have. Even if kickstarting it requires holding back one inconsequential language model for which that may not be entirely necessary. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-01 16:32:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> You can audit Coursera courses for free. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-01 13:19:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> No company or country benefits from an AI apocalypse. I think of it as an organization being willing to spend X amount of time and resources to make their AGI safe(r) before they will switch their AGI on. If they remain unconvinced of the likely dangers, or feel forced by the time pressure of an arms race, X will be lower. Spreading awareness and convincing AI experts that the default outcome is likely very bad may help here, and can be done more-or-less unilaterally. (It may also be possible to reduce arms race dynamics, but this would indeed likely require international coordination and difficult-to-enforce regulations.)

Another way to help is to reduce the needed time and resource investment required to make AGI safe(r). Public research, tools, frameworks and guidelines might help here. I think it would be valuable for more research to crop up in this direction. Of course building AGI to be safe from the ground up is a great and worthy goal, and perhaps the only way to make it truly safe. But if someone beats you to it, then it would be great if there are also easy ways out there for them to make their AGI safer. This may be suboptimal, but sufficient in some scenarios (e.g. we don't know how fast recursive self-improvement might go, and if it's slow enough then a strong enough containment box might be enough to stop catastrophe.)

Finally, if you're the first person to build a value-aligned ASI singleton with a decisive strategic advantage, it could presumably stop other people from making unsafe AGIs.

---

I do agree that international coordination, cooperation and collaboration is very desirable, but these are some things that people/companies could do on their own.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-01 11:52:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know. What is the alternative? It seems that this is a system that will sometimes turn off the saw, which comes in place of "no system". If there are a lot of false positives, that will clearly be annoying, but it doesn't seem more dangerous than having *nothing*.

You seem to be thinking that in this case a human engineer could make a better system, to the point where nothing can be gained from adding an additional ML on top (/u/throwaway-ssc's suggestion). If that's true, then obviously you should use that better solution instead of ML.

Of course, it also depends on the ML algorithm you use. If the task is apparently simple enough that a human engineer can set the thresholds, then you can also just make a similarly simple ML system and have it learn the parameters. You could even check if they seem reasonable by comparing them with the engineer's, but they will probably be more precise.

But obviously ML is most suitable if you're trying to outperform humans or automate the ineffable.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-03-01 11:10:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> It seems like it's probably possible. E.g. see [here](http://www.cs.columbia.edu/~vondrick/tinyvideo/) for video generation. The thing to look out for would probably be to generate next frames that are *actually correct* rather than hallucinate ones that seem plausible. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-28 04:30:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> Concerns about existential risks from AGI tend to come from the definition of AGI and what we know (or imagine) about computers. Work in this area is typically fairly implementation-agnostic. Sometimes examples are provided, but they're typically fairly high-level (e.g. whole brain emulation or reinforcement learning). Concerns are sometimes attached to estimated timelines, but I have the impression that most AI safety researchers acknowledge that we really have no idea how long it will take to create AGI, and that we *also* don't know how long it will take to solve the control problem, so we'd better get cracking. /r/ControlProblem is specifically about these concerns, and they have a lot of information on their wiki and sidebar.

If you're interested in how we might build AGI (whether it's safe or not), then there's a bit on [getting started](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) with that on /r/artificial's wiki (/r/agi is also dedicated to this, but it doesn't seem to be very active anymore). I think it's a few years old and mostly based on my experience with the AGI Society, which was not as excited about deep learning as the rest of the AI field seemingly was. Instead most approaches seem to focus on building cognitive architectures, using algorithmic information theory or (for a few) probabilistic programming. Some (many?) of these AGI researchers are now incorporating deep learning into their work, however, and obviously companies like DeepMind and OpenAI seem to think this is the way to go.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-27 13:19:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> [Link](https://www.coursera.org/learn/ai-for-everyone). [Announcement](https://blog.coursera.org/announcing-ai-for-everyone-a-new-course-from-deeplearning-ai-on-coursera/). It's taught by the famous Andrew Ng of [deeplearning.ai](https://deeplearning.ai), who also taught the excellent [Machine Learning course](https://www.coursera.org/learn/machine-learning) (among many other things). The course will take approximately 11 hours to complete.

---

I really liked Ng's ML course (linked above) so I imagine this will be quite good. I might audit it (i.e. take it for free without the homeworks), but I probably won't because I'm not a beginner and I'm quite busy with other things. I'd be very interested in hearing others' experience with it once it's over, because it may be a good course to add to the /r/artificial wiki's (kinda outdated) [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-26 23:58:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> > So it's possible the judgement is less against AI, than against the content of tests and homework that can be graded by AI.

This entered my mind too, but I thought it would require knowledge of AI that the general public is unlikely to have. But your explanation that standardized testing has been a hot-button item makes a lot of sense, so I think it's very possible that this is going on. I also think this is a valid concern, because making tests so that AI can grade them will likely not result in the highest quality exams (at least currently).

I do seriously doubt that AI would be a great career counselor in the way that you mention. It could probably do an okay job as an expert system or trained to mimic human counselors in some aspects, but it wouldn't have the contextual and human/emotional understanding of a human counselor. I also don't think this is the kind of prediction AI is good at, because it involves basically everything about the world and the economy. What would the inputs and outputs even look like? And even if you can figure something out, there's probably not that much relevant training data. This is actually a problem with macro economics in general: major changes tend to happen on the order of decades, and there haven't really been that many decades where we've had an economic system similar to now. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-26 11:20:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> Based on what you're saying, it sounds to me like a PhD is probably the way to go for you, but keep in mind that I'm not working with a lot of information here and I don't know you.

I don't know what you want in life, but if you enjoy research and staying in academia, then you kind of have to get a PhD. If you'd rather work in industry it's perhaps less clear cut. However, earning 2/3rds of the salary isn't that bad. After you completed your PhD it seems that a wider array of (industry) jobs is open to you and you might be able to earn a higher salary. You should perhaps try to find out how true this is for your country, but I suspect you could earn back the money you missed out on as a PhD student over the course of your career. My guess of your total career earning potential if you get a PhD is that it ranges from slightly less to significantly more compared to not getting a PhD.

But even if it's slightly less, what seals it for me is that you actually like being in this environment. At worst you're paying a little money for 4 years of enjoyment and learning.

On the other hand, I did not personally enjoy my PhD very much. I want to do research, so there wasn't much choice for me. But if I wanted to work on applied AI in industry, then I wish I hadn't done my PhD, even if it would help me earn more money. So if you think there's a decent chance you'll hate your PhD work and you'd be much happier working another job, then the case in favor of a PhD is much less strong. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-26 09:13:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> You can find the actual report [here](https://www.nesta.org.uk/report/education-rebooted/). Unfortunately I can't find the YouGov survey.

I'd like to know how the respondents were primed, because I find it quite surprising that parents are apparently less apprehensive about AI "[s]uggesting future career paths for students" than they are about "[a]utomating exam marking". It seems to me that for a lot of exams, marking can indeed be done quite simply and objectively by a mindless automaton while suggesting future career paths probably requires a much more personalized, context-aware and "human" touch, and it ties into the "determinism" concern that parents apparently had.

I'm also wondering how the parents were asked about their concerns, because the percentages much higher than what I expect to be the level of awareness among laymen.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-25 19:30:17 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I hate to say this but the article is more about a politician than AI by far.

If it's 99% about a politician and 1% about AI, then this is the place to discuss that 1%.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-25 17:08:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> Please remember that this is a sub about Artificial Intelligence and *not* about politics. Obviously a US presidential candidate talking about AI is relevant to discuss in this subreddit, but please do not drag in any other political issues if it is at all avoidable and be sure to be on your absolute best behavior to avoid triggering people on the opposite side of the political spectrum.

If you see anyone behave otherwise, please report and downvote, and do not add to the mess by engaging with them.

Politics is the mind killer; let's try to have a productive discussion about AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-25 17:04:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Try to remember that this is the guy that ...

Try to keep the discussion focused on AI and avoid dragging in other political issues unless absolutely necessary to your point. And in that case, please moderate your tone to avoid triggering people on the opposite side of the political spectrum.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-25 08:20:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> Your evaluation function is just trying to determine whether the game has ended, right? In other words, you're not (yet) using any heuristics, right?

In that case, yes, you only have to check if the last move ended the game. For this game, I think that can happen if the last player finished 5-in-a-row *or* if the board is full (in which case it's a draw). You should check both of those things. For the 5-in-a-row I think you should also check diagonals though, in addition to horizontal and vertical possibilities.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-25 07:51:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> So if I understand correctly you don't want to use someone else's images (a publicly available data set) but your own, and you're asking how to get bounding boxes on them?

If you already have your own images, that's great. If not, you can look into how these publicly available data sets got theirs. One way is to make a web crawler to download (certain kinds of) images from (certain parts of) the internet according to your wishes. If you do this, you should take care to respect websites' terms and conditions though, which typically means you should throttle your bot and only download something every N seconds rather than constantly, in order to avoid overwhelming their servers (and getting yourself banned).

Then you need to annotate or label those images. If you have a lot of images that need annotation, you may want to use a service like Amazon Mechanical Turk. Mechanical Turk has image annotation with bounding boxes as one of its built-in options. If you want to do it yourself (or with your employees / friends or something), or you need special options, you may need your own annotation tool. There are existing tools (google "annotation tool bounding box" or something) or you could make your own (maybe add "tutorial" to the previous query).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-23 13:31:12 </DATE>
<INFO> reddit post </INFO>
<TEXT> We have a rule against promoting your own work. We make exceptions for that all the time as I do want to encourage people to post their hobby projects like you're doing.

Posting this once is great. After that you can maybe post updates of they're interesting for the people of this sub. What I mean by an update is that you post about the difference between now and the last time you posted. E.g. if you have interesting descriptions of how you improved the AI/ML in your program (not just saying it's better now).

Since this is the third post in less than two weeks and the fourth in 3 months with the exact same text, I'm going to remove this. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-22 17:44:31 </DATE>
<INFO> reddit post </INFO>
<TEXT> Some things were not directly obvious to me. The below doesn't contain direct spoilers, but if you don't want to do the quiz on your own without any potential tips, don't read the rest of this post.

.

.

.

Questions can *locally* have multiple correct answers. So if for instance answer D implies that answer A would *also* be correct, that doesn't mean you can immediately cross out D. (I suppose reading question 19 should have made this obvious to me.)

When a question says something like *"15. The only question with answer C is: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5"*, this does indeed apply to the whole quiz and should not be read as *"out of the possibilities listed here, which is the only one with answer C?"* (i.e. in my made up question it means you could cross out answer C for all questions that aren't 1/2/3/4/5).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-22 10:18:18 </DATE>
<INFO> reddit post </INFO>
<TEXT> Okay, great!

> Singularity or Bust

I forgot about this, but I've seen it many years ago. I've mostly forgotten it, but I remember that it's mostly just following Ben Goertzel, with a little bit of Hugo de Garis. De Garis worked on the China Brain project and Goertzel is the driving force behind the AGI community and annual conferences, and the developer of OpenCog (and now also the Sophia robot with David Hanson, but not at the time of the documentary). The documentary is about AGI and the singularity, and not about contemporary (narrow) AI.

I have met and like both of them, so I thought it was interesting to see them on TV. However, like most AI docs I've seen, it should probably be called "Ben's (and Hugo's) opinions on the singularity". Or you should at least understand that that's what you're watching. Goertzel and De Garis's opinions are pretty close together (e.g. they both believe in a near-future singularity), and this documentary doesn't really question that or cover the wide spectrum of opinions among A(G)I researchers. I think that's too bad, but their opinions are kind of interesting and I guess it's good to know that they exist, so with the above caveats it's not a bad documentary to watch.

> Humans need not apply

I guess I didn't really think of short made-for-YouTube videos as documentaries, but that's probably just me. I thought this CGP Grey video (like all of his videos) was entertaining and informative. It talks about technological unemployment using an analogy with horses and the industrial revolution: basically horse populations grew for a long time because we had jobs for them, but then when we got the steam engine most of those jobs disappeared, no other jobs came in their place, and the horse populations decreased massively. I think this analogy is originally from Nick Bostrom.

Like I said, it's a nice video, but I think CGP Grey mentioned he doesn't exactly believe it anymore in one of his QA videos. There was a thread on /r/badeconomics that seems to have convinced him. Experts disagree greatly on what the impact of AI will be on unemployment.

If you like this, you may also like CGP Grey's animation of Bostrom's Fable of the Dragon-Tyrant. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-21 18:18:49 </DATE>
<INFO> reddit post </INFO>
<TEXT> I was going to say "AlphaGo", but you already mentioned it. It's the only AI documentary I remember that I actually enjoyed. The others I've seen always seem to push a narrative and get a lot wrong. What did you think of "Lo and behold", and could you perhaps share your list of 10 documentaries? Other users might like to look them up as well.

Are you just looking for documentaries? If you want to learn more about AI we have a [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) on the wiki with (among other things) links to video lectures and online courses. I think these will teach you a lot more about the topic.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-21 18:13:11 </DATE>
<INFO> reddit post </INFO>
<TEXT> Another user [posted](https://www.reddit.com/r/artificial/comments/at2jw4/philosopher_believes_ai_can_never_be_an_artist/) this [article](https://www.technologyreview.com/s/612913/a-philosopher-argues-that-an-ai-can-never-be-an-artist/) about a philosopher who argues that an AI can never be an artist. You might find it interesting. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-21 18:04:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> For video game AI that controls NPCs and opponents, you could also look on /r/gameai. As I understand it, they often use decision/behavior trees, finite state machines, "utility AI" and A\* (for pathfinding). Machine learning is usually not used, and the AI can often "cheat". It's more optimized for fun than for performance.

If you're looking at games as a test bed or challenge for AI, then people have taken different approaches, but I think tree search is the most common. In fact, AlphaGo/Zero also uses that. And it's also used with [General Game Playing](http://ggp.stanford.edu/iggpc/winners.php) and [General Video Game Playing](http://gvgai.net/papers.php).

As far as I know most serious game playing AI systems use tree search. For instance, [Deep Blue](https://www.sciencedirect.com/science/article/pii/S0004370201001291) used minimax to beat Kasparov and [AlphaGo](https://www.nature.com/articles/nature16961) used Monte Carlo tree search in addition to supervised learning (Alpha(Go)Zero got rid of that) and (deep) reinforcement learning (so perhaps that can also be considered hybrid). Another example of a very hybrid system is IBM Watson's [DeepQA system](https://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=2159) that won Jeopardy! (that's also a game, right?). Use of reinforcement learning isn't new (e.g. Tesauro already did it with [TD-Gammon](https://go.galegroup.com/ps/i.do?p=AONE&sw=w&u=googlescholar&v=2.1&it=r&id=GALE%7CA16764437&sid=googleScholar&asid=a24c4d7a) in 1992), and of course recently you see it for [arcade games](https://www.nature.com/articles/nature14236/), [Dota](https://openai.com/five/) and [StarCraft](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/). OpenAI found that [evolution strategies](https://blog.openai.com/evolution-strategies/) can be a scalable alternative to RL.

For more, check out Georgios N. Yannakakis and Julian Togelius' recent book [Artificial Intelligence and Games](http://gameaibook.org/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-20 17:06:58 </DATE>
<INFO> reddit post </INFO>
<TEXT> It's hard for me to pick out any specific thing. I just find it useful to have some idea of what we know about human cognition, because we are essentially trying to build something similar with AGI.

I think that for me it just helps me think about the different components of AGI. I could probably have realized there should be something like "attention" without any CogSci courses, but I also find it useful to know a little about how it seems to work in both a top-down and bottom-up fashion in the brain. Or if I look at memory: humans (apparently) have semantic memory, episodic memory, procedural memory, working memory, etc. Should our cognitive architectures also have different kinds of memory? What kind of memory does e.g. a particular neural network have? These are just examples though: I'm not saying "just learn about attention and memory".

There are a lot of interesting subjects in CogSci, and I would say that I'm *mostly* just inspired by them (perhaps to ask the right questions), and that it's quite rare for me to really glean implementation details from it. My CogSci knowledge is fairly superficial at this point and you could probably get the same level by just reading an introductory textbook.

These articles might also interest you:


* Kenneth D. Forbus - [AI and Cognitive Science: The Past and Next 30 Years](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1756-8765.2010.01083.x) (2010)
* Marisa Tschopp - [Human Cognition and AI](https://medium.com/womeninai/human-cognition-and-artificial-intelligence-a-plea-for-science-21a2388f6e7e) (2018)
 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-20 16:10:36 </DATE>
<INFO> reddit post </INFO>
<TEXT> From the Conclusion:

> If you are a social scientist interested in these questions, please talk to AI safety researchers! We are interested in both conversation and close collaboration. There are many institutions engaged with safety work using reward learning, including our own institution [OpenAI](https://openai.com/), [DeepMind](https://deepmind.com/), and [Berkeley’s CHAI](https://humancompatible.ai/). The AI safety organization [Ought](https://ought.org/) is already exploring similar questions, asking how iterated amplification behaves with humans.

Aside from this, there are quite a few other institutes that are concerned with AGI safety, like [MIRI](https://intelligence.org/), [FHI](https://www.fhi.ox.ac.uk/research/research-areas/#1513087763365-e148efe6-2d23), [FLI](https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/), [LCFI](http://lcfi.ac.uk/), [CSER](https://www.cser.ac.uk/research/risks-from-artificial-intelligence/), [GCRI](https://gcrinstitute.org/ai/), and I'm probably forgetting a few. [80,000 hours](https://80000hours.org/ai-safety-syllabus/) wants to help people work on AI Safety (scroll to the bottom), and has lots of suggestions on what people can work on.

Despite the fairly large amount of links above, I think there are not that many places to work on A*G*I safety, and none may be near you. However, I feel like the idea of ethical AI or ethical use of AI is getting more popular worldwide. There are things like the [Partnership on AI](https://www.partnershiponai.org/), [IEEE Ethics of Autonomous and Intelligent Systems](https://standards.ieee.org/industry-connections/ec/autonomous-systems.html), [AI4People](http://www.eismd.eu/ai4people/), [TUM Institute for Ethics in Artificial Intelligence](https://www.wi.tum.de/new-research-institute-for-ethics-in-artificial-intelligence/), workshops and conferences on [Fairness, Accountability & Transparency](https://fatconference.org/), the European Commission's [High-Level Expert Group on AI](https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence), among many others. (You can't directly work at most of these things, but they're made up of people you could maybe work for.) Many universities also already have an "Ethics of Technology" departments, or (computational) cognitive science departments that already focus on AI.

I'm not saying any of these are necessarily looking for a psychologist who wants to do AGI safety research, but you could give it a look. Sometimes you have to put a bit of a twist on your research pitch to make it appealing to your employer.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-20 13:32:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> > If AI had been tackled logically, it would perhaps have begun as an artificial biology, looking at living things and saying "Can we model these with machines?".

This is a somewhat confusing sentence, because the AI field *did* mostly start by looking at (formal) logic. We call that Good Old-Fashioned AI (GOFAI) now.

But anyway, what you talk about sounds a lot like [cybernetics](https://en.wikipedia.org/wiki/Cybernetics) which kind of predates AI. Of course, these days you also have ALife and developmental robotics etc.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-20 13:27:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> I talk about AGI and education a bit in the [Getting Started article](https://www.reddit.com/r/artificial/wiki/getting-started) on the wiki. In my opinion CogSci is virtually useless for most careers in narrow AI, but it can be a bit more useful for AGI. However, my degree programs had significant CogSci components, and I always wish I knew more math. I wouldn't necessarily want to miss my CogSci knowledge, but I feel like it's easier to pick up on your own from a book or the internet. Mathematics is a skill that seems to benefit more from having a good teacher and lots of practice, so it probably makes more sense to follow courses on that.

I would indeed go for probability & statistics, but if you're super interested in CogSci it's not a *bad* choice either. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-20 12:35:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> They gave the paper to everybody. It's part of their strategy for starting the discussion I mentioned. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-20 09:36:46 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I suspect it was a Big PR stunt, but I fail to see how it would help them.

They stated that their purpose was to start a discussion in the field about responsible disclosure. To the degree that this is a publicity stunt, it is to get publicity *for that discussion*, to hopefully get more people involved.

This "helps them" because AI safety is the reason they exist in the first place. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-20 08:48:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't really see how the title matches the text, but I'd say that AI greatly enables individuality. Through machine learning all kinds of things can be exactly tailored to me. Compare watching TV to watching YouTube: TV programming is tailored to "everyone" or some large demographic and I don't like what I see, but YouTube is always recommending things that I like (I think Netflix does the same). Most websites now show me ads that are somewhat tailored to me. If generative AI gets even better, it could perhaps generate music, movies, games and stories for me that exactly match my tastes and don't have to compromise to appeal to a larger audience.

This has obvious upsides (in isolation I would enjoy that entertainment/art more), but also downsides, because maybe a part of what makes e.g. music great is the ability to enjoy it with others and share in the culture surrounding it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-19 23:56:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I also thank them for their deliberate thought and willingness to engage in a much needed discussion on research ethics, something the AI field does not often talk about despite its paramount importance. OpenAI was correct to raise questions regarding misuse of AI ...

If you truly believe this, it seems like a small step to realize that (partially) keeping GPT-2 private is part of the strategy to indeed start that much needed and rarely had discussion in the research field. Do you honestly think it would have gotten as much attention for this idea if they had said "Here's our model, we kind of doubted whether we should release it, please discuss."? I'm not sure if it has worked very well so far, because I've actually seen very little substantial discussion on research ethics and what responsible disclosure *should* look like. Your article is much better in this regard, because it actually addresses these issues (although I'm more inclined to agree with /u/BeatLeJuce).

> ...but incorrect to use this as a justification to not open source its research.

This seems to suggest that OpenAI has *other* reason for not fully releasing GPT-2 that they are hiding by providing this justification. What would that be?

To be honest, that seems very unlikely to me. As you mentioned, GPT-2 as a technology basically doesn't matter. There's nothing really innovative about it, and it's just the next iteration in a whole sequence of language models released by multiple companies. Most research can just use the old methods, or we can just wait three seconds for Google or AllenAI or whoever to come out with *their* next model (or six months for OpenAI to revisit this issue).

The harm of not having the GPT-2 model is tiny, which is probably part of the reason why OpenAI decided to temporarily "sacrifice" it in order to have this important discussion in the field.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-19 23:19:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you need to work on your question asking skills (essentially since this appears to be your third try to ask the same thing). It's not clear what you mean.

I skimmed the video, and there appears to be an idea in it that once you enter an organization it appears to be much bigger from the inside than it looked from the outside (Doctor Who's Tardis is referenced). The obvious AI related phenomenon is that a naive AI practitioner might glance at a certain job and think "I can automate that" without realizing all the tiny details of what that job involves. I don't know a specific term for that, except maybe that this person lacks *domain knowledge*. The opposite fallacy exists too in people without AI knowledge who think *their* job couldn't possibly be automated.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-19 09:58:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> [12 December 2018](https://twitter.com/indexingai/status/1072883880707964928) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-18 18:25:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> We used LEGO Mindstorms in high school and university, which was pretty cool.

I'm guessing it's probably a bit too high level for middle school students, but /r/LudoBots is an online robotics course by Dr. Josh Bongard that you might be able to take inspiration from. (maybe /u/DrJosh also has some advice for you)

Things like [Robocode](https://robocode.sourceforge.io/) or [the AI games](http://theaigames.com/) are also fun, but probably beyond the programming ability of middle schoolers.

If they can't program at all you could still potentially run through some algorithms like e.g. minimax for tic-tac-toe.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-18 12:07:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> Why is this subreddit irrelevant? Surely if you think advances in AI point to a sooner arrival of AGI, then addressing the control problem becomes more urgent? </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-18 12:06:42 </DATE>
<INFO> reddit post </INFO>
<TEXT> > What do you believe these developments signify about AI timelines, progress towards AGI and so on?

I don't think it means much. I mean, new games getting tackled and language models coming out should be part of your model already. There don't appear to be any fundamental new insights powering these systems. GPT-2 is just bigger and uses more data than GPT. AlphaStar has a network architecture that's tailor made for this particular narrow task, and was then trained in a perfect simulator. That is not to say that these aren't impressive pieces of applied AI, but it's not even clear to me that they're a step in the direction of AGI.


If they are, then I guess it means AGI is much easier than we think. That would suggest we'll probably have AGI sooner rather than later. AlphaStar seems to be a fairly standard reinforcement learner, so standard control problem issues apply: an ASI optimizing a utility function is dangerous unless we can specify our values perfectly (which we currently can't). The GPT-2 language model seems so far from AGI that I have trouble envisioning what it would even look like. I guess it's learning a model of the world and would still need to be coupled to an AI system that bases its behavior on that learned world model. That still leaves a large portion of the AGI problem (and how to control it) open. I guess the model itself is motivated by minimizing prediction error, so it probably wants to sit in a dark corner somewhere if it got its way.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-18 11:44:29 </DATE>
<INFO> reddit post </INFO>
<TEXT> Yes, but they kind of failed because they still allowed very high burst speeds. At least that seems to be the consensus among ML and StarCraft experts. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-18 00:33:48 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think your first paragraph is broadly correct, although I wouldn't phrase it so negatively. OpenAI literally stated that this is an experiment in responsible disclosure. GPT-2 doesn't seem so dangerous to me, but I also think that if you're going to run this experiment it's probably a bad idea to do it with something that's actually very dangerous. OpenAI still disclosed quite a lot, probably enough for it to be replicated, and many people have sort of vowed to do just that to spite OpenAI.

Your last paragraph may be correct as well, except if OpenAI does indeed spark a conversation about responsible disclosure in the research field, and other researchers also come to think that releasing a better language model to the public is irresponsible. Given the (mostly very childish) reactions I've seen, I think the odds of that are quite low. But I also think the idea of responsible disclosure does warrant consideration, and that this was a somewhat valiant attempt by OpenAI to get more researchers to think about it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-17 14:34:27 </DATE>
<INFO> reddit post </INFO>
<TEXT> Having been somewhat shamed by not knowing this was about universities rather than companies, I've now taken a look at the actual [paper](http://spire.sciencespo.fr/hdl:/2441/7bucmgmilh9ul9ogmiku5legh5/resources/wp82-pierre-deschamps.pdf). The result is in Table 9, but unfortunately I don't really understand the statistics or how the author used them. The weird division into treatment and control groups is still there though, so I'm still fairly suspicious.

Something I found noteworthy in Sec. 2.2 (descriptive analytics of the whole data set):

> An important takeaway
from these two tables is that these raw figure do not suggest any systematic discrimination
against women in recruiting, since there are no significant differences between the percentage of women who apply and those that are hired. 16 This is consistent with another
article by Bosquet, Combes, and Garcia-Peñalosa (2018) that also uses French data, and
concludes that gender differences in promotion rates are mostly driven by gender differences in applications. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-17 13:07:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Deschamps created a model that predicted hiring on the basis of past patterns and applicant qualifications. He then compared committees that had to reconfigure themselves to comply with the law — the treatment groups — with committees that were unaffected because they were already in compliance — the control groups. Overall, Deschamps estimated that the quotas reduced the hiring of women by 38% in the affected committees.

So if I understand correctly the "treatment groups" hired 38% less women than the "control groups". Is that right?

It also seems that the "control groups" were in companies\* that were already woke enough to put a bunch of women on their committee, while the "treatment groups" were in companies that didn't.

If so, this seems like supremely bad study design. This sounds like a cancer treatment study where the control group consists of people without cancer, and the conclusion is that more of the treated patients died...

Clearly the control group should have been other companies that didn't have enough women on the committee but didn't participate in the reform (i.e. that had the same "disease" but weren't "treated")\*\*. I could still totally imagine that the reform breeded resentment that was counterproductive, but this research doesn't seem to show it, unless I'm misunderstanding the study design.


---

Edit: For this response I just went by the parent comment and didn't read the source.

\* It's about universities, not companies.

\*\* Not participating in the reform was not an option, as it was legally mandated. A better comparison might be between the same departments before and after the reform, with the obvious caveat that there might be other time-related confounders. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-17 12:37:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not a language expert, but looking at the examples in [OpenAI's blog post](https://blog.openai.com/better-language-models/) I find it totally believable that a human wrote them. That's not to say that there are no errors or silliness, but the thing is that a human could totally have made those errors too. Perhaps if GPT-2's texts are studied it will be possible to detect some pattern though (I don't know).

So yes, "bad actors" could fairly easily take this model (if it was available) and use it. The same is true for ELMo, BERT and pretty much any other technology. Do we really know that bad actors *aren't* using those? The main difference between GPT-2 and ELMo/BERT seems to be that GPT-2 outperforms the others and that it's made by OpenAI who want to experiment with "responsible disclosure".

Since most technology is dual-use, I think the decision is typically made based on what the default use case is or on the tradeoff between potential benefits and damage. GPT-2 has been called "deep fakes for text", but that doesn't really seem so damaging to me (much less than for video and audio), while the potential benefits of GPT-2 seem much larger (since it can be used for lots of NLP tasks). So I don't really see why this model would need to be kept private.

I wonder what OpenAI's motivation was for choosing this case. Did they think they needed a controversial example to generate discussion? Perhaps if they develop something actually dangerous, everybody will just agree with a decision not to publish it (although I doubt it). Or maybe they were afraid to do this with something actually dangerous, because people might consider this amount of disclosure ("we developed something awesome in these ways, but we won't release it") a challenge to reproduce it. Or maybe they *want* to publish all/most of their stuff and are seeding the discussion a case where everybody says they should publish, so that they can always point to that when they publish something in the future. Or maybe they actually think "deep fakes for text" is a great danger that outweighs the benefits... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-16 17:59:02 </DATE>
<INFO> reddit post </INFO>
<TEXT> Given that this video is almost 2 hours long and neither Rogan nor Yang are primarily about AI, could you post the timestamp(s) where they discuss AI? Thanks! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-16 17:57:05 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't really understand what you're trying to say; what your main point is. Grouping within groups can be done by hierarchical clustering. All (unsupervised) learning algorithms based their output on the features. Not all unsupervised learning is about clustering, and in e.g. dimensionality reduction it is about transforming the input features to other features, which are typically assumed to be more meaningful in some sense. For instance, they can capture the main axes of variation or create "disentangled" representations. You can use your favorite similarity measures to measure the similarity between any two things based on their (input or transformed) features or to measure the degree of membership/centrality to a cluster.
 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-16 17:44:20 </DATE>
<INFO> reddit post </INFO>
<TEXT> The meeting is obviously very relevant to this subreddit, because unlike you, the president of a big 5 tech company and leader of one of the world's largest religions talked about AI. Specifically, they talked about ethics, which a religious leader might obviously have some opinions about.

It's fine to disagree with the pope's opinions on AI, or with the idea that religious beliefs should play a role in the discussion of the ethics of AI (despite the fact that many people say they derive their ethics from their religion). Just make those AI-related arguments and don't drag in a ton of other stuff.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-16 17:04:08 </DATE>
<INFO> reddit post </INFO>
<TEXT> Let's keep the discussion focused on AI. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-16 17:02:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I often find Superintelligence by Nick Bostrom is recommended, but I also usually find very critical opinions about the book and the author.

I will also recommend this book. It's controversial because it shows why artificial superintelligence might be an existential risk for humanity, which is not something many people like to hear, especially if they dedicated their careers to AI. It is often thoroughly misrepresented, but I think if you read it you'll see that's it's actually very reasonable most of the time.

However, this only covers one of the many aspects of the [philosophy of AI](https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence).These include not just existential risks, but also questions about roborights, ethics/philosophy of technology, other impacts of AI, consciousness, agency, autonomy, the(ory of) mind, intelligence, knowledge and its representation, uncertainty, logic, ontology, etc. Unfortunately I don't know a book that covers these, but a good place to start may be the AI entries on [SEP](https://plato.stanford.edu/entries/artificial-intelligence/) and [IEP](https://www.iep.utm.edu/art-inte/).

Recommendations I've often seen: Gödel, Escher, Bach and Moral Machines. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-15 19:51:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> Wow! I checked and it was $160-200 on Amazon and others. Good tip! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-15 16:51:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> > It just worries me that if they call themselves open, but then don't do that, do hold back others who might have done it.

They are not against openness. In fact, they are very much for it. However, they are *even more* in favor of safety and responsibility. If they set an example for others, that would be a good thing in my opinion. "Open as much as possible, but don't be an idiot about it" is a message that's clearly sorely needed (judging by many reactions).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-15 16:45:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> Already posted [here](https://www.reddit.com/r/artificial/comments/aqw25e/ai_is_reinventing_the_way_we_invent/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-15 11:44:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Am I wrong in any of this?

Let the nitpicking begin.

> So, to start at the beginning of my understanding of this universe, everything is ultimately about pattern searches. [examples omitted]

I wouldn't call all of these things search, but I do think evolution is basically an optimization process that favors things that are good at existing (i.e. things that are easy to create, can self-replicate and/or persist).

> Then, we invented the computer, and, for the first time, we found a way to augment our minds, with AI.

I don't computers are the first invention to do augment our minds. A lot of inventions augment our working memory or long-term memory (e.g. abacus, drawing in the sand, pen and paper, books). Computers allow us to solve problems faster or better, but so do innovations in communication, language, mathematics and other areas.

> So, for the first time, we have a way to carry out a search above the ability of our minds. It no longer has to be a long term, slow search, instead, it can be directed.

Evolution is indeed kind of a blind, random, brute force process in terms of how well it can optimize for existence. Humans on the other hand can do more directed optimization / searches. However, this was already the case without computers or AI. Our tools just aid us in this respect, especially if you want to limit the discussion to narrow AI.

> If you want to design a new product, you can have an AI perform a search on possible solutions etc.

> what's going to happen is that AI platforms are going to become the next internet, more and more companies will use them to be able to totally predict what is their next best move

Narrow AI can only search within a confined space of solutions, and "totally predict the next best move" is probably intractable even with artificial superintelligence (ASI) if there are also other ASIs. Basically, the economy is hard to predict because of the other actors. Even very simple games like rock-paper-scissors or matching pennies are difficult against a good opponent (if you're actually trying to win and not draw).

> our economy will become entirely run by AI predictions.

Maybe, but I think it will be difficult for narrow AI, because it probably can't take all relevant context into account. But with superhuman artificial general intelligence (AGI), the AGI would be able to take better economic decisions than humans, so they will probably (be allowed to) make them.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-15 11:05:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://www.reddit.com/r/artificial/wiki/getting-started#wiki_narrow_ai_or_general_ai.3F

or

https://en.wikipedia.org/w/index.php?title=Narrow_AI </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-15 08:36:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> See the [Getting Started section](https://www.reddit.com/r/artificial/wiki/index#wiki_getting_started_with_ai) of /r/artificial's wiki.

> Generative, deep and convolutional neural networks

Goodfellow et al.'s [Deep Learning book](http://www.deeplearningbook.org/).

> Learning methods

Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/).

> Computational intelligence methods

Not exactly sure what you mean, but Poole & Mackworth's free book is called [Artificial Intelligence: Foundations of Computational Agents](https://artint.info/html/ArtInt.html).

> Stochastic dynamic programming

There are probably books that are directly about this that I don't personally know. But dynamic programming and stochastic methods are covered extensively in Sutton & Barto's [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html).

---

I'll also say that Stuart & Russell's [AI: A Modern Approach](http://aima.cs.berkeley.edu/) is kind of the AI bible, but if you buy it, it will cost almost $200.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-15 01:31:47 </DATE>
<INFO> reddit post </INFO>
<TEXT> I posted this news article because the title and discussion is directly related to the control problem. Here is OpenAI's [official blog post](https://blog.openai.com/better-language-models/) where the last paragraph specifically discusses their publication/release strategy and invites others to think about this too.
Also, links to the actual [PDF paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and parts of the [code](https://github.com/openai/gpt-2) that they did release.

Just preemptively I'd like to note that not releasing code, data and/or the trained model doesn't really seem so unorthodox to me. Many researchers do that, and OpenAI still released some of their code and a smaller model. Also, OpenAI was either never about complete openness or realized very soon that they shouldn't be, so this isn't them betraying their values or anything: their actual stated goal is safe AGI. Nevertheless, I think it's interesting to see how they handled this and to discuss how it perhaps should be handled.

Related reading: Bostrom's 2014 paper on ["Strategic Implications of Openness in AI
Development"](https://nickbostrom.com/papers/openness.pdf) and [Hoffman's critique](http://benjaminrosshoffman.com/openai-makes-humanity-less-safe/) of (how he perceived) OpenAI's openness. </TEXT>
</WRITING>
<WRITING>
<TITLE> OpenAI Guards Its ML Model Code & Data to Thwart Malicious Usage </TITLE>
<DATE> 2019-02-15 01:31:41 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-15 01:06:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know much about that. I've also heard about it, but always found it a little hard to believe. When we're reasoning *consciously* we absolutely suck at (Bayesian) probability theory. [Doctors](https://blogs.cornell.edu/info2040/2014/11/12/doctors-dont-know-bayes-theorem/) have no idea what the probability is that a patient has a disease that 1% of the population has when they get a positive measurement from a test with 90% specificity and sensitivity. And not that it's specifically Bayesian, but many people don't even understand that [Pr(A) >= Pr(A & B)](https://en.wikipedia.org/wiki/Conjunction_fallacy) for all values of A and B.

But of course most of what the brain does isn't conscious, and there are apparently many cognitive and neuroscientists who take the "Bayesian brain hypothesis" seriously. I think it's related to [predictive coding/processing](https://en.wikipedia.org/wiki/Predictive_coding) and the [free energy principle](https://en.wikipedia.org/wiki/Free_energy_principle). However, I also get the impression from the abstract of [this book](https://mitpress.mit.edu/books/bayesian-brain) (which I haven't read) that the Bayesian brain is often used as a sort of idealized model.

The question of whether the brain is Bayesian appears to be somewhat open, and was the subject of a [2015 conference](https://wp.nyu.edu/consciousness/bayesian/). I didn't attend, but I'm inclined to believe [this skeptic](https://blogs.scientificamerican.com/cross-check/are-brains-bayesian/), also based on papers like [this](https://pdfs.semanticscholar.org/6586/c5a76173dca0151bdb66e9a641910c44be31.pdf) (titled "Predictive coding and the Bayesian brain:
Intractability hurdles that are yet to be overcome"). I think Julia Galef gives a very short and decent explanation in [this video](https://www.youtube.com/watch?v=cFv5DvrLDCg) why it seems that are brains are indeed not Bayesian belief networks that also involves intractability.

The 2013 AGI conference also had a [workshop](http://www.agi-conference.org/2013/workshops/) on "Probability Theory or Not? Practical and Theoretical  Concerns on Uncertainty Handling in AGI" with a few papers. Videos of that are on the [AGI Society's channel](https://www.youtube.com/watch?v=UBBXusPSd7g&list=PLZlLHCryX93J5O2iGzkSd7HjRKU9kb0tF&index=10), and some are also filmed in higher quality by Adam Ford: [Hutter](https://www.youtube.com/watch?v=gb4oXRsw3yA), [Goertzel](https://www.youtube.com/watch?v=Q00XWfeEZYg), [Wang](https://www.youtube.com/watch?v=cyzc26pKaAc), [discussion panel](https://www.youtube.com/watch?v=nqW4HGY5bHQ) (I don't see the presentations from Schockart, Benferhat and Rodionov there).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-15 00:22:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> It seems to fall under game theory. This [course](http://modelai.gettysburg.edu/2013/cfr/index.html) on counterfactual regret minimization may be helpful, although it doesn't talk about using Monte Carlo sampling for computing that regret.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-14 23:58:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think SICP can help you become a better programmer, which may help a bit in AI research, but I wouldn't say it's a must-read for most AI researchers. However, I will say that Lisp-variants (like Racket) are often used for genetic and metaprogramming (and some kinds of metalearning), because of their homoiconicity. However, it's not like you need to be a master Lisp programmer for that, so you might be able to pick up enough knowledge from much shorter and easier tutorials.

I don't know. SICP is great, and if you're going to learn Lisp/Racket anyway, that's kind of an extra reason to read it. But I wouldn't say it's necessary. You could probably also just start with the first few chapters, and maybe look at the [1986 course videos](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video-lectures/). And then you can decide if you want to continue. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-14 23:42:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think they were initially under the misguided impression that openness was an unequivocal good, but they have since come to their senses.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-14 23:41:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> Eh, they're still more open here than most researchers. At least they published their code. Publishing potentially dangerous information would be stupid, just because the word "open" is in their name. See my [top-level comment](https://www.reddit.com/r/artificial/comments/aqnuak/new_openai_paper/egho06v/). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-14 23:39:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> It would probably have been helpful to link to the [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) or [accompanying blogpost](https://blog.openai.com/better-language-models/).

For the record, most research papers don't even open source their code ([which OpenAI did do](https://github.com/openai/gpt-2)\*). Releasing trained models is even rarer. If this was any other company or research group, they would simply not have mentioned it. However, because OpenAI does want to be open, they explain explicitly when they're not.

I do think they probably regret the choice of their name though. I think the original philosophy was indeed to be [open about almost anything](https://blog.openai.com/introducing-openai/), but they pretty quickly realized this clearly conflicted with their actual (stated) goal of safely developing artificial general intelligence (or it was [pointed out](http://benjaminrosshoffman.com/openai-makes-humanity-less-safe/) to them). Their [About page](https://openai.com/about/) reflects this (they won't keep information private for private benefit, but they might for safety reasons), but I remember employees/founders given interviews pretty quickly after launch where they mentioned they'd evaluate the safety of publishing things before doing so.

\* Edit: Apparently they only released some of their code *and* a smaller trained model. This is still more open than most researchers. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-14 16:35:01 </DATE>
<INFO> reddit post </INFO>
<TEXT> Do you mean approaches that are not inspired by biology? (I wouldn't call that anthropomorphism.)

I think biological inspiration exists to varying degrees in AGI projects (arguably you can't have "none", because the concept of general intelligence itself is inspired by humans). Pei Wang has a nice [overview](https://cis.temple.edu/~pwang/Writing/AGI-Intro.html#TOC-What-is-AGI) of what people are trying to accomplish when they say they're pursuing AGI: Structure, Behavior, Capability, Function and Principle. You should click the link for better explanations and associated projects, but we might say that Structure, Behavior and arguably Function are heavily human-inspired, while Capability, Principle and arguably Function have little to do with humans.

In a [tutorial](http://agi-conf.org/2017/?page_id=24) at the 2017 AGI conference Alexey Potapov outlined 4 classes of approaches to AGI: Cognitive architectures, Deep learning, Probabilistic models, Universal algorithmic intelligence. It may sound like the first two are heavily biology inspired, and the latter two aren't. However, many "cognitive" architectures are not particularly inspired by biology (e.g. NARS and AERA), and should perhaps just be called "control architectures", and deep learning / neural networks tend to be only very loosely inspired by the brain. Perhaps a bit like how airplanes are inspired by birds in that they have two (totally different) wings. On the other hand, probabilistic models and approximations to AIXI / Gödel machines *may* use something like a neural network under the hood (although that wouldn't really be because they want the system to be like the brain, but rather because it happens to work well).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-14 12:00:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> I don't know. Is ornithology useful to aerospace engineering? I know our airplanes don't fly like birds exactly, but did "we" take some inspiration from them? Should we? As I understand it birds are more efficient and maneuverable than man-made flying machines.

Cognitive science may be similar. It seems possible to build AI without it, in a way that's entirely unlike the human mind. Or like the human mind, but based more on neuroscience without much cogsci knowledge. But humans are clearly generally intelligent, and *if* we could copy that we would have AGI. And perhaps cognitive science is the right level of abstraction for that. There are many (biologically inspired) cognitive architectures, and they seem to me like a somewhat promising road to AGI. Maybe other roads will be better/easier, but at this time I'd say the jury is still out. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-14 09:22:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> The simplest way is to just use your personal domain knowledge of the game and compute a weighted average. If you let the weights correspond to relative importance, you should (first) make sure all heuristics are on a similar scale: the low and high values for each heuristics should be similar (e.g. all between 0 and 1, and not one between -100 and 100, another between 10 and 20 and the third between e and pi).

If you really don't know, or you want to improve on this, run some experiments with AIs with different (combined) heuristics. How does the AI with only heuristic 1 (H1) perform against one with only H2, and against the AI which averages all heuristics. From this you can get an idea of what works well and what doesn't, and you can make new weightings based on that.

In that case you're basically doing a kind of machine learning, but manual. It is of course also possible to automate that. This will also allow you to use more complex models than a simple linear combination (weighted average). For instance, you could use a neural network with your three heuristics as inputs and the final heuristic score as an output. But you're not even limited to that: you could also try inputting the entire game state to the neural network (perhaps in addition to your own heuristics) and then output the final heuristic.

You will then need to learn the parameters of your model. This can be done with e.g. genetic algorithms or reinforcement learning and self-play (kind of like AlphaGo/AlphaZero, except you're using minimax instead of MCTS).  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-13 23:02:53 </DATE>
<INFO> reddit post </INFO>
<TEXT> Hard to believe I've been on Reddit for a decade...  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-12 15:51:09 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think both options can work. It depends a little bit on what you want to do exactly. I think your background is sufficient to enter industry work, and having a more economic background can actually be a big boon there. However, if the company uses AI/ML techniques they know nothing about, they might prefer to hire someone else (this is not guaranteed), and if they don't, then you're probably not going to learn about them on the job. Perhaps with your probably sufficiently strong background it will be reasonably easy to learn this on your own though.

I do think that the best place to learn the basics of an area is in school: it's literally made for that. If you can find a good master program in AI/ML you will more quickly learn a wider range of AI/ML related basics than you would in a job or in your free time. How optimal this is depends on how good the program, teachers and fellow students are, how well it fits with your skill level, and how it complements your previous education and interests. Another advantage is the potential to meet more peers and teachers who are interested in AI/ML, which can be good for your professional network if you intend to work in the field. Also, a second degree may open up some doors. The obvious downside is the (opportunity) cost.

Alternative approaches might be to aim for a PhD or to work during your studies (especially if the second master is too easy/slow for you or you really want to make money now). A PhD might be more at your level if you already have a master degree, and it typically allows you to focus on the most relevant things for you(r project). It will obviously take longer, but it's genuinely a step up, and I you might actually make the money back, because I hear that companies often pay more if you have a PhD.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-12 09:00:26 </DATE>
<INFO> reddit post </INFO>
<TEXT> You could pretty much read any books by Ben Goertzel or Ray Kurzweil. Bostrom's Superintelligence and Tegmark's Life 3.0 are about the possibility of artificial superintelligence (which would potentially be quite dangerous). And although AI: A Modern Approach is mostly about narrow AI, I think it talks a bit about the possibility of human-level AGI as well. I don't know any books about machine consciousness / sentience and artificial emotions off the top of my head, but there should be plenty of papers about both.

On YouTube I think the channels of Adam Ford and Robert SK Miles both cover a lot of AGI (Safety) stuff. The AGI Society also has videos of old conferences.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-11 18:04:55 </DATE>
<INFO> reddit post </INFO>
<TEXT> Relevant concepts: [automatic programming](https://en.wikipedia.org/wiki/Automatic_programming), [program synthesis](https://en.wikipedia.org/wiki/Program_synthesis), [inductive programming](https://en.wikipedia.org/wiki/Inductive_programming), [genetic programming](https://en.wikipedia.org/wiki/Genetic_programming). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-11 17:41:30 </DATE>
<INFO> reddit post </INFO>
<TEXT> Sounds like [Moravec's paradox](https://en.wikipedia.org/wiki/Moravec%27s_paradox): things that are hard for us are easy for AI (e.g. logical reasoning), and vice versa (e.g. sensorimotor operation). Or more eloquently:

> Encoded in the large, highly evolved sensory and motor portions of the human brain is a billion years of experience about the nature of the world and how to survive in it. The deliberate process we call reasoning is, I believe, the thinnest veneer of human thought, effective only because it is supported by this much older and much more powerful, though usually unconscious, sensorimotor knowledge. We are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy. Abstract thought, though, is a new trick, perhaps less than 100 thousand years old. We have not yet mastered it. It is not all that intrinsically difficult; it just seems so when we do it.

---

> I remember having read about the optimists (the most famous being Marvin Minsky) in the 60s and 70s talking about how AI would bascially surpass human intelligence within a few years. They based this (among other findings) on the fact that a computer was ablet to beat a world champion chess player.

IBM's Deep Blue beat Garry Kasparov in 1997, so something is a little off here. But you already realized that. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-11 08:12:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> https://en.wikipedia.org/wiki/List_of_artificial_intelligence_films

Although they tend to be very unrealistic I enjoy movies and series about AI a lot. Ex Machina, Her and Age of Ultron may be the most popular recent(ish) ones, but I also quite enjoyed Transcendence, Eagle Eye and Autómata.

There are too many to name. What are you looking for? (i.e. what kind of story / genre) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-10 13:35:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I'm a lot more concerned about humans mis-using AI at this point than about AI getting out of control.

I'm also somewhat concerned about near-term negative impacts of (use of) AI, and clearly those things are going to happen sooner (and are already happening). But it's possible to be concerned about multiple things at once. Even though AGI/ASI is further away, the impact would be many orders of magnitude larger, and making sure that impact is beneficial seems like an extremely difficult problem. I think that makes it worth working on.

I don't even think those things are mutually exclusive. I was recently at a "Future of AI" workshop that completely ignored AGI, but was nevertheless in part about putting our ethics into AI systems. That's basically value-alignment. I'm all for trying to figure out how to do that with current AI systems, and then hopefully getting better at it so we can do it with AGI as well.

> Most of the near-future problems are gonna be the same as any standard **software that makes decisions without need for human input**. **[emphasis mine]**

That sounds like AI to me. But to the degree that it's not, I'm not sure why it matters. Sure, let's try to get software systems that make understandable fair/ethical decisions/predictions over which we retain meaningful human control to ensure beneficial outcomes for humanity.

> But my point was, regardless of his talks with researchers, whenever I see him opine on the topic, it's with very sci-fi tropes.

So? He might be doing that to make things understandable to his audience, or he might genuinely misunderstand the relevance of these stories. The misconception I'm talking about is mostly that many researchers are unaware of the existential risks from AGI/ASI and the arguments put forth about this by researchers like Stuart Russell, Nick Bostrom and Eliezer Yudkowsky. If for some reason it was important to show that Musk was wrong, the correct/informed way to do it would not be to say "there's nothing to fear about AI, these are fantasies", but rather "Musk is wrong about the reasons for this risk, the actual reasons are XYZ and you should instead listen to Russell/Bostrom/Yudkowsky".  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-10 13:16:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> AI really isn't that different from science and psychology. Ask two experts what those terms mean, and they'll likely give different answers as well. Which of the following are sciences: physics, mathematics, philosophy, psychology, gender studies? I think most experts would agree on physics, but the others are more disputable. Psychology is the study of the mind and behavior, although I'm sure some psychologists have other definitions. Of course, this almost always refers to *human* mind and behavior, and in many contexts specifically to pathologies. What is a/the mind? Open for debate.

I agree that despite the lack of crisp, concrete and clear universally agreed-upon definitions, "science" and "psychology" have meaning. AI is exactly the same. AI is the study of (creating) intelligent machines. What is intelligence? Well... what is the mind in psychology's definition?

Like with all of these, there will be things that clearly fit the category, things that clearly don't, and things that are more dubious. If you know anything about AI and your company/marketing, it shouldn't be that hard for most things to say whether it is AI. In cases of doubt, you could tell your marketing team that it's AI according to one common perspective, and not according to another and let them decide whether it should be used in that case. (Answer in 2019: probably yes.) </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-10 13:02:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> This subreddit does nothing but talk about AI/ML. People posting their personal projects is certainly not the main source of content here, but it does happen. If you want to post your project, you should go right ahead. If it's at the level that a professional might produce, you could perhaps also post it on /r/MachineLearning. /r/MLQuestions and /r/learnmachinelearning are likely more forgiving, and decent places to ask about feedback. I don't know if /r/practicalagi is still active, but it was originally conceived for people working on their own AGI projects. I think /r/robotics is pretty good if you're doing robotics. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-09 14:08:40 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you have a weird idea of something not meaning anything. Clearly AI means something, just like "science" means something more general, "psychology" means something on roughly the same level, and "machine learning" means something more specific (and you can go even more specific with e.g. RL, model-free RL and Q-learning). It's typically good to be as specific as possible, but it also depends on the knowledge level of the people you're communicating with.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-09 12:44:10 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I will say that people who know anything about pop culture IME usually can conceptualize AI as disembodied and as robotic.

It's probably not as dramatic as I put it, but when I describe what AI is to other people (without giving specific disembodied examples), I usually get a response like "Ooohh... So, robots?". I think it's because movie/TV depictions tend to be robots (or not called AI), and there are often pictures of robots next to writings about AI (probably in part because you can't make a picture of software that's meaningful to the average reader). But of course, some people have seen entertainment with disembodied AI, and since the term is back in vogue more people seem to know what it actually is. So I agree that people can actually conceptualize of AI as disembodied, and almost everyone can after it's explained to them.

> I can't speak to Hawking, but Musk definitely got his ideas of AI from science fiction.

It may depend on what you consider sci-fi, but I'm pretty sure Musk got his ideas from talking to researchers who actually work on A(G)I Safety, and that this has convinced him it's an important issue. The way he talks about it is steeped in metaphor and perhaps sometimes (erroneously) referring to pop culture, presumably because he's making these statements to layman audiences. I don't really think he's the best ambassador for this kind of research and it's entirely possible that he has a bunch of erroneous beliefs about it. But the main misconception is that he and sci-fi are the best sources/arguments for the idea of existential risk from ASI, while not knowing there are researchers who actually work on that stuff because they have good reasons for doing so. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-09 12:30:00 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think if you're talking to someone who actually knows the field, they will know that all of the things you mention are in fact (narrow) AI. Being more specific is of course good when you're talking to others who know the more specific terms as well, but "AI" is just as valid a word as "psychology", "physics" or "computer science": it's just the name of a field of inquiry which in this case happens to be synonymous with the category of product that comes out of it. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-09 12:24:13 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think that depends on what kind of professional. There are lots of researchers who actually know what AI stands for and it's very meaningful to them. But if your background is marketing, then it's just the next bullshit term, and if it's something like data science it will seem weird that your work is suddenly called AI by a lot of people. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 23:09:34 </DATE>
<INFO> reddit post </INFO>
<TEXT> The main one is probably the confusion between artificial general intelligence (AGI) and narrow AI (ANI), and what the term "AI" refers to. All existing AI products are ANI and that's also what most AI professionals are working on. To them "AI" means "ANI". For many laymen and Hollywood the term "AI" at least includes "AGI" and might actually refer to it exclusively. This causes ideas that the launch of some AI product or new progress will quickly lead to AGI and our machine overlords.

A misconception that exists among many professionals is that nobody is working on AGI. Also, they typically don't know the research and arguments about existential risk from AGI and assume Musk and Hawking just got their fears from Hollywood.

Other misconceptions include that AI is all about robots, and that AI will be very humanlike (including conscious, which we actually have no scientific idea about). </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 23:02:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> Nils Nilsson's The Quest for AI is 10 years old, but it probably matches your description the best.

Pedro Domingos' The Master Algorithm is a nice book about machine learning.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 15:57:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> > Mhh... can you expand on why it would be a mistake to change its terminal goal?

This is assuming that you pursuing your terminal goal X will make it more likely that it's accomplished/optimized (something similar is often used as a definition of intelligence). If there's some other goal Y that's really good at helping you accomplish X, then you can always make it a subgoal. By virtue of being a different goal, there will be situations where X and Y conflict. If Y is a subgoal, it can just be discarded at this point in favor of something else that will help achieve X better. But if you've replaced X with Y, you would pursue Y in these situations and get a less good result on X.

> Is everything a human does, really to reach the terminal goal of feeling good?

Probably not, for a number of reasons:

1. "feel good" is probably not a terminal value or not the only one. I used it as an example because it's not easy to ask/answer more "why" questions. Unfortunately I don't really know exactly what my terminal values are, and the same seems true for other humans. This is part of what makes the value alignment problem difficult / unsolved.
* Whatever those terminal values are, they might change because our bodies/brains might change in ways we have no control over (e.g. brain tissue growing / dying).
* Humans are neither purely rational nor superintelligent.
 </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 13:19:21 </DATE>
<INFO> reddit post </INFO>
<TEXT> The idea is basically that changing its terminal goals would be a (stupid) mistake, and that if the ASI is really really intelligent, then the chances of making such a mistake are low. In fact, having its terminal goals changed (by a human, by an accident, or by itself) is likely one of the worst things that might happen, so a very intelligent agent should take every precaution to prevent this.

I'm not claiming this is 100% foolproof (100% foolproof is impossible anyway), but it is different from containment. With containment, we're pitting the superintelligence of the ASI *against* our measures: it will try to break the box. With this, that superintelligence will be used to *protect* the terminal goals we put in.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 12:56:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> > That depends on why it want's to achieve a goal.

Perhaps I should have been clearer. Imagine a goal hierarchy with *terminal* goals at the top, and *instrumental* subgoals below it that are only there to help achieve a goal that's higher up the hierarchy. The answer to **why** a rational agent wants to achieve a goal is always "because I think it helps achieve the goal above it", until you get to the top of the hierarchy where the answer is basically just "because" (or maybe "because that's how I work / how I'm programmed").

I might have a goal to make money. Why? I don't intrinsically care about money, but it will help me get drugs. Why? I don't intrinsically care about drugs, but they help me feel good. Why do I want to feel good? I don't know. That's how I'm "programmed"? I don't need a higher reason.

In your example, if the goal was only there to get some reward, then that goal is a subgoal of the goal to get that reward. Subgoals can always be changed if you think there are better ways to achieve the higher level goal. My point was about terminal goals.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 12:31:59 </DATE>
<INFO> reddit post </INFO>
<TEXT> Because an intelligent system pursues its goals (i.e. what you programmed in). If you put it in a box, that will probably make it harder to achieve its goals, so breaking out of the box will become a subgoal. Changing your goals on the other hand is not a good subgoal for achieving those goals.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 12:26:23 </DATE>
<INFO> reddit post </INFO>
<TEXT> You could look at the cases of [Microsoft's Tay chatbot](https://www.techrepublic.com/article/how-the-microsoft-tay-chatbot-debacle-could-have-been-prevented-with-better-ai/), some of [Facebook's AI "inventing" a new language](https://towardsdatascience.com/the-truth-behind-facebook-ai-inventing-a-new-language-37c5d680e5a7) or [how voice assistants (should) respond to sexism](https://qz.com/911681/we-tested-apples-siri-amazon-echos-alexa-microsofts-cortana-and-googles-google-home-to-see-which-personal-assistant-bots-stand-up-for-themselves-in-the-face-of-sexual-harassment/). If you want to be more general, you could write about chatbots, negotiation/argumentation systems, or virtual assistants.

I know you don't necessarily have to stay within Communications, but I don't know what else you're interested in. As you mention, there is a lot to write about. This shouldn't really be a problem, since you don't need the perfect topic: you can just pick anything out of the many options out there.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 12:10:15 </DATE>
<INFO> reddit post </INFO>
<TEXT> This is a hotly debated issue with lots of information published that reaches different conclusions. I think CGP Grey changed his mind about his own video after he saw some responses on /r/badeconomics. Most of the sounds I'm hearing recently indicate that as long as we don't develop AGI, we don't have to be too scared of long-term technological unemployment.

Automation has happened since the dawn of humanity and we always managed to invent more jobs to keep people busy. I'm not sure we can rule out the possibility that this time will be different, but you need to take into account that throughout history it has always looked like "this time is different". That's because it's relatively easy to see which current jobs might be displaced as the result of a new technology, but it's much harder to think about the jobs that will be created.

In the short term there have historically been problems though, because it takes time for the new jobs to be created and workers to move into them (and some fired workers may be unable too switch careers, so it may take a generation).

There are older discussions on this topic [here](https://www.reddit.com/r/artificial/comments/a5pv2u/will_ai_destroy_more_jobs_than_it_can_create/), [here](https://www.reddit.com/r/artificial/comments/8zx2mx/artificial_intelligence_will_create_as_many_jobs/) and [here](https://www.reddit.com/r/academia/comments/663mtz/what_are_some_objective_telltale_signs_of_a/) with some links to (old) reports. I thought [this](https://slatestarcodex.com/2018/02/19/technological-unemployment-much-more-than-you-wanted-to-know/) was also a good article on the matter.

---

Musk and Hawking's concerns about an AI takeover are about artificial general intelligence (AGI), and are better described by researchers like Stuart Russell, Nick Bostrom and Eliezer Yudkowsky. /r/ControlProblem is about this. It's a real issue, but most people think it's a bit further away and discussions of technological unemployment tend to focus on the time before AGI. The development of AGI and especially artificial superintelligence (ASI) would change everything, and I think most people would agree that most jobs for humans would disappear. However, this would likely be the least of our concerns.

> Another great source I would like further insight into is the question "If the AI will develop their own communications between each other on a global scale, will we ever be able to shut it down."

I think it is at least conceivable that an ASI could be powerful enough that we can't shut it down. One approach to solving the control problem is "corrigibility", which would allow us to shut down a corrigible ASI no matter how intelligent it is.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 11:42:04 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think you're correct that AGI is the most dangerous thing imaginable, but 1) researchers are working to prevent catastrophic outcomes, 2) if we get it right, it'll also be the best thing imaginable, so there's an opportunity cost to slowing down development, and 3) outlawing it would be difficult, especially because nations can't really afford to lose the AGI race. So in light of those, I think the best thing to do is to increase awareness about the great risks, and invest more in AGI safety research (#1).

> I see some people talk about 'AGI safety' and theorising about how we could have such an AI serve humanity...and all I can think is, 'man, you really are stupid, aren't you?'.

This sounds like a really bad attitude. I can assure you that AGI safety researchers have given this much more thought than you have. You seem to be under the impression that the main strategy to control AGI would be something called "boxing" or "containment": i.e. contain the AI in some kind of box that it can't get out of.

This is an incredibly unpopular approach to AGI safety, for basically the reasons you mentioned: a sufficiently intelligent ASI could get out. To deal with arbitrary high levels of superintelligence, researchers acknowledge that the AI would be *able* to do pretty much anything, so the main focus is on making AI *willing* to do what we want. It's important to remember that even if an ASI is highly capable, it is not literally magic: it is still running on the programming we gave it (or it has reprogrammed itself based on our programming), so our challenge is to get this programming right.

This appears to be an extremely difficult programming that is far from solved, and until it's solved, I agree that AGI is potentially extremely dangerous technology. But another thing to acknowledge is that it's highly uncertain what will happen. I think the hard takeoff hypothesis that you subscribe to (AGI -> extreme ASI in hours/days) is somewhat plausible, but we don't *know* that's what will happen or at what "level" of intelligence self-improvement will run into a bottleneck. It may turn out fine with relatively minor precautions. I wouldn't want to bet the future of humanity on that, but I think all scenarios should be taken into account, including plausible good ones.

See /r/ControlProblem for a subreddit dedicated to these issues.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 11:11:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> AI/ML programmers &mdash; i.e. the people whose main task is to code the application, typically using some framework &mdash; tend to be fairly anonymous. If you want to hear from them, then online fora like /r/MachineLearning and others are probably the best for that, or maybe seek out some vloggers on YouTube.

I don't really know of any public figures who also develop AI, but there are of course people who are famous within the field. A list of AMAs can be found on /r/MachineLearning's side bar.

Adam Ford regularly interviews AGI researchers on his YouTube channel. If you want to hear more about dangers of the singularity, look for talks by / interviews with Stuart Russell, Nick Bostrom and Eliezer Yudkowsky. Robert SK Miles also has a good YouTube channel on the matter. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 01:37:06 </DATE>
<INFO> reddit post </INFO>
<TEXT> I think we probably see the most influence from neuroscience (I wouldn't really say neuropsychology, but others might disagree). Mainly in neural networks. The idea itself kind of comes from neuroscience, originally with (biologically plausible) Hebbian learning, which has mostly been replaced with the more efficient (but not biologically inspired) backpropagation. I think modular neural networks and convolutions are also inspired by nature.

For cognitive science/psychology I was mainly thinking of [biologically inspired cognitive architectures](http://bicasociety.org/resources/). But I think a lot of machine learning paradigms are also inspired by biologists and psychologists. Developmental psychology is basically about learning, and includes theories of reinforcement learning, active learning, social learning (cf. imitation learning for ML), and from this (and animal training, educational science, etc.) we also get ideas about shaping, chaining, scaffolding, curriculum learning... </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 01:22:24 </DATE>
<INFO> reddit post </INFO>
<TEXT> I haven't seen a huge difference between researchers in ML and other areas of AI with respect to EY/Bostrom, but it's possible that I just haven't noticed. I'm not even sure AGI researchers are necessarily more receptive. I think they're more inclined to think we'll have AGI soon, but they're probably also more invested in the idea that it will turn out fine.

The main difference I think I noticed is that younger researchers seem to be more likely to take AGI risks seriously than older ones. AGI risk believers (like me) might say this is because they're more invested and less open to change (Planck's idea that science progresses one funeral at a time). Deniers might say this is because older scientists are more knowledgeable and the youth is getting brainwashed or something.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-08 01:11:37 </DATE>
<INFO> reddit post </INFO>
<TEXT>


Interesting question. I guess the main thing I'd like is if more narrow/mainstream AI/ML researchers knew about the research into AGI and A(G)I Safety.

I have a bachelor and master degree in AI ("AI" is actually the name of the program), and I came away from it thinking that AGI was a layman fantasy that nobody was working on. I think the same is true for many people in the AI field. I've talked to numerous senior professors at AI conferences and workshops who thought it was perhaps time to get back to the field's original dreams, and had no idea of the research that's already explicitly aimed at this. And I see the same in a lot of commentary that argues against Musk/Hawking without seemingly knowing about the actual research that's happening in that area.

My primary hope for increased awareness is to draw in more people, because I think a large percentage probably went into AI with dreams of AGI, and switched to narrow AI because they didn't know there was another option. A secondary hope is for more collaboration options, and a third is that we'll see less uninformed commentary from AI/ML experts on AGI (safety). If you'd ask a ML expert about e.g. case-based reasoning, I think most would respond humbly and mention that it's not their specialty. But with AGI it seems the response is quite different, even though their actual work is "extremely divorced" from it, and I suspect this is partially because they're not aware that anyone is actually working on that, so they figure they're the closest thing to an expert that exists. But even if they keep commenting, I hope that more awareness would improve the quality (or the analysis) of their comments.

I think many AI/ML researchers are just trying to create smart, useful applications, and I think that's great. They don't really need to know anything about AGI.

But sometimes I also encounter AI/ML researchers who say something like "aren't we all working towards AGI?". And I find that they typically haven't given AGI much thought at all. For instance, they might confuse domain-independent AI for AGI. Or they might think all the narrow AI people are making a million applications that "just" need to be glued together, and then you'll basically have AGI because it can do a million things. (Problem: roughly the entire complexity of the AGI problem is offloaded to this unspecified glue.)

I think the AI field split into many subfields in a standard scientific divide-and-conquer approach, with the idea that they're each solving a smaller problem that will be a piece of the AGI puzzle. But most researchers haven't really given much thought to what the puzzle should look like, or how their work fits into it. AGI research often tries to come up with a grand vision or (often holistic) idea of how to solve the problem: constructing the puzzle, but perhaps with less attention to the content of the pieces.

It seems that a standard approach in AI/ML is to take some current model and try to make a small incremental improvement to it, typically in a direction of "least resistance" rather than towards some bigger plan of how to achieve AGI. There's also a strong focus on empiricism and (incremental) results. I think there are some upsides to this, but it also kind of disincentivizes different ideas. To be fair, I think many ML researchers feel the same way and are working to improve the situation. This approach might be "slow but steady", but it doesn't seem likely to lead to the fundamental breakthroughs most AI/ML/AGI researchers seem to think are necessary for AGI.

In particular, I think it's sad how symbolic AI is currently viewed in many circles, because I think (symbolic) cognitive architectures are a more promising route to (quickly and safely) developing AGI. But in general, it's good to realize that there are many different possible approaches, and I would like it if we could try to evaluate ideas without necessarily demanding direct results. Especially on single-task benchmarks which will inevitably be solved with narrow AI.

Maybe I'll think of more later, but I think this will have to do for now. The post is already long enough. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-07 19:09:52 </DATE>
<INFO> reddit post </INFO>
<TEXT> > I hope my opinions/experiences is helpful

Definitely! Thank you! And I agree with everything in this post! </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-07 18:05:25 </DATE>
<INFO> reddit post </INFO>
<TEXT> Thanks! I agree that there is relatively high utility in learning math with an instructor / in university, and I could see the same for scientific skills. What you say about learning to model the world is interesting too, probably especially if you want to do applied AI.

I understand better now why physics is such a good background to have, with some advantages over pure math (and of course the disadvantage of having less math). But do you think it should be recommended over something like e.g. data science, if the person already knows they want to do AI? As you mention, data science would also teach you about science and building domain models, and it seems more directly related to AI/ML.

I hope this doesn't come across as me attacking physics, but I'm genuinely curious how high I should place it on the list of recommendations for someone who's interested in AI. Or perhaps: under what circumstances (i.e. additional interests) is it the optimal choice.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-07 16:37:41 </DATE>
<INFO> reddit post </INFO>
<TEXT> AGI researcher here (kinda). I agree with some of what you said. Most of current AI is extremely divorced from AGI, people (including EY) tend to get too excited about DeepMind's latest system and imagined implications for AGI, and I think we mainly need more/better ideas rather than better hardware.

But while I don't agree with EY on everything, I also think that he and Bostrom tend to be way more reasonable about these issues than most AI/ML researchers. Which shouldn't really be a surprise, since we agree that their research is extremely divorced from AGI, so they should probably not be considered experts on this at all. Furthermore, there's also a large difference between researching how to make (capability) AGI and the kind of research that's happening in the A(G)I Safety domain.

It seems to me that many people are way too confident about their AGI-related beliefs, and EY is sometimes guilty of that too. But he is correct in realizing that [there's no fire alarm for AGI](https://intelligence.org/2017/10/13/fire-alarm/) and that among the plausible scenarios are ones that would be catastrophic for humanity. </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-07 15:01:43 </DATE>
<INFO> reddit post </INFO>
<TEXT> Most of what AI is today is basically making "smart" software. So you need engineers and programmers. On the research side, the algorithms tend to be based on mathematics (statistics) or logic.
I think psychologists mainly work in human-machine interaction.

Some psychologists (like other professions) have occasionally developed computational models to test their theories which might be considered AI. As far as I know it's rare for these models to outcompete something that was developed by computer scientists and/or mathematicians. Of course, neuroscience and cognitive science have on occasion influenced the development of AI and some of these scientists (sometimes psychologists) now also work in the field. For instance, many cognitive architectures are at least somewhat based on cognitive science, and of course neural networks are loosely based on neuroscience. However, once an original idea is taken from these mind sciences, it's usually the case that they will primarily be implemented and improved by (mostly) computer scientists and mathematicians.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-07 14:36:07 </DATE>
<INFO> reddit post </INFO>
<TEXT> We actually have a small section on [Getting Started with AGI](https://www.reddit.com/r/artificial/wiki/getting-started#wiki_how_to_get_started_with_agi.3F) on our wiki. I was like you and had no idea that people were actually working on this, until I found the AGI Society that's linked there.

> The Nick Bostrom-esque doomsday scenarios and AI ethics people are a bit hard to stomach.

I used to think this too, but I think you may want to hold your judgement until you've actually learned a bit about AGI. /r/ControlProblem is a good starting point with their side bar and wiki.  </TEXT>
</WRITING>
<WRITING>
<TITLE>   </TITLE>
<DATE> 2019-02-07 14:28:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> It seems to me that the United States is relatively averse to government regulation of people and industry, but Europe is way more open to it. The European Commission has a [High-Level Expert Group on AI](https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence) "to support the implementation of the European strategy on AI", including "the elaboration of recommendations on future-related policy development and on ethical, legal and societal issues related to AI, including socio-economic challenges".

Most nations are also individually developing strategies, that often involve thinking about ethical, legal, societal and economic impacts and issues, and how to ensure beneficial results. Also, in my experience politicians are becoming more and more aware of the importance of AI, especially as their institutions start using it for themselves and perhaps notice how citizens use it too (sometimes adversarially).

Getting involved is probably not so different from getting involved in any other area. Start by studying and specializing in something relevant. For instance, you could study Law, Policymaking, Philosophy/Ethics or Sociology or something, specialize on technology, and write your dissertation about AI. If you have a background in AI, focus on bias/fairness or transparency or something. Seek out the people and organizations who are working on these things, and try to get a job there (and/or more specialized advice). For instance, if you're in Europe you could look at the members of the High-Level Expert Group, and otherwise you could look at the people who wrote your national AI strategy or seek out some advocacy groups.

Edit: this seems relevant: http://news.mit.edu/2019/first-ai-policy-congress-0118 </TEXT>
</WRITING>
<WRITING>
<TITLE> Facebook and the Technical University of Munich Announce New Independent TUM Institute for Ethics in Artificial Intelligence | Facebook Newsroom </TITLE>
<DATE> 2019-01-21 09:15:18 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Mathematics for Data Science </TITLE>
<DATE> 2019-01-21 09:12:09 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Beyond Machine Learning: Capturing Cause-and-Effect Relationships — Irving Wladawsky-Berger </TITLE>
<DATE> 2019-01-21 09:10:35 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> How to be a good parent to artificial intelligence - Ben Goertzel on value learning </TITLE>
<DATE> 2019-01-10 18:10:59 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Artificial Intelligence Demystified – [pretty good article except maybe the 'robot uprising' part, for which I recommend Bostrom/Yudkowsky/Russell and /r/ControlProblem] </TITLE>
<DATE> 2019-01-08 13:40:40 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Unprovability comes to machine learning </TITLE>
<DATE> 2019-01-08 13:20:58 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> IBM's Project Debater AI helps humans process complex arguments </TITLE>
<DATE> 2019-01-08 13:19:00 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> As a mod of /r/artificial, should I remove a submission that offers free mammogram analysis to everyone? </TITLE>
<DATE> 2018-06-15 18:40:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> I'm not sure if this is the best healthcare/medical subreddit to ask. If there is a better one, I apologize and would be grateful to know which one I should ask instead.

I'm a mod on /r/artificial, the subreddit for Artificial Intelligence. Recently someone posted a link to a web service he made that freely analyzes mammograms and detects lesions. He claims it works better than the state of the art, which in turn is better than humans (I think). This person seems to have spent a great deal of time, effort and money to help people, and I think that's great (and the post was *really* well received while it was up).

However, it has been brought to my attention that it can be dangerous to provide medical advice like this. He doesn't have any (FDA or otherwise) certifications, didn't detail his process, didn't open source his code or his data, etc. So there's basically no way to actually verify how good this is, other than to take his word. Given the dangers of overdiagnosis (and possibly underdiagnosis caused by getting a false negative second opinion from this tool instead of going to a second human doctor), I think it may be irresponsible to aid in the spread of uncertified tools like this. It's also my impression that FDA-like institutions would agree, and that medical procedures (like this?) should be certified first. So I removed the post...

However, I'm not sure. The poster makes a fairly compelling point that false negatives are obviously terrible, and that the false negative rate of doctors is really high (["Multiple studies have shown that 20–30% of diagnosed cancers could be found retrospectively on the previous negative screening exam by blinded reviewers"](https://www.nature.com/articles/s41598-018-22437-z)). And I can understand the tendency to want to give a lot of weight to false negatives in the tradeoff with false positives.

So I would really appreciate some input from healthcare professionals who may be more familiar with this area. Should I let this great person just get on with helping lots of people find their cancer in earlier stages, or is the risk of spreading uncertified medical tools too great?

Thank you! </TEXT>
</WRITING>
<WRITING>
<TITLE> Hanson Robotics' Chief Scientist Ben Goertzel talks about their Sophia robot's AI to address controversy </TITLE>
<DATE> 2018-06-10 16:43:00 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> [1802.07228] The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation </TITLE>
<DATE> 2018-06-04 13:15:12 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Robots in Society: Blessing or Curse? - EdX learning experience on Responsible AI, part of the open source Mind of the Universe series </TITLE>
<DATE> 2018-05-01 13:51:33 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Does anyone remember a research challenge for detecting AI/AGI on the web? </TITLE>
<DATE> 2018-04-17 18:50:54 </DATE>
<INFO> reddit post </INFO>
<TEXT> I thought that OpenAI (or someone else?) at some point in 2016?) released some research challenges (I believe 4), and one of them was to detect signs that someone somewhere had a running AGI system. However, this is apparently not part of their [requests](https://openai.com/requests-for-research/) for [research](https://blog.openai.com/requests-for-research-2/) pages, and I can't find it anywhere else either.

I could be wrong about the details, so it might be a different company/institution and it might be that it wasn't specifically about AGI but e.g. about detecting whether AI was being used at all. Does anyone else remember something like this, or am I just imagining it? </TEXT>
</WRITING>
<WRITING>
<TITLE> Should we fear artificial intelligence? - Report from the European Parliamentary Research Service with chapters from Peter Bentley, Miles Brundage, Olle Häggström and Thomas Metzinger </TITLE>
<DATE> 2018-03-27 19:21:26 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> AI Watch - a website to track people and organizations working on AI safety </TITLE>
<DATE> 2017-11-20 19:11:17 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Seth Baum - A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy </TITLE>
<DATE> 2017-11-20 14:38:52 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Race Avoidance in the Development of Artificial General Intelligence | AI Roadmap Institute </TITLE>
<DATE> 2017-09-11 22:51:02 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Welcome to /r/artificial! </TITLE>
<DATE> 2017-06-19 22:16:35 </DATE>
<INFO> reddit post </INFO>
<TEXT> /r/artificial is the largest subreddit dedicated to all issues related to Artificial Intelligence or AI. What does that mean? That is actually a tricky question, as the definition of AI is a topic of hot debate among people both inside and outside of the field. Broadly speaking, it is about machines that behave intelligently in some way, but this means different things to different people.

Most notably, there is the distinction between machines that are (at least) as intelligent as humans (artificial general intelligence / AGI) and machines that are capable of performing one task very well that would require intelligence if a human did it (narrow AI / ANI). When people outside the field think of "AI", they often think of AGI and possibly very humanlike AGI, often inspired by sci-fi books, shows and movies. However, today we are unable to create such systems. What we *can* do is create magnificently useful software and robotic tools, and that is what most of the professional AI field does. So to most professionals "AI" tends to refer to ANI. This can lead to a lot of confusion.

Another important thing to realize is that AI is an incredibly broad field that touches on Computer Science, Cognitive Science, Mathematics, Philosophy, Neuroscience, Linguistics and many others, and includes many subfields like Machine Learning, Robotics, Natural Language Processing, Computer Vision, Knowledge-Based Systems, Evolutionary Algorithms, Search and Planning. Many of these have subreddits dedicated to them as well (see [this list](https://www.reddit.com/r/artificial/wiki/related-subreddits)). /r/artificial is about all of these things. For instance, posts about computer vision are very welcome here, although the poster should realize people here will have a broader AI background than the specialists on /r/computervision, which might affect the kind of discussion that emerges.

On /r/artificial we welcome anyone who is interested in intelligent and respectful discussion of AI in any form. We want to provide a low barrier of entry, specifically because there are so many misconceptions about AI. We do ask that you put in a little effort before posting. Check out our burgeoning [wiki](https://www.reddit.com/r/artificial/wiki/index) and [Wikipedia's article on AI](https://en.wikipedia.org/wiki/Artificial_intelligence) to appreciate the breadth of the field. When you ask a question, [do so intelligently](http://www.wikihow.com/Ask-a-Question-Intelligently). When you post a story, prefer balanced discussion to clickbait, and please seek out the original source (many website just copy each others' stories without attribution). When you post a paper, please link to where it can be (legally) obtained for free and ideally to the landing page rather than directly to a PDF. Also consider jumpstarting the discussion with your own insights, questions, additional links and/or a short summary for people outside the niche the article was written for.

Please use this thread for suggestions, comments and questions *about* this subreddit.

Let's make this a great place for discussing artificial intelligence! </TEXT>
</WRITING>
<WRITING>
<TITLE> AMA on AI and startups at /r/startups with /u/testmypatience and /u/CyberByte </TITLE>
<DATE> 2017-06-06 20:11:37 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> [D] Deep, Deep Trouble Deep Learning’s Impact on Image Processing, Mathematics, and Humanity by Michael Elad </TITLE>
<DATE> 2017-05-08 20:53:37 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity by Michael Elad </TITLE>
<DATE> 2017-05-08 20:52:02 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> [R] [1611.02252] Hierarchical compositional feature learning - Vicarious </TITLE>
<DATE> 2017-02-25 18:47:37 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Asilomar AI Principles - Future of Life Institute </TITLE>
<DATE> 2017-01-30 22:03:57 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Asilomar AI Principles - Future of Life Institute </TITLE>
<DATE> 2017-01-30 22:03:08 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Videos from the 2015 and 2016 conferences on Artificial General Intelligence are now on YouTube </TITLE>
<DATE> 2016-07-26 16:20:31 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Videos from AGI-15 and AGI-16 conferences are now on YouTube </TITLE>
<DATE> 2016-07-26 16:18:57 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Concrete Problems in AI Safety (Google Brain, OpenAI, Berkeley + Stanford) </TITLE>
<DATE> 2016-06-23 00:50:01 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> New paper: “Safely interruptible agents” by Laurent Orseau (DeepMind) & Stuart Armstrong (FHI) </TITLE>
<DATE> 2016-06-03 22:28:22 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Discussion and Multiple (Expert) Surveys on Predictions of Human-Level AI Timelines </TITLE>
<DATE> 2016-02-18 17:53:49 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Right now more than 10% of my front page is spam. Any idea what is making this spammer so effective in bypassing the filters and how it might be stopped? </TITLE>
<DATE> 2016-02-14 19:29:28 </DATE>
<INFO> reddit post </INFO>
<TEXT> For a while now I have noticed an increase in spam on the subreddits that I visit, but I just went over my front page and now a whopping 11 out of 100 posts are spam: see [here](http://i.imgur.com/cOZFVON.png). I think *maybe* the "vot tut group" one is from someone else, but otherwise all seem to come from the same spammer.

Until a while ago I almost never saw spam on Reddit, and I'm still not seeing a lot except for this one spammer. So my guess is that it's something they are doing, rather than e.g. a change in Reddit's spam filter. I was curious about what, and was hoping someone could illuminate the issue a bit for me. My guess is that it might have something to do with the weird character sequences at the end of the titles, but I have no idea.

I'm not sure where to ask this, but I figured I'd try here since I know very little about programming spam bots and filters and am basically asking to be educated a bit. Hopefully some intelligent speculation/education is possible with the limited information we have. </TEXT>
</WRITING>
<WRITING>
<TITLE> Creating Human-Friendly AGIs and Superintelligences: Two Theses by Ben Goertzel </TITLE>
<DATE> 2015-11-01 02:48:39 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Inside DeepMind - upcoming Nature paper on "Human-level control through deep reinforcement learning" in Atari games </TITLE>
<DATE> 2015-02-25 17:21:59 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Is deontology dead? </TITLE>
<DATE> 2015-02-11 18:33:45 </DATE>
<INFO> reddit post </INFO>
<TEXT> I was reading [this article](http://io9.com/why-asimovs-three-laws-of-robotics-cant-protect-us-1553665410) a while ago, and one quote stood out to me:

> "One reason is that rule-abiding systems of ethics — referred to as 'deontology' — are known to be a broken foundation for ethics. There are still a few philosophers trying to fix systems of deontology — but these are mostly the same people trying to shore up 'intelligent design' and 'divine command theory'," says Helm. "No one takes them seriously."

I don't think Louie Helm is a prominent philosopher or anything, but at the time of writing he was the deputy director of probably the largest research institute for Friendly AI. As someone with an AI background and some interest in philosophy, I wonder what made Helm make these statements. Is there indeed such a consensus among modern philosophers? What criticisms of deontology is he referencing? Do you agree? </TEXT>
</WRITING>
<WRITING>
<TITLE> An Open Letter To Everyone Tricked Into Fearing AI </TITLE>
<DATE> 2015-01-15 21:54:49 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> An Open Letter To Everyone Tricked Into Fearing AI </TITLE>
<DATE> 2015-01-15 21:52:35 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Paris Hilton on Existential Risks from Artificial Intelligence </TITLE>
<DATE> 2014-10-28 02:10:40 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Video and proceedings links for the 7th Conference on Artificial General Intelligence (AGI-14) </TITLE>
<DATE> 2014-08-14 14:44:15 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> Video and proceedings links for the 7th Conference on Artificial General Intelligence (AGI-14) </TITLE>
<DATE> 2014-08-14 14:44:00 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> How does the time between the creation of a person's copy and the original's death affect transfer of personal identity? </TITLE>
<DATE> 2014-07-26 20:32:44 </DATE>
<INFO> reddit post </INFO>
<TEXT> Suppose there is a button that makes a person better (let's say smarter). There are 3 scenarios:

1. The button just makes the person smarter.
2. The button creates a perfect copy of the person, makes it smarter, and instantly kills the original person.
3. #2, but the original is killed some known fixed time T after pressing the button (if he doesn't die before that).

I'll assume that scenarios #1 and #2 are equivalent from the perspective of the person pressing the button (but feel free to correct me here). My main question is how the length of the time period T would affect the rationality of the decision to press the button (assuming coexistence with the copy isn't necessarily beneficial). It seems to me that if T is one microsecond, then scenario #3 is basically equivalent to #2, and if T is 10 years you just created a separate (but extremely similar) person and the button just shortened your life expectancy. But what if T is one minute? One minute is pretty inconsequential in the life of a human, but it is enough to become aware of the copy and how it is a separate person. It seems that at some length of time T, the original's personal identity is no longer moved (rather than copied) to the copy, but how should this value be determined?

Can anyone shed some light on this (or perhaps correct my mistakes)? Thanks! </TEXT>
</WRITING>
<WRITING>
<TITLE> Any tips for finding the closed-form solution of expressions? </TITLE>
<DATE> 2014-05-28 20:56:56 </DATE>
<INFO> reddit post </INFO>
<TEXT> tl;dr: I want to learn about how to find closed-form solutions in general (and how to tell when that's impossible).

---

Sorry for asking such a general question, but my Google-fu is failing me. I can only find people asking for closed-form solutions to specific equations.

I was just mucking around a bit with a recursive equation and I wanted to find the closed form, but realized I didn't really know how to start. It had the form

    [; F_{n+1} = a \cdot F_n + b \cdot F_{\lfloor n/2 \rfloor} + c ;]

where `a`, `b` and `c` are constants. I know how to do it if `a` or `b` is 0, but otherwise I don't even know where to begin. It's also possible that this equation doesn't have a closed-form solution, but I don't know how to find that out either.

Although I gave an example above, I'm actually interested about learning about this stuff in general. I would greatly appreciate any pointers to online resources like tutorials, courses or bags of tricks, or even search terms because I don't even know what branch of mathematics this falls under. And if you want to share your own tips that would of course also be very welcome.

Thanks! </TEXT>
</WRITING>
<WRITING>
<TITLE> How do the young of intelligent animals acquire adult-level skills and intellect? </TITLE>
<DATE> 2014-03-09 16:09:37 </DATE>
<INFO> reddit post </INFO>
<TEXT> Clearly humans aren't born with the full palette of (mental) abilities that we have as adults. To acquire these, it seems that we rely heavily on other humans (especially adults). Obviously adults actively try to educate us, but it also seems that we are innately predisposed to imitate others.

How is this for other (intelligent) animals? Can they learn everything on their own? Do they also heavily rely on imitating others of their species? And do adults of (some) species actively make an attempt at education? For instance, I can imagine predators taking their adolescents on training missions where they can either observe the adults, or perhaps even hunt easier prey. Or maybe animals with "language" (e.g. orca's) instruct each other... Do such things happen?

I would be very grateful for any answers and references to literature relating to (intelligent) animal cognitive development, and especially the role of other other members of the species. </TEXT>
</WRITING>
<WRITING>
<TITLE> Deep learning pioneer Yoshua Bengio AMA: Thursday 1-2PM EST in /r/MachineLearning </TITLE>
<DATE> 2014-02-24 18:42:42 </DATE>
<INFO> reddit post </INFO>
<TEXT>  </TEXT>
</WRITING>
<WRITING>
<TITLE> [Request] What's the force of impact of a consumer-grade fireworks rocket fired horizontally? </TITLE>
<DATE> 2014-01-26 17:16:32 </DATE>
<INFO> reddit post </INFO>
<TEXT> Background: On New Year's Eve my ankle got hit with a neighbor's skyrocket that had fallen over and launched horizontally. My ankle just bruised and swelled a bit and although it's still not completely over it wasn't very serious. Jeans, 2-3mm of leather shoe and a sock were protecting my ankle and I estimate I was standing 15m away. I'm saying this, because I don't know the exact kind of rocket and I guess it could help with identification. I'm guessing it was kind of like the first rocket that explodes in [this video](http://www.youtube.com/watch?v=vDBAlp7B-c4).

I was just wondering about the speed, weight, force, etc. of such rockets, but my Google-fu has abandoned me today. Thanks for any help! </TEXT>
</WRITING>
<WRITING>
<TITLE> How can I deal with uncertain and not equally important training data? </TITLE>
<DATE> 2009-11-16 16:14:51 </DATE>
<INFO> reddit post </INFO>
<TEXT> I have a signal processing problem that involves classifying time samples into one of several categories. The labels in the train set look more or less like this:

    000000???111111???00000????111??0000??22222????00000
    abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ

The x-axis here is time, the numbers on the first row are the labels I know about and the question marks are there because I don't exactly know what the labels there should be. The second row is just used to refer to timepoints in this example. The question marks really show transitions from one label into another, but I don't know exactly where the transition is. This means that a question mark should be one of the two surrounding labels (and not anything else) and that if we have AA????BB and the second question mark is a B, then the next ones should also be B's.
My first problem is basically how to incorporate the information I do have about these samples in my algorithm.

The second problem has to do with the fact that not all labels are equally important. If for every group of non-zero's just one would be classified correctly and the rest would be classified as zero, that would be great. So in the above example I would be happier if all samples would be classified as zeros except *m*, *B* and *O* than if for instance *j-o* would be classified correctly and the rest would be zeros, even though the latter would technically give a higher accuracy.
My second problem is thus how to deal with this unequal preference for different labels.

I guess I could just ignore all of this information I have, but the problem is that when I do that, performance is very bad. Of course, there is no guarantee that using it will make things much better, but I'm hoping it will.

I'm still quite new to this field, so if anyone could point me in the direction of algorithms that can be used for this, suggest resources that apply to this or even (like books, webpages or other places to ask questions like this; a StackOverflow for science questions would be great) come up with solutions for my problems, that would be great! I've tried to keep this as abstract and general as possible and I hope it is still clear what the problem is. If not, please ask and I will try to clarify. If you feel it's inappropriate for me to ask you to help with my problems, feel free to downvote. Thanks for your time! </TEXT>
</WRITING>
</INDIVIDUAL>
